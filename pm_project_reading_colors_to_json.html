<!DOCTYPE html>
<html>
<head>
<link rel="Stylesheet" type="text/css" href="style.css">
<link rel="alternate" type="application/rss+xml" title="RSS" href="rss.xml">
<title>pm_project_reading_colors_to_json</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
</head>
<body>

<p>
<a href="index.html">Index</a>
</p>

<p>
<a href="pmount_index.html">PMount</a>
</p>

<div id="Reading Colors to JSON"><h1 id="Reading Colors to JSON" class="header"><a href="#Reading Colors to JSON">Reading Colors to JSON</a></h1></div>


<table>
<tr>
<td>
I want to get a list of common colors in a sequence of images. I'm bringing in sequences of images with around 30 colors per sequence (they are comic book style). I want to make a json list of RGB colors. I can do this in Nuke using the 'sample' method in Python, but it's slow. Can I use some other tool (inside or outside Nuke) to achieve this more efficiently and faster. For example I've heard the 'Open Image IO' may be faster. I'm open to any ideas.
</td>
</tr>
</table>

<p>
The slow performance of Nuke's Python sample method is a common issue because it incurs overhead from the Nuke evaluation engine. To achieve this more efficiently and faster, you should move the image processing outside of the Nuke Python API and use high-performance, dedicated Python libraries.
</p>

<p>
The best approach for finding a limited set of dominant colors is to use a Machine Learning/Statistical Clustering method, specifically K-Means Clustering, which is highly optimized in popular scientific libraries.
</p>

<p>
Here is the most efficient solution, combining fast I/O and statistical processing:
</p>

<div id="Reading Colors to JSON-Recommended Solution: External Python with NumPy + Scikit-learn"><h2 id="Recommended Solution: External Python with NumPy + Scikit-learn" class="header"><a href="#Reading Colors to JSON-Recommended Solution: External Python with NumPy + Scikit-learn">Recommended Solution: External Python with NumPy + Scikit-learn</a></h2></div>

<p>
This method is significantly faster because it uses optimized C/C++ backend libraries (like NumPy) for file reading and computation, bypassing Nuke's slower internal Python wrappers.
</p>

<div id="Reading Colors to JSON-Recommended Solution: External Python with NumPy + Scikit-learn-The Strategy: K-Means Clustering"><h3 id="The Strategy: K-Means Clustering" class="header"><a href="#Reading Colors to JSON-Recommended Solution: External Python with NumPy + Scikit-learn-The Strategy: K-Means Clustering">The Strategy: K-Means Clustering</a></h3></div>

<p>
Since your images are "comic book style" with a naturally limited number of distinct colors (around 30), K-Means Clustering is the ideal algorithm.
</p>
<ul>
<li>
<span id="Reading Colors to JSON-Recommended Solution: External Python with NumPy + Scikit-learn-The Strategy: K-Means Clustering-How it works"></span><strong id="How it works">How it works</strong>: It groups all the pixels in an image into K clusters (where K is the number of colors you expect, e.g., K=30). The center of each cluster (centroid) is one of your dominant colors.

<li>
<span id="Reading Colors to JSON-Recommended Solution: External Python with NumPy + Scikit-learn-The Strategy: K-Means Clustering-Speed"></span><strong id="Speed">Speed</strong>: Libraries like Scikit-learn or OpenCV implement K-Means using highly optimized algorithms that can process millions of pixels quickly.

</ul>
<div id="Reading Colors to JSON-Recommended Solution: External Python with NumPy + Scikit-learn-The Tools"><h3 id="The Tools" class="header"><a href="#Reading Colors to JSON-Recommended Solution: External Python with NumPy + Scikit-learn-The Tools">The Tools</a></h3></div>

<table>
<tr>
<td>
Tool
</td>
<td>
Purpose	Why it's Fast
</td>
<td>
&nbsp;
</td>
</tr>
<tr>
<td>
NumPy
</td>
<td>
High-performance, low-level array manipulation.
</td>
<td>
Core of Python scientific computing, operates on pre-compiled C/C++ code.
</td>
</tr>
<tr>
<td>
OpenCV (cv2) or PIL/Pillow
</td>
<td>
Fast image loading and pixel access.
</td>
<td>
Optimized for reading and writing large image data efficiently.
</td>
</tr>
<tr>
<td>
Scikit-learn (sklearn)
</td>
<td>
Implements the fast K-Means Clustering algorithm.
</td>
<td>
Excellent, robust, and fast statistical library.
</td>
</tr>
<tr>
<td>
OpenImageIO (OIIO)
</td>
<td>
Optional. The Python bindings are great for VFX file formats (EXR, TIFF, etc.), offering fast, consistent I/O, as you suspected.
</td>
<td>
Specifically designed for high-performance reading of image sequences in a production pipeline.
</td>
</tr>
</table>

<div id="Reading Colors to JSON-Python Code Example (K-Means)"><h2 id="Python Code Example (K-Means)" class="header"><a href="#Reading Colors to JSON-Python Code Example (K-Means)">Python Code Example (K-Means)</a></h2></div>

<p>
This script iterates through your sequence, processes each frame for K=30 colors, and aggregates the results.
</p>

<pre python>
import numpy as np
import cv2  # You could use Pillow or OIIO instead of OpenCV
from sklearn.cluster import MiniBatchKMeans
import json
import glob
import os

def extract_dominant_colors_sequence(sequence_path, num_colors=30, frames=None):
    """
    Extracts the dominant RGB colors from a sequence of images using MiniBatchKMeans.
    
    Args:
        sequence_path (str): File path pattern (e.g., '/path/to/img.%04d.exr').
        num_colors (int): The target number of dominant colors (K).
        frames (list): Optional list of frame numbers to process (e.g., [1, 10, 20]).
    
    Returns:
        dict: A dictionary mapping frame numbers to a list of RGB color lists.
    """
    
    all_dominant_colors = {}
    
    if not frames:
        # Simple glob to find all files if frame list isn't provided
        # NOTE: This part needs careful handling for frame number parsing if not using Nuke's native Read node features
        print("Warning: Processing a basic file glob. Consider using OIIO for robust sequence handling.")
        file_list = sorted(glob.glob(sequence_path.replace('%04d', '*')))
        
        # Simple frame number extraction for file_list:
        frame_list = [os.path.basename(f).split('.')[1] for f in file_list]
        file_frame_pairs = zip(file_list, frame_list)
        
    else:
        # Use nuke.frame() functionality replacement for a range of frames
        def get_frame_path(frame_num):
            # Assumes the sequence path uses Nuke-style padding (e.g., %04d)
            return sequence_path % frame_num
        
        file_frame_pairs = [(get_frame_path(f), str(f)) for f in frames]


    for file_path, frame_num_str in file_frame_pairs:
        try:
            # 1. Load Image (using OpenCV for speed)
            # flag=1 loads as BGR (color), flag=-1 loads all channels including alpha
            img = cv2.imread(file_path)
            if img is None:
                print(f"Skipping frame {frame_num_str}: Could not load image.")
                continue

            # Convert BGR (OpenCV default) to RGB
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            
            # 2. Reshape and Normalize Pixel Data
            # Reshape the image array to a list of pixels (N rows, 3 columns for R, G, B)
            pixels = img.reshape((-1, 3))
            pixels = np.float32(pixels)

            # 3. Apply K-Means Clustering (using MiniBatchKMeans for even more speed)
            # This finds 'num_colors' representative colors
            kmeans = MiniBatchKMeans(n_clusters=num_colors, n_init='auto', random_state=42)
            kmeans.fit(pixels)
            
            # The cluster centers are the dominant colors
            dominant_colors = kmeans.cluster_centers_

            # 4. Format Output: Convert float RGB (0-255) to integer list
            # The result is a list of [R, G, B] lists
            color_list = dominant_colors.astype(int).tolist()
            
            all_dominant_colors[frame_num_str] = color_list
            print(f"Processed frame {frame_num_str}: Found {len(color_list)} colors.")

        except Exception as e:
            print(f"Error processing frame {frame_num_str}: {e}")
            
    # 5. Output to JSON
    json_output = json.dumps(all_dominant_colors, indent=4)
    return json_output

# --- Example Usage ---
# NOTE: Replace 'read_node_file_path' with the actual file sequence path.
# Example path with Nuke-style padding:
# read_node_file_path = '/path/to/my/comic/scene_1.shot_01.%04d.exr'
# frames_to_process = list(range(101, 201))  # Process frames 101 through 200

# # Run the function
# json_data = extract_dominant_colors_sequence(read_node_file_path, num_colors=30, frames=frames_to_process)

# # Save the JSON file
# with open('comic_palette.json', 'w') as f:
#     f.write(json_data)

</pre>

<div id="Reading Colors to JSON-The OpenImageIO (OIIO) Option"><h2 id="The OpenImageIO (OIIO) Option" class="header"><a href="#Reading Colors to JSON-The OpenImageIO (OIIO) Option">The OpenImageIO (OIIO) Option</a></h2></div>

<p>
You specifically mentioned OpenImageIO. This is an excellent idea, especially if your pipeline uses complex file formats like OpenEXR, TIFF, or DPX.
</p>

<ul>
<li>
<span id="Reading Colors to JSON-The OpenImageIO (OIIO) Option-Integration"></span><strong id="Integration">Integration</strong>: Use the Python bindings for OIIO (import OpenImageIO as OIIO).

<li>
<span id="Reading Colors to JSON-The OpenImageIO (OIIO) Option-Fast Reading"></span><strong id="Fast Reading">Fast Reading</strong>: Replace cv2.imread(file_path) with OIIO code to read the image data into a NumPy array (which OIIO handles natively).

<li>
<span id="Reading Colors to JSON-The OpenImageIO (OIIO) Option-Process"></span><strong id="Process">Process</strong>: Pass the NumPy array directly to the K-Means clustering step (as shown above).

</ul>
<p>
This separates the fast I/O (OIIO) from the fast computation (NumPy/Scikit-learn), maximizing efficiency.
</p>

<div id="Reading Colors to JSON-The OpenImageIO (OIIO) Option-OIIO Replacement for cv2.imread:"><h3 id="OIIO Replacement for cv2.imread:" class="header"><a href="#Reading Colors to JSON-The OpenImageIO (OIIO) Option-OIIO Replacement for cv2.imread:">OIIO Replacement for cv2.imread:</a></h3></div>

<pre python>
import OpenImageIO as OIIO
import numpy as np
# ... (rest of your imports)

def read_image_with_oiio(file_path):
    """Loads an image into a NumPy array using OpenImageIO."""
    
    # 1. Read the image and spec
    img_input = OIIO.ImageInput.open(file_path)
    if not img_input:
        raise IOError(f"Could not open image with OIIO: {OIIO.geterror()}")

    spec = img_input.spec()
    
    # 2. Create NumPy array and read pixel data
    # Read as float type and get R, G, B channels
    pixel_data = np.empty((spec.height, spec.width, spec.nchannels), dtype=np.float32)
    img_input.read_image(0, 0, spec.nchannels, OIIO.TypeDesc.FLOAT, pixel_data)
    img_input.close()
    
    # Only return the first 3 channels (R, G, B), dropping Alpha if present
    return (pixel_data[:, :, :3] * 255.0).astype(np.uint8) # Convert to 0-255 range for clustering
</pre>

<p>
You would then call this function within your main loop and feed the resulting array to the clustering steps.
</p>

<table>
<tr>
<td>
I won't know the number of colors for each image sequence, but I know they will be relatively low in number. How can I change the code to expect any number of colors?
</td>
</tr>
</table>

<p>
That's a key challenge when using K-Means Clustering! If you don't know the optimal number of clusters (K), you need an automated way to estimate it for each frame.
</p>

<p>
You can modify the code to use the Elbow Method to dynamically determine the best K for a given image, making the process adaptive to your comic book style sequences.
</p>

<div id="Reading Colors to JSON-The Strategy: The Elbow Method üìê"><h2 id="The Strategy: The Elbow Method üìê" class="header"><a href="#Reading Colors to JSON-The Strategy: The Elbow Method üìê">The Strategy: The Elbow Method üìê</a></h2></div>

<p>
The Elbow Method is a heuristic used to find the optimal number of clusters (K) in K-Means.
</p>

<ul>
<li>
<span id="Reading Colors to JSON-The Strategy: The Elbow Method üìê-Calculate Distortion"></span><strong id="Calculate Distortion">Calculate Distortion</strong>: Run K-Means multiple times, increasing K (e.g., from 5 to 50). For each run, record the distortion (the sum of squared distances between each point and its assigned centroid).

<li>
<span id="Reading Colors to JSON-The Strategy: The Elbow Method üìê-Find the &quot;Elbow&quot;"></span><strong id="Find the &quot;Elbow&quot;">Find the "Elbow"</strong>: Plot K (x-axis) vs. Distortion (y-axis). The distortion will decrease sharply as K increases, then slow down dramatically. The point where the curve bends, resembling an "elbow," is the optimal number of clusters, as adding more clusters provides diminishing returns.

</ul>
<p>
Since your color counts are low (around 30), we can define a reasonable search range for K.
</p>

<div id="Reading Colors to JSON-Modified Python Code with Dynamic K-Selection"><h2 id="Modified Python Code with Dynamic K-Selection" class="header"><a href="#Reading Colors to JSON-Modified Python Code with Dynamic K-Selection">Modified Python Code with Dynamic K-Selection</a></h2></div>

<p>
This modified script uses the scipy.spatial.distance.cdist function (often necessary for distortion calculation) and the MiniBatchKMeans inertia (which is equivalent to distortion) over a range of possible K values.
</p>

<pre python>
import numpy as np
import cv2  # or OIIO, as discussed
from sklearn.cluster import MiniBatchKMeans
from scipy.spatial.distance import cdist
import json
import glob
import os

def determine_optimal_k(pixels_data, k_range=(5, 50)):
    """
    Uses the Elbow Method (via inertia) to find the optimal number of clusters (K).
    
    Args:
        pixels_data (np.ndarray): Reshaped image pixels (N, 3).
        k_range (tuple): (min K, max K) to search within.
        
    Returns:
        int: The estimated optimal number of clusters.
    """
    k_min, k_max = k_range
    inertias = []
    
    # Run MiniBatchKMeans for a range of K values
    for k in range(k_min, k_max + 1, 5): # Check every 5th K for speed
        kmeans = MiniBatchKMeans(n_clusters=k, n_init='auto', random_state=42)
        kmeans.fit(pixels_data)
        inertias.append(kmeans.inertia_)
    
    # --- Simplified Elbow Detection ---
    # This is often done visually, but we can approximate by looking for the 
    # point where the rate of decrease in inertia slows down.
    
    # Calculate the percentage change in inertia
    changes = np.diff(inertias)
    percent_drops = np.abs(changes / inertias[:-1])
    
    # Find the index where the drop (gain in information) falls below a threshold (e.g., 5%)
    # This threshold may need adjustment based on your image style.
    threshold = 0.05 
    
    # Add 1 to the index because diff/changes array is one element shorter
    elbow_index = np.where(percent_drops &lt; threshold)[0]
    
    if len(elbow_index) &gt; 0:
        # The elbow is typically the first index where the drop slows down
        return k_range[0] + (elbow_index[0] + 1) * 5 
    else:
        # If no elbow is clearly found, default to the max K in the range
        return k_max

def extract_dominant_colors_sequence_dynamic(sequence_path, k_search_range=(5, 50), frames=None):
    """
    Extracts dominant colors, dynamically determining the optimal K for each frame.
    """
    all_dominant_colors = {}
    
    # --- Setup file/frame list logic (omitted for brevity, assume same as before) ---
    # NOTE: You'd implement your file iteration here. For this example, we'll
    # assume the input is a list of file paths named 'file_frame_pairs'.
    
    # Placeholder for file iteration based on a single sample frame:
    
    # Example to load the first file in the sequence as a test:
    # file_list = sorted(glob.glob(sequence_path.replace('%04d', '*')))
    # file_frame_pairs = zip(file_list, [os.path.basename(f).split('.')[1] for f in file_list])
    
    # --- Main Loop ---
    # We will assume a single image file for demonstration
    file_path = "path/to/single/image.jpg" # Replace with actual file iteration
    frame_num_str = "001" 
    
    # 1. Load Image
    img = cv2.imread(file_path) 
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    pixels = img.reshape((-1, 3))
    pixels = np.float32(pixels)

    # 2. Determine Optimal K
    optimal_k = determine_optimal_k(pixels, k_search_range)
    print(f"Frame {frame_num_str}: Optimal K estimated at {optimal_k}")

    # 3. Final K-Means with Optimal K
    kmeans = MiniBatchKMeans(n_clusters=optimal_k, n_init='auto', random_state=42)
    kmeans.fit(pixels)
    dominant_colors = kmeans.cluster_centers_

    # 4. Format Output
    color_list = dominant_colors.astype(int).tolist()
    all_dominant_colors[frame_num_str] = color_list
        
    json_output = json.dumps(all_dominant_colors, indent=4)
    return json_output

# --- Considerations ---
# 1. The range for K_RANGE (5, 50) is a guess. Adjust based on how many
#    unique colors your images actually contain.
# 2. The ELBOW DETECTION THRESHOLD (0.05) is a heuristic. It may need tuning.
# 3. For faster processing, you might want to run the K-Means on a 
#    *downscaled* version of the image (e.g., resize to 500x500 pixels) 
#    before calculating the elbow and the final colors.
</pre>

<table>
<tr>
<td>
I believe the RGB values would need to be in the same units/representation as used in Nuke - zero to one for standard black to white (while HDR images are possible). Would I forego the conversion to 0 - 255?
</td>
</tr>
</table>

<p>
Yes, you are absolutely correct and should forego the conversion to the 0-255 range if you intend to use the resulting RGB values directly within Nuke (or any standard VFX application). üí°
</p>

<p>
Nuke and OpenEXR/HDR pipelines natively operate on floating-point color values where standard dynamic range is represented by 0.0 to 1.0.
</p>

<div id="Reading Colors to JSON-The Necessary Adjustment to the Code"><h2 id="The Necessary Adjustment to the Code" class="header"><a href="#Reading Colors to JSON-The Necessary Adjustment to the Code">The Necessary Adjustment to the Code</a></h2></div>

<p>
You need to ensure two things in your processing pipeline:
</p>
<ul>
<li>
<span id="Reading Colors to JSON-The Necessary Adjustment to the Code-Image Read"></span><strong id="Image Read">Image Read</strong>: The image data is loaded directly as floating-point numbers.

<li>
<span id="Reading Colors to JSON-The Necessary Adjustment to the Code-K-Means Processing"></span><strong id="K-Means Processing">K-Means Processing</strong>: The clustering algorithm runs on the 0.0‚àí1.0 floating-point data.

<li>
<span id="Reading Colors to JSON-The Necessary Adjustment to the Code-Output"></span><strong id="Output">Output</strong>: The final cluster centers are outputted as floating-point numbers.

</ul>
<p>
If you are using OpenImageIO (OIIO), this is much simpler as it defaults to reading data as float (or whatever the file precision is) and gives you the 0.0‚àí1.0 data naturally. If you are using OpenCV or Pillow, you must explicitly ensure the conversion to float and division by 255 does not happen, or handle the data type correctly.
</p>

<div id="Reading Colors to JSON-Corrected Code Snippet using OIIO (Recommended)"><h2 id="Corrected Code Snippet using OIIO (Recommended)" class="header"><a href="#Reading Colors to JSON-Corrected Code Snippet using OIIO (Recommended)">Corrected Code Snippet using OIIO (Recommended)</a></h2></div>

<p>
Since OIIO is designed for VFX, it's the most reliable for float data.
</p>

<pre python>
import OpenImageIO as OIIO
import numpy as np
# ... (rest of your imports)

def read_image_with_oiio(file_path):
    """Loads an image into a NumPy array using OpenImageIO as float32 (0.0 - 1.0 range)."""
    
    img_input = OIIO.ImageInput.open(file_path)
    if not img_input:
        raise IOError(f"Could not open image with OIIO: {OIIO.geterror()}")

    spec = img_input.spec()
    
    # Read pixel data directly as float32
    # OIIO reads data in its native float range (0.0-1.0 or HDR range for EXR)
    pixel_data = np.empty((spec.height, spec.width, spec.nchannels), dtype=np.float32)
    img_input.read_image(0, 0, spec.nchannels, OIIO.TypeDesc.FLOAT, pixel_data)
    img_input.close()
    
    # Return R, G, B channels in 0.0-1.0 (or HDR float) range
    return pixel_data[:, :, :3] # Keep R, G, B and discard others

# --- The clustering function remains the same, but operates on float data ---

# Inside extract_dominant_colors_sequence_dynamic:
# 1. Load Image
img_float_data = read_image_with_oiio(file_path) 

# 2. Reshape Pixel Data (already float, no normalization needed)
pixels = img_float_data.reshape((-1, 3))
# pixels is now np.float32 in 0.0-1.0+ range
# ... (Proceed with determine_optimal_k and MiniBatchKMeans on 'pixels')

# 3. Final Output (NO .astype(int) conversion!)
# Keep the cluster centers as a float list for JSON output
# color_list = dominant_colors.astype(int).tolist()  &lt;-- DELETE THIS LINE
color_list = dominant_colors.tolist() 
</pre>

<div id="Reading Colors to JSON-Corrected Code Snippet using OpenCV (If Used)"><h2 id="Corrected Code Snippet using OpenCV (If Used)" class="header"><a href="#Reading Colors to JSON-Corrected Code Snippet using OpenCV (If Used)">Corrected Code Snippet using OpenCV (If Used)</a></h2></div>

<p>
If you must use OpenCV (or Pillow), the key is to load the image and manually convert to float in the correct range, then bypass any conversion back to integer.
</p>

<pre python>
# Using OpenCV
img = cv2.imread(file_path) 
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

# Convert to float and normalize to 0.0 - 1.0 range
# Assumes the input image is 8-bit (0-255). If it's a 16-bit TIFF, this is wrong.
pixels = img.reshape((-1, 3))
pixels = np.float32(pixels) / 255.0  
# For true HDR/float files, you must use OIIO or specialized readers like OpenEXR.

# ... (Proceed with clustering)

# Final Output (still must output as float list)
color_list = dominant_colors.tolist() 
</pre>

<div id="Reading Colors to JSON-Corrected Code Snippet using OpenCV (If Used)-Crucial Takeaway:"><h3 id="Crucial Takeaway:" class="header"><a href="#Reading Colors to JSON-Corrected Code Snippet using OpenCV (If Used)-Crucial Takeaway:">Crucial Takeaway:</a></h3></div>

<p>
By removing the .astype(int) and .astype(int) conversions and ensuring the input array is already in the float (0.0 - 1.0+) range, your resulting JSON will contain values ready to be pasted directly into a Nuke node's RGB color field.
</p>

<p>
<a href="pmount_index.html">PMount</a>
</p>

<p>
<a href="index.html">Index</a>
</p>

</body>
</html>
