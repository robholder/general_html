<!DOCTYPE html>
<html>
<head>
<link rel="Stylesheet" type="text/css" href="style.css">
<title>pm_project_reading_colors_to_json</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
</head>
<body>

<p>
<a href="index.html">Index</a>
</p>

<p>
<a href="pmount_index.html">PMount</a>
</p>

<div id="Reading Colors to JSON"><h1 id="Reading Colors to JSON" class="header"><a href="#Reading Colors to JSON">Reading Colors to JSON</a></h1></div>

<div id="Reading Colors to JSON-QUESTION"><h2 id="QUESTION" class="header"><a href="#Reading Colors to JSON-QUESTION">QUESTION</a></h2></div>

<table>
<tr>
<td>
I want to get a list of common colors in a sequence of images. I'm bringing in sequences of images with around 30 colors per sequence (they are comic book style). I want to make a json list of RGB colors. I can do this in Nuke using the 'sample' method in Python, but it's slow. Can I use some other tool (inside or outside Nuke) to achieve this more efficiently and faster. For example I've heard the 'Open Image IO' may be faster. I'm open to any ideas.
</td>
</tr>
</table>

<p>
The slow performance of Nuke's Python sample method is a common issue because it incurs overhead from the Nuke evaluation engine. To achieve this more efficiently and faster, you should move the image processing outside of the Nuke Python API and use high-performance, dedicated Python libraries.
</p>

<p>
The best approach for finding a limited set of dominant colors is to use a Machine Learning/Statistical Clustering method, specifically K-Means Clustering, which is highly optimized in popular scientific libraries.
</p>

<p>
Here is the most efficient solution, combining fast I/O and statistical processing:
</p>

<div id="Reading Colors to JSON-Recommended Solution: External Python with NumPy + Scikit-learn"><h2 id="Recommended Solution: External Python with NumPy + Scikit-learn" class="header"><a href="#Reading Colors to JSON-Recommended Solution: External Python with NumPy + Scikit-learn">Recommended Solution: External Python with NumPy + Scikit-learn</a></h2></div>

<p>
This method is significantly faster because it uses optimized C/C++ backend libraries (like NumPy) for file reading and computation, bypassing Nuke's slower internal Python wrappers.
</p>

<div id="Reading Colors to JSON-Recommended Solution: External Python with NumPy + Scikit-learn-The Strategy: K-Means Clustering"><h3 id="The Strategy: K-Means Clustering" class="header"><a href="#Reading Colors to JSON-Recommended Solution: External Python with NumPy + Scikit-learn-The Strategy: K-Means Clustering">The Strategy: K-Means Clustering</a></h3></div>

<p>
Since your images are "comic book style" with a naturally limited number of distinct colors (around 30), K-Means Clustering is the ideal algorithm.
</p>
<ul>
<li>
<span id="Reading Colors to JSON-Recommended Solution: External Python with NumPy + Scikit-learn-The Strategy: K-Means Clustering-How it works"></span><strong id="How it works">How it works</strong>: It groups all the pixels in an image into K clusters (where K is the number of colors you expect, e.g., K=30). The center of each cluster (centroid) is one of your dominant colors.

<li>
<span id="Reading Colors to JSON-Recommended Solution: External Python with NumPy + Scikit-learn-The Strategy: K-Means Clustering-Speed"></span><strong id="Speed">Speed</strong>: Libraries like Scikit-learn or OpenCV implement K-Means using highly optimized algorithms that can process millions of pixels quickly.

</ul>

<div id="Reading Colors to JSON-Recommended Solution: External Python with NumPy + Scikit-learn-The Tools"><h3 id="The Tools" class="header"><a href="#Reading Colors to JSON-Recommended Solution: External Python with NumPy + Scikit-learn-The Tools">The Tools</a></h3></div>

<table>
<tr>
<td>
Tool
</td>
<td>
Purpose	Why it's Fast
</td>
<td>
&nbsp;
</td>
</tr>
<tr>
<td>
NumPy
</td>
<td>
High-performance, low-level array manipulation.
</td>
<td>
Core of Python scientific computing, operates on pre-compiled C/C++ code.
</td>
</tr>
<tr>
<td>
OpenCV (cv2) or PIL/Pillow
</td>
<td>
Fast image loading and pixel access.
</td>
<td>
Optimized for reading and writing large image data efficiently.
</td>
</tr>
<tr>
<td>
Scikit-learn (sklearn)
</td>
<td>
Implements the fast K-Means Clustering algorithm.
</td>
<td>
Excellent, robust, and fast statistical library.
</td>
</tr>
<tr>
<td>
OpenImageIO (OIIO)
</td>
<td>
Optional. The Python bindings are great for VFX file formats (EXR, TIFF, etc.), offering fast, consistent I/O, as you suspected.
</td>
<td>
Specifically designed for high-performance reading of image sequences in a production pipeline.
</td>
</tr>
</table>

<div id="Reading Colors to JSON-Python Code Example (K-Means)"><h2 id="Python Code Example (K-Means)" class="header"><a href="#Reading Colors to JSON-Python Code Example (K-Means)">Python Code Example (K-Means)</a></h2></div>

<p>
This script iterates through your sequence, processes each frame for K=30 colors, and aggregates the results.
</p>

<pre python>
import numpy as np
import cv2  # You could use Pillow or OIIO instead of OpenCV
from sklearn.cluster import MiniBatchKMeans
import json
import glob
import os

def extract_dominant_colors_sequence(sequence_path, num_colors=30, frames=None):
    """
    Extracts the dominant RGB colors from a sequence of images using MiniBatchKMeans.
    
    Args:
        sequence_path (str): File path pattern (e.g., '/path/to/img.%04d.exr').
        num_colors (int): The target number of dominant colors (K).
        frames (list): Optional list of frame numbers to process (e.g., [1, 10, 20]).
    
    Returns:
        dict: A dictionary mapping frame numbers to a list of RGB color lists.
    """
    
    all_dominant_colors = {}
    
    if not frames:
        # Simple glob to find all files if frame list isn't provided
        # NOTE: This part needs careful handling for frame number parsing if not using Nuke's native Read node features
        print("Warning: Processing a basic file glob. Consider using OIIO for robust sequence handling.")
        file_list = sorted(glob.glob(sequence_path.replace('%04d', '*')))
        
        # Simple frame number extraction for file_list:
        frame_list = [os.path.basename(f).split('.')[1] for f in file_list]
        file_frame_pairs = zip(file_list, frame_list)
        
    else:
        # Use nuke.frame() functionality replacement for a range of frames
        def get_frame_path(frame_num):
            # Assumes the sequence path uses Nuke-style padding (e.g., %04d)
            return sequence_path % frame_num
        
        file_frame_pairs = [(get_frame_path(f), str(f)) for f in frames]


    for file_path, frame_num_str in file_frame_pairs:
        try:
            # 1. Load Image (using OpenCV for speed)
            # flag=1 loads as BGR (color), flag=-1 loads all channels including alpha
            img = cv2.imread(file_path)
            if img is None:
                print(f"Skipping frame {frame_num_str}: Could not load image.")
                continue

            # Convert BGR (OpenCV default) to RGB
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            
            # 2. Reshape and Normalize Pixel Data
            # Reshape the image array to a list of pixels (N rows, 3 columns for R, G, B)
            pixels = img.reshape((-1, 3))
            pixels = np.float32(pixels)

            # 3. Apply K-Means Clustering (using MiniBatchKMeans for even more speed)
            # This finds 'num_colors' representative colors
            kmeans = MiniBatchKMeans(n_clusters=num_colors, n_init='auto', random_state=42)
            kmeans.fit(pixels)
            
            # The cluster centers are the dominant colors
            dominant_colors = kmeans.cluster_centers_

            # 4. Format Output: Convert float RGB (0-255) to integer list
            # The result is a list of [R, G, B] lists
            color_list = dominant_colors.astype(int).tolist()
            
            all_dominant_colors[frame_num_str] = color_list
            print(f"Processed frame {frame_num_str}: Found {len(color_list)} colors.")

        except Exception as e:
            print(f"Error processing frame {frame_num_str}: {e}")
            
    # 5. Output to JSON
    json_output = json.dumps(all_dominant_colors, indent=4)
    return json_output

# --- Example Usage ---
# NOTE: Replace 'read_node_file_path' with the actual file sequence path.
# Example path with Nuke-style padding:
# read_node_file_path = '/path/to/my/comic/scene_1.shot_01.%04d.exr'
# frames_to_process = list(range(101, 201))  # Process frames 101 through 200

# # Run the function
# json_data = extract_dominant_colors_sequence(read_node_file_path, num_colors=30, frames=frames_to_process)

# # Save the JSON file
# with open('comic_palette.json', 'w') as f:
#     f.write(json_data)

</pre>

<div id="Reading Colors to JSON-The OpenImageIO (OIIO) Option"><h2 id="The OpenImageIO (OIIO) Option" class="header"><a href="#Reading Colors to JSON-The OpenImageIO (OIIO) Option">The OpenImageIO (OIIO) Option</a></h2></div>

<p>
You specifically mentioned OpenImageIO. This is an excellent idea, especially if your pipeline uses complex file formats like OpenEXR, TIFF, or DPX.
</p>

<ul>
<li>
<span id="Reading Colors to JSON-The OpenImageIO (OIIO) Option-Integration"></span><strong id="Integration">Integration</strong>: Use the Python bindings for OIIO (import OpenImageIO as OIIO).

<li>
<span id="Reading Colors to JSON-The OpenImageIO (OIIO) Option-Fast Reading"></span><strong id="Fast Reading">Fast Reading</strong>: Replace cv2.imread(file_path) with OIIO code to read the image data into a NumPy array (which OIIO handles natively).

<li>
<span id="Reading Colors to JSON-The OpenImageIO (OIIO) Option-Process"></span><strong id="Process">Process</strong>: Pass the NumPy array directly to the K-Means clustering step (as shown above).

</ul>

<p>
This separates the fast I/O (OIIO) from the fast computation (NumPy/Scikit-learn), maximizing efficiency.
</p>

<div id="Reading Colors to JSON-The OpenImageIO (OIIO) Option-OIIO Replacement for cv2.imread:"><h3 id="OIIO Replacement for cv2.imread:" class="header"><a href="#Reading Colors to JSON-The OpenImageIO (OIIO) Option-OIIO Replacement for cv2.imread:">OIIO Replacement for cv2.imread:</a></h3></div>

<pre python>
import OpenImageIO as OIIO
import numpy as np
# ... (rest of your imports)

def read_image_with_oiio(file_path):
    """Loads an image into a NumPy array using OpenImageIO."""
    
    # 1. Read the image and spec
    img_input = OIIO.ImageInput.open(file_path)
    if not img_input:
        raise IOError(f"Could not open image with OIIO: {OIIO.geterror()}")

    spec = img_input.spec()
    
    # 2. Create NumPy array and read pixel data
    # Read as float type and get R, G, B channels
    pixel_data = np.empty((spec.height, spec.width, spec.nchannels), dtype=np.float32)
    img_input.read_image(0, 0, spec.nchannels, OIIO.TypeDesc.FLOAT, pixel_data)
    img_input.close()
    
    # Only return the first 3 channels (R, G, B), dropping Alpha if present
    return (pixel_data[:, :, :3] * 255.0).astype(np.uint8) # Convert to 0-255 range for clustering
</pre>

<p>
You would then call this function within your main loop and feed the resulting array to the clustering steps.
</p>

<div id="Reading Colors to JSON-QUESTION"><h2 id="QUESTION" class="header"><a href="#Reading Colors to JSON-QUESTION">QUESTION</a></h2></div>

<table>
<tr>
<td>
I won't know the number of colors for each image sequence, but I know they will be relatively low in number. How can I change the code to expect any number of colors?
</td>
</tr>
</table>

<p>
That's a key challenge when using K-Means Clustering! If you don't know the optimal number of clusters (K), you need an automated way to estimate it for each frame.
</p>

<p>
You can modify the code to use the Elbow Method to dynamically determine the best K for a given image, making the process adaptive to your comic book style sequences.
</p>

<div id="Reading Colors to JSON-The Strategy: The Elbow Method üìê"><h2 id="The Strategy: The Elbow Method üìê" class="header"><a href="#Reading Colors to JSON-The Strategy: The Elbow Method üìê">The Strategy: The Elbow Method üìê</a></h2></div>

<p>
The Elbow Method is a heuristic used to find the optimal number of clusters (K) in K-Means.
</p>

<ul>
<li>
<span id="Reading Colors to JSON-The Strategy: The Elbow Method üìê-Calculate Distortion"></span><strong id="Calculate Distortion">Calculate Distortion</strong>: Run K-Means multiple times, increasing K (e.g., from 5 to 50). For each run, record the distortion (the sum of squared distances between each point and its assigned centroid).

<li>
<span id="Reading Colors to JSON-The Strategy: The Elbow Method üìê-Find the &quot;Elbow&quot;"></span><strong id="Find the &quot;Elbow&quot;">Find the "Elbow"</strong>: Plot K (x-axis) vs. Distortion (y-axis). The distortion will decrease sharply as K increases, then slow down dramatically. The point where the curve bends, resembling an "elbow," is the optimal number of clusters, as adding more clusters provides diminishing returns.

</ul>

<p>
Since your color counts are low (around 30), we can define a reasonable search range for K.
</p>

<div id="Reading Colors to JSON-Modified Python Code with Dynamic K-Selection"><h2 id="Modified Python Code with Dynamic K-Selection" class="header"><a href="#Reading Colors to JSON-Modified Python Code with Dynamic K-Selection">Modified Python Code with Dynamic K-Selection</a></h2></div>

<p>
This modified script uses the scipy.spatial.distance.cdist function (often necessary for distortion calculation) and the MiniBatchKMeans inertia (which is equivalent to distortion) over a range of possible K values.
</p>

<pre python>
import numpy as np
import cv2  # or OIIO, as discussed
from sklearn.cluster import MiniBatchKMeans
from scipy.spatial.distance import cdist
import json
import glob
import os

def determine_optimal_k(pixels_data, k_range=(5, 50)):
    """
    Uses the Elbow Method (via inertia) to find the optimal number of clusters (K).
    
    Args:
        pixels_data (np.ndarray): Reshaped image pixels (N, 3).
        k_range (tuple): (min K, max K) to search within.
        
    Returns:
        int: The estimated optimal number of clusters.
    """
    k_min, k_max = k_range
    inertias = []
    
    # Run MiniBatchKMeans for a range of K values
    for k in range(k_min, k_max + 1, 5): # Check every 5th K for speed
        kmeans = MiniBatchKMeans(n_clusters=k, n_init='auto', random_state=42)
        kmeans.fit(pixels_data)
        inertias.append(kmeans.inertia_)
    
    # --- Simplified Elbow Detection ---
    # This is often done visually, but we can approximate by looking for the 
    # point where the rate of decrease in inertia slows down.
    
    # Calculate the percentage change in inertia
    changes = np.diff(inertias)
    percent_drops = np.abs(changes / inertias[:-1])
    
    # Find the index where the drop (gain in information) falls below a threshold (e.g., 5%)
    # This threshold may need adjustment based on your image style.
    threshold = 0.05 
    
    # Add 1 to the index because diff/changes array is one element shorter
    elbow_index = np.where(percent_drops &lt; threshold)[0]
    
    if len(elbow_index) &gt; 0:
        # The elbow is typically the first index where the drop slows down
        return k_range[0] + (elbow_index[0] + 1) * 5 
    else:
        # If no elbow is clearly found, default to the max K in the range
        return k_max

def extract_dominant_colors_sequence_dynamic(sequence_path, k_search_range=(5, 50), frames=None):
    """
    Extracts dominant colors, dynamically determining the optimal K for each frame.
    """
    all_dominant_colors = {}
    
    # --- Setup file/frame list logic (omitted for brevity, assume same as before) ---
    # NOTE: You'd implement your file iteration here. For this example, we'll
    # assume the input is a list of file paths named 'file_frame_pairs'.
    
    # Placeholder for file iteration based on a single sample frame:
    
    # Example to load the first file in the sequence as a test:
    # file_list = sorted(glob.glob(sequence_path.replace('%04d', '*')))
    # file_frame_pairs = zip(file_list, [os.path.basename(f).split('.')[1] for f in file_list])
    
    # --- Main Loop ---
    # We will assume a single image file for demonstration
    file_path = "path/to/single/image.jpg" # Replace with actual file iteration
    frame_num_str = "001" 
    
    # 1. Load Image
    img = cv2.imread(file_path) 
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    pixels = img.reshape((-1, 3))
    pixels = np.float32(pixels)

    # 2. Determine Optimal K
    optimal_k = determine_optimal_k(pixels, k_search_range)
    print(f"Frame {frame_num_str}: Optimal K estimated at {optimal_k}")

    # 3. Final K-Means with Optimal K
    kmeans = MiniBatchKMeans(n_clusters=optimal_k, n_init='auto', random_state=42)
    kmeans.fit(pixels)
    dominant_colors = kmeans.cluster_centers_

    # 4. Format Output
    color_list = dominant_colors.astype(int).tolist()
    all_dominant_colors[frame_num_str] = color_list
        
    json_output = json.dumps(all_dominant_colors, indent=4)
    return json_output

# --- Considerations ---
# 1. The range for K_RANGE (5, 50) is a guess. Adjust based on how many
#    unique colors your images actually contain.
# 2. The ELBOW DETECTION THRESHOLD (0.05) is a heuristic. It may need tuning.
# 3. For faster processing, you might want to run the K-Means on a 
#    *downscaled* version of the image (e.g., resize to 500x500 pixels) 
#    before calculating the elbow and the final colors.
</pre>

<div id="Reading Colors to JSON-QUESTION"><h2 id="QUESTION" class="header"><a href="#Reading Colors to JSON-QUESTION">QUESTION</a></h2></div>

<table>
<tr>
<td>
I believe the RGB values would need to be in the same units/representation as used in Nuke - zero to one for standard black to white (while HDR images are possible). Would I forego the conversion to 0 - 255?
</td>
</tr>
</table>

<p>
Yes, you are absolutely correct and should forego the conversion to the 0-255 range if you intend to use the resulting RGB values directly within Nuke (or any standard VFX application). üí°
</p>

<p>
Nuke and OpenEXR/HDR pipelines natively operate on floating-point color values where standard dynamic range is represented by 0.0 to 1.0.
</p>

<div id="Reading Colors to JSON-The Necessary Adjustment to the Code"><h2 id="The Necessary Adjustment to the Code" class="header"><a href="#Reading Colors to JSON-The Necessary Adjustment to the Code">The Necessary Adjustment to the Code</a></h2></div>

<p>
You need to ensure two things in your processing pipeline:
</p>
<ul>
<li>
<span id="Reading Colors to JSON-The Necessary Adjustment to the Code-Image Read"></span><strong id="Image Read">Image Read</strong>: The image data is loaded directly as floating-point numbers.

<li>
<span id="Reading Colors to JSON-The Necessary Adjustment to the Code-K-Means Processing"></span><strong id="K-Means Processing">K-Means Processing</strong>: The clustering algorithm runs on the 0.0‚àí1.0 floating-point data.

<li>
<span id="Reading Colors to JSON-The Necessary Adjustment to the Code-Output"></span><strong id="Output">Output</strong>: The final cluster centers are outputted as floating-point numbers.

</ul>

<p>
If you are using OpenImageIO (OIIO), this is much simpler as it defaults to reading data as float (or whatever the file precision is) and gives you the 0.0‚àí1.0 data naturally. If you are using OpenCV or Pillow, you must explicitly ensure the conversion to float and division by 255 does not happen, or handle the data type correctly.
</p>

<div id="Reading Colors to JSON-Corrected Code Snippet using OIIO (Recommended)"><h2 id="Corrected Code Snippet using OIIO (Recommended)" class="header"><a href="#Reading Colors to JSON-Corrected Code Snippet using OIIO (Recommended)">Corrected Code Snippet using OIIO (Recommended)</a></h2></div>

<p>
Since OIIO is designed for VFX, it's the most reliable for float data.
</p>

<pre python>
import OpenImageIO as OIIO
import numpy as np
# ... (rest of your imports)

def read_image_with_oiio(file_path):
    """Loads an image into a NumPy array using OpenImageIO as float32 (0.0 - 1.0 range)."""
    
    img_input = OIIO.ImageInput.open(file_path)
    if not img_input:
        raise IOError(f"Could not open image with OIIO: {OIIO.geterror()}")

    spec = img_input.spec()
    
    # Read pixel data directly as float32
    # OIIO reads data in its native float range (0.0-1.0 or HDR range for EXR)
    pixel_data = np.empty((spec.height, spec.width, spec.nchannels), dtype=np.float32)
    img_input.read_image(0, 0, spec.nchannels, OIIO.TypeDesc.FLOAT, pixel_data)
    img_input.close()
    
    # Return R, G, B channels in 0.0-1.0 (or HDR float) range
    return pixel_data[:, :, :3] # Keep R, G, B and discard others

# --- The clustering function remains the same, but operates on float data ---

# Inside extract_dominant_colors_sequence_dynamic:
# 1. Load Image
img_float_data = read_image_with_oiio(file_path) 

# 2. Reshape Pixel Data (already float, no normalization needed)
pixels = img_float_data.reshape((-1, 3))
# pixels is now np.float32 in 0.0-1.0+ range
# ... (Proceed with determine_optimal_k and MiniBatchKMeans on 'pixels')

# 3. Final Output (NO .astype(int) conversion!)
# Keep the cluster centers as a float list for JSON output
# color_list = dominant_colors.astype(int).tolist()  &lt;-- DELETE THIS LINE
color_list = dominant_colors.tolist() 
</pre>

<div id="Reading Colors to JSON-Corrected Code Snippet using OpenCV (If Used)"><h2 id="Corrected Code Snippet using OpenCV (If Used)" class="header"><a href="#Reading Colors to JSON-Corrected Code Snippet using OpenCV (If Used)">Corrected Code Snippet using OpenCV (If Used)</a></h2></div>

<p>
If you must use OpenCV (or Pillow), the key is to load the image and manually convert to float in the correct range, then bypass any conversion back to integer.
</p>

<pre python>
# Using OpenCV
img = cv2.imread(file_path) 
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

# Convert to float and normalize to 0.0 - 1.0 range
# Assumes the input image is 8-bit (0-255). If it's a 16-bit TIFF, this is wrong.
pixels = img.reshape((-1, 3))
pixels = np.float32(pixels) / 255.0  
# For true HDR/float files, you must use OIIO or specialized readers like OpenEXR.

# ... (Proceed with clustering)

# Final Output (still must output as float list)
color_list = dominant_colors.tolist() 
</pre>

<div id="Reading Colors to JSON-Corrected Code Snippet using OpenCV (If Used)-Crucial Takeaway:"><h3 id="Crucial Takeaway:" class="header"><a href="#Reading Colors to JSON-Corrected Code Snippet using OpenCV (If Used)-Crucial Takeaway:">Crucial Takeaway:</a></h3></div>

<p>
By removing the .astype(int) and .astype(int) conversions and ensuring the input array is already in the float (0.0 - 1.0+) range, your resulting JSON will contain values ready to be pasted directly into a Nuke node's RGB color field.
</p>

<div id="Reading Colors to JSON-QUESTION"><h2 id="QUESTION" class="header"><a href="#Reading Colors to JSON-QUESTION">QUESTION</a></h2></div>

<table>
<tr>
<td>
Can Open Image IO read data from TGA and PNG files?
</td>
</tr>
</table>

<p>
Yes, OpenImageIO (OIIO) can read data from both TGA (Truevision Targa) and PNG (Portable Network Graphics) files.
</p>

<p>
OIIO is designed specifically for the visual effects (VFX) and animation industry, meaning it supports a wide variety of formats commonly used in production pipelines, which includes these two.
</p>

<div id="Reading Colors to JSON-QUESTION-Key Points for VFX Work:"><h3 id="Key Points for VFX Work:" class="header"><a href="#Reading Colors to JSON-QUESTION-Key Points for VFX Work:">Key Points for VFX Work:</a></h3></div>

<ul>
<li>
<span id="Reading Colors to JSON-QUESTION-Key Points for VFX Work:-TGA Support"></span><strong id="TGA Support">TGA Support</strong>: TGA files are a legacy format but are fully supported by OIIO for both reading and writing.

<li>
<span id="Reading Colors to JSON-QUESTION-Key Points for VFX Work:-PNG Support"></span><strong id="PNG Support">PNG Support</strong>: PNG is widely supported and OIIO handles it reliably.

<li>
<span id="Reading Colors to JSON-QUESTION-Key Points for VFX Work:-Data Handling"></span><strong id="Data Handling">Data Handling</strong>: The main benefit of using OIIO, particularly in your case, is its format-agnostic API and its focus on using floating-point data for internal image representation. When reading, OIIO will automatically convert the integer-based data from 8-bit or 16-bit TGA/PNG files into the 0.0 to 1.0 floating-point range (or higher for HDR files like OpenEXR). This eliminates the need for manual normalization steps like dividing by 255.0

</ul>

<div id="Reading Colors to JSON-QUESTION"><h2 id="QUESTION" class="header"><a href="#Reading Colors to JSON-QUESTION">QUESTION</a></h2></div>

<table>
<tr>
<td>
When these image files are imported into Nuke through the Read nodes, will the colorspace settings affect the values? (In other words, will getting a list of colors from the files be accurate once the images have been ingested or imported into a Nuke scene?
</td>
</tr>
</table>

<p>
Yes, the colorspace settings on the Nuke Read node will absolutely affect the pixel values read into the scene. ‚ö†Ô∏è This is a crucial consideration for your color extraction script.
</p>

<p>
The goal in a Nuke pipeline is usually to convert all incoming image data into a single, standardized Working Colorspace (often linear sRGB/Rec. 709 or ACEScg). The Read node performs this conversion.
</p>

<div id="Reading Colors to JSON-QUESTION-How Nuke Read Nodes Affect Pixel Values"><h3 id="How Nuke Read Nodes Affect Pixel Values" class="header"><a href="#Reading Colors to JSON-QUESTION-How Nuke Read Nodes Affect Pixel Values">How Nuke Read Nodes Affect Pixel Values</a></h3></div>

<p>
The Nuke Read node is responsible for two main actions:
</p>
<ul>
<li>
<span id="Reading Colors to JSON-QUESTION-How Nuke Read Nodes Affect Pixel Values-Reading Data"></span><strong id="Reading Data">Reading Data</strong>: It takes the raw digital values from the file (e.g., the 8-bit integer data from a PNG or TGA).

<li>
<span id="Reading Colors to JSON-QUESTION-How Nuke Read Nodes Affect Pixel Values-Colorspace Transformation"></span><strong id="Colorspace Transformation">Colorspace Transformation</strong>: It applies a color transformation to convert the file's native colorspace (e.g., sRGB, Rec. 709, or Log) into the scene's working colorspace (usually linear).

</ul>

<div id="Reading Colors to JSON-QUESTION-The Inaccuracy of Extracting Color Post-Read Node"><h3 id="The Inaccuracy of Extracting Color Post-Read Node" class="header"><a href="#Reading Colors to JSON-QUESTION-The Inaccuracy of Extracting Color Post-Read Node">The Inaccuracy of Extracting Color Post-Read Node</a></h3></div>

<p>
If you run your Python script on the image before it enters Nuke (as intended with OIIO), your script is operating on the file's native colorspace data (e.g., sRGB).
</p>

<p>
If you were to sample the colors inside Nuke (e.g., using an expression or Python in the Nuke script itself), the values you get would be the linearized values.
</p>

<table>
<tr>
<td>
Stage
</td>
<td>
Operation
</td>
<td>
Colorspace of Pixel Values
</td>
<td>
Resulting Values
</td>
</tr>
<tr>
<td>
File on Disk
</td>
<td>
None
</td>
<td>
sRGB (or similar standard)
</td>
<td>
Non-linear, 0.0‚àí1.0 (or 0‚àí255)
</td>
</tr>
<tr>
<td>
Your Python/OIIO Script
</td>
<td>
Normalization (OIIO)
</td>
<td>
sRGB (or similar standard)
</td>
<td>
Non-linear, 0.0‚àí1.0
</td>
</tr>
<tr>
<td>
Nuke Read Node Output
</td>
<td>
Conversion to Linear
</td>
<td>
Linear (Working Space)
</td>
<td>
Linear, 0.0‚àí1.0+
</td>
</tr>
</table>

<div id="Reading Colors to JSON-QUESTION-The Inaccuracy of Extracting Color Post-Read Node-Conclusion for Your Workflow"><h4 id="Conclusion for Your Workflow" class="header"><a href="#Reading Colors to JSON-QUESTION-The Inaccuracy of Extracting Color Post-Read Node-Conclusion for Your Workflow">Conclusion for Your Workflow</a></h4></div>

<p>
The colorspace transformation means that the floating-point RGB values you extract from the raw files (which are in the sRGB/Rec. 709 gamma space) will NOT numerically match the pixel values you see in the Nuke Viewer or are used by Nuke nodes (which are in the linear space), except for pure black (0,0,0) and pure white (1,1,1).
</p>

<div id="Reading Colors to JSON-QUESTION-The Inaccuracy of Extracting Color Post-Read Node-Recommended Solution: Use Linear Data for Clustering"><h4 id="Recommended Solution: Use Linear Data for Clustering" class="header"><a href="#Reading Colors to JSON-QUESTION-The Inaccuracy of Extracting Color Post-Read Node-Recommended Solution: Use Linear Data for Clustering">Recommended Solution: Use Linear Data for Clustering</a></h4></div>

<p>
For maximum accuracy and consistency with the Nuke scene, you should perform your K-Means clustering in linear color space.
</p>

<p>
This means you need to modify your image loading/preprocessing to perform the same colorspace transformation that Nuke is doing:
</p>
<ul>
<li>
<span id="Reading Colors to JSON-QUESTION-The Inaccuracy of Extracting Color Post-Read Node-Recommended Solution: Use Linear Data for Clustering-Read the File (OIIO)"></span><strong id="Read the File (OIIO)">Read the File (OIIO)</strong>: Get the non-linear pixel data in the 0.0‚àí1.0 float range.

<li>
<span id="Reading Colors to JSON-QUESTION-The Inaccuracy of Extracting Color Post-Read Node-Recommended Solution: Use Linear Data for Clustering-Linearize the Data"></span><strong id="Linearize the Data">Linearize the Data</strong>: Apply a colorspace conversion (usually the inverse of the sRGB or Rec. 709 transfer function, which is a power 2.2 or 2.4 function) to put the data into linear light.

<li>
<span id="Reading Colors to JSON-QUESTION-The Inaccuracy of Extracting Color Post-Read Node-Recommended Solution: Use Linear Data for Clustering-Run K-Means"></span><strong id="Run K-Means">Run K-Means</strong>: Cluster the linear data.

<li>
<span id="Reading Colors to JSON-QUESTION-The Inaccuracy of Extracting Color Post-Read Node-Recommended Solution: Use Linear Data for Clustering-Output Colors"></span><strong id="Output Colors">Output Colors</strong>: The resulting cluster centers will be in the scene's linear working space, ready for direct use in Nuke.

</ul>

<div id="Reading Colors to JSON-QUESTION-Code Adjustment for Linearization"><h3 id="Code Adjustment for Linearization" class="header"><a href="#Reading Colors to JSON-QUESTION-Code Adjustment for Linearization">Code Adjustment for Linearization</a></h3></div>

<p>
If you are confident your image files are standard sRGB (which most PNGs and JPEGs are):
</p>

<pre python>
def read_image_and_linearize(file_path):
    # 1. Load Image using OIIO (assumes 0.0-1.0 non-linear data)
    img_non_linear = read_image_with_oiio(file_path) 
    
    # 2. Linearize the data using the inverse sRGB EOTF (roughly a power 2.2 function)
    # The simple power 2.2 is a good approximation for sRGB/Rec. 709:
    # Value_Linear = Value_Non_Linear ^ 2.2
    
    # We apply the power function to only the color channels (R, G, B)
    # np.power(img_non_linear, 2.2) is the standard approximation
    img_linear = np.power(img_non_linear, 2.2) 
    
    return img_linear # The data is now in the Nuke working space
    
# In your main extraction function:
# pixels = img_float_data.reshape((-1, 3))  &lt;-- CHANGE THIS
img_linear_data = read_image_and_linearize(file_path)
pixels = img_linear_data.reshape((-1, 3))
</pre>

<p>
This approach ensures that your extracted dominant colors are numerically equivalent to the colors that Nuke is actually using for compositing, which is usually what you want.
</p>

<div id="Reading Colors to JSON-QUESTION"><h2 id="QUESTION" class="header"><a href="#Reading Colors to JSON-QUESTION">QUESTION</a></h2></div>

<table>
<tr>
<td>
Is there an alternative to using the conventional Python libraries? I don't have the facility to add Numpy to Nuke if that takes another install.
</td>
</tr>
</table>

<p>
That's a very common constraint in production environments! üò´
</p>

<p>
Yes, there is an alternative to using NumPy and scikit-learn's MiniBatchKMeans for K-Means clustering, but it will require writing the clustering logic yourself using only Nuke's native Python libraries and basic data structures.
</p>

<p>
Since you can't install NumPy, you also won't be able to use the scipy.spatial or the numpy.power functions mentioned in the previous solutions.
</p>

<div id="Reading Colors to JSON-QUESTION-Alternative: Pure Python K-Means Clustering"><h3 id="Alternative: Pure Python K-Means Clustering" class="header"><a href="#Reading Colors to JSON-QUESTION-Alternative: Pure Python K-Means Clustering">Alternative: Pure Python K-Means Clustering</a></h3></div>

<p>
You can implement a basic, unoptimized K-Means algorithm using only core Python libraries. This will be slower than the NumPy/scikit-learn version, but it is necessary if external libraries are forbidden.
</p>

<p>
Here's how to adapt the logic to pure Python, including the dynamic K (Elbow Method) and the necessary colorspace conversions.
</p>

<div id="Reading Colors to JSON-Pure Python K-Means Function"><h2 id="Pure Python K-Means Function" class="header"><a href="#Reading Colors to JSON-Pure Python K-Means Function">Pure Python K-Means Function</a></h2></div>

<p>
This function replaces the MiniBatchKMeans from scikit-learn.
</p>

<pre python>
import random

def pure_python_kmeans(pixels, k, max_iterations=30):
    """
    Basic K-Means implementation using pure Python lists.
    
    Args:
        pixels (list of lists): The input pixel data [[R, G, B], ...].
        k (int): The number of clusters (colors).
        max_iterations (int): Maximum loops to run before stopping.
        
    Returns:
        list of lists: The final cluster centers [[R, G, B], ...].
    """
    if not pixels or len(pixels) &lt; k:
        return []

    # 1. Initialize K centroids randomly
    centroids = random.sample(pixels, k)
    
    for i in range(max_iterations):
        # 2. Assignment Step: Create K empty lists for the clusters
        clusters = [[] for _ in range(k)]
        
        # Calculate Euclidean distance (squared distance for speed)
        for pixel in pixels:
            distances = []
            for centroid in centroids:
                # Euclidean distance squared: (R2-R1)^2 + (G2-G1)^2 + (B2-B1)^2
                dist_sq = sum([(p - c)**2 for p, c in zip(pixel, centroid)])
                distances.append(dist_sq)
            
            # Find the index of the closest centroid
            closest_index = distances.index(min(distances))
            clusters[closest_index].append(pixel)
            
        # 3. Update Step: Calculate new centroids
        new_centroids = []
        is_converged = True
        
        for j, cluster in enumerate(clusters):
            if cluster:
                # Calculate the mean of the cluster (the new centroid)
                # Sum the R, G, B components, then divide by the cluster size
                sum_rgb = [sum(dim) for dim in zip(*cluster)]
                new_centroid = [s / len(cluster) for s in sum_rgb]
                
                # Check for convergence (simplified: if any centroid moves significantly)
                # The minimum change threshold is set very low to be robust
                if sum([(p - c)**2 for p, c in zip(new_centroid, centroids[j])]) &gt; 1e-4:
                    is_converged = False
                    
                new_centroids.append(new_centroid)
            else:
                # Handle empty cluster: re-initialize it randomly
                new_centroids.append(random.choice(pixels))
                
        centroids = new_centroids
        if is_converged:
            break
            
    return centroids

# Note: The 'distortion' required for the Elbow Method is the sum of all 'dist_sq' values.
</pre>

<div id="Reading Colors to JSON-Pure Python K-Means Function-Replacing the Elbow Method (Pure Python)"><h3 id="Replacing the Elbow Method (Pure Python)" class="header"><a href="#Reading Colors to JSON-Pure Python K-Means Function-Replacing the Elbow Method (Pure Python)">Replacing the Elbow Method (Pure Python)</a></h3></div>

<p>
Calculating the full Elbow Method in pure Python for a large number of frames would be very slow. Since you know the number of colors is low, you can use a fast heuristic instead of a full cluster analysis:
</p>

<div id="Reading Colors to JSON-Pure Python K-Means Function-Replacing the Elbow Method (Pure Python)-Heuristic Alternative: Fixed K-range and a Simple Threshold"><h4 id="Heuristic Alternative: Fixed K-range and a Simple Threshold" class="header"><a href="#Reading Colors to JSON-Pure Python K-Means Function-Replacing the Elbow Method (Pure Python)-Heuristic Alternative: Fixed K-range and a Simple Threshold">Heuristic Alternative: Fixed K-range and a Simple Threshold</a></h4></div>

<p>
Instead of relying on the mathematical "elbow," you could define a narrow, reasonable search range (e.g., K=10 to K=30) and simply check a few common K values, perhaps picking the one that provides a high-enough compression ratio or the one that minimizes the distortion per pixel most effectively.
</p>

<p>
A simpler approach for production is to pre-set a maximum K value (e.g., K=30) and rely on the fact that K-Means will naturally group similar colors even if you request more clusters than strictly necessary.
</p>
<ul>
<li>
<span id="Reading Colors to JSON-Pure Python K-Means Function-Replacing the Elbow Method (Pure Python)-Heuristic Alternative: Fixed K-range and a Simple Threshold-Recommendation"></span><strong id="Recommendation">Recommendation</strong>: If performance is critical, use a fixed, slightly generous K (like K=30) and skip the dynamic Elbow Method entirely.

</ul>

<div id="Reading Colors to JSON-Pure Python K-Means Function-Pure Python Colorspace and JSON Handling"><h3 id="Pure Python Colorspace and JSON Handling" class="header"><a href="#Reading Colors to JSON-Pure Python K-Means Function-Pure Python Colorspace and JSON Handling">Pure Python Colorspace and JSON Handling</a></h3></div>

<p>
You will replace NumPy's methods with pure Python math.
</p>

<pre python>
# Assuming you have the linear RGB color list from pure_python_kmeans
linear_rgb = [0.88, 0.35, 0.35]

# --- 1. Linear-to-Display Conversion (Replacing numpy.power) ---
# Goal: Value_Display = Value_Linear ^ (1.0 / 2.2)

# Using the built-in pow() function for the gamma correction
gamma = 1.0 / 2.2 

display_rgb = []
for val in linear_rgb:
    # Use max(0, val) for safety, as pow() on a negative linear value is undefined
    display_val = pow(max(0, val), gamma)
    
    # Clip to 0.0 - 1.0, as node colors don't support HDR
    clipped_val = min(1.0, max(0.0, display_val))
    display_rgb.append(clipped_val)
    
# display_rgb is now ready to be packed into the tile_color integer

# --- 2. JSON Handling ---
# The standard Python 'json' library is almost always available in Nuke's environment
# You'd use the 'json' module to load and parse the data exactly as previously shown:
# extracted_colors = json.loads(json_string) 
</pre>

<div id="Reading Colors to JSON-Pure Python K-Means Function-Summary of Changes:"><h3 id="Summary of Changes:" class="header"><a href="#Reading Colors to JSON-Pure Python K-Means Function-Summary of Changes:">Summary of Changes:</a></h3></div>

<table>
<tr>
<td>
Functionality
</td>
<td>
Standard Python (Nuke's Environment)
</td>
<td>
Conventional Python
</td>
</tr>
<tr>
<td>
Clustering
</td>
<td>
Custom pure_python_kmeans function
</td>
<td>
sklearn.cluster.MiniBatchKMeans
</td>
</tr>
<tr>
<td>
Math/Vectors
</td>
<td>
zip, List comprehensions, sum, pow()
</td>
<td>
numpy (arrays and functions)
</td>
</tr>
<tr>
<td>
Elbow Method
</td>
<td>
Pre-set K (e.g., K=30) or custom, slow logic
</td>
<td>
scipy.spatial.distance.cdist (Inertia)
</td>
</tr>
</table>

<div id="Reading Colors to JSON-QUESTION"><h2 id="QUESTION" class="header"><a href="#Reading Colors to JSON-QUESTION">QUESTION</a></h2></div>

<table>
<tr>
<td>
Sorry, I don't think I communicated my question well enough. Your answer is very useful however! I think I can use this information. The better framed question would be, can I implement the colorspace conversion from the JSON color sets and Nuke's Node Graph colors without an additional library in Nuke's standard Python libraries?
</td>
</tr>
</table>

<p>
I apologize for misinterpreting the initial constraint. Your question is perfectly clear now: Can you perform the color space conversion (linear to display) within Nuke's Python environment without using an external library like NumPy?
</p>

<p>
The answer is Yes, absolutely. You can use the built-in math module and core Python functions to achieve the same result as NumPy for the sRGB linearization inverse (the View Transform).
</p>

<div id="Reading Colors to JSON-QUESTION-Pure Python Colorspace Conversion"><h3 id="Pure Python Colorspace Conversion" class="header"><a href="#Reading Colors to JSON-QUESTION-Pure Python Colorspace Conversion">Pure Python Colorspace Conversion</a></h3></div>

<p>
The required conversion is the simple power function:
</p>

<pre>
Value Display=Value Linear 1/2.2 ‚Äã	
</pre>
 
<p>
You can use Python's built-in math.pow() function for this calculation, which is included in Nuke's standard Python installation and requires no external packages.
</p>

<pre python>
import nuke
import math
import json
# You would also need the pure_python_kmeans function if extracting colors within Nuke

def convert_linear_to_display_color(linear_rgb_list):
    """
    Converts a linear [R, G, B] color (0.0-1.0+) to a display-ready 
    [R, G, B] color (0.0-1.0) using the sRGB/Rec. 709 gamma (1/2.2).
    
    Args:
        linear_rgb_list (list): A list of three floats representing the linear RGB values.
        
    Returns:
        list: A list of three floats representing the display-ready RGB values.
    """
    
    # Reciprocal of 2.2 (the gamma value)
    gamma_correction = 1.0 / 2.2 
    
    display_rgb = []
    
    for val in linear_rgb_list:
        # Nuke nodes don't support HDR, so we must clamp positive values.
        # We also ensure the base of pow() is non-negative.
        clamped_linear_val = max(0.0, val)
        
        # Apply the power function
        display_val = math.pow(clamped_linear_val, gamma_correction)
        
        # Clip the result to 0.0 - 1.0 for the 8-bit color space used by tile_color
        clipped_display_val = min(1.0, display_val)
        
        display_rgb.append(clipped_display_val)
        
    return display_rgb

def float_to_packed_int(r, g, b, a=1.0):
    """Packs float RGB values (0.0-1.0) into Nuke's single tile_color integer."""
    r_int = int(r * 255)
    g_int = int(g * 255)
    b_int = int(b * 255)
    a_int = int(a * 255)
    
    # Nuke uses the format 0xAABBGGRR (little-endian)
    return (a_int &lt;&lt; 24) + (b_int &lt;&lt; 16) + (g_int &lt;&lt; 8) + r_int

# --- Example Usage in Nuke ---
# This is a linear red that looks dark and washed out if used directly
linear_red = [0.8, 0.1, 0.1] 

# Convert to the color that visually matches the Nuke Viewer
display_red = convert_linear_to_display_color(linear_red) 

# Pack the color for the node property
r, g, b = display_red[0], display_red[1], display_red[2]
packed_color = float_to_packed_int(r, g, b)

# node = nuke.selectedNode() 
# node['tile_color'].setValue(packed_color)
</pre>

<div id="Reading Colors to JSON-QUESTION-Using Nuke's Native Color Functions (Advanced)"><h3 id="Using Nuke's Native Color Functions (Advanced)" class="header"><a href="#Reading Colors to JSON-QUESTION-Using Nuke's Native Color Functions (Advanced)">Using Nuke's Native Color Functions (Advanced)</a></h3></div>

<p>
If you need a more robust, OCIO-aware solution‚Äîone that respects Nuke's actual project settings (which may not be a simple power 2.2)‚Äîyou can use Nuke's internal color conversion functions. This is the most accurate way to match the Nuke Viewer, as it utilizes the exact Viewer Process configuration.
</p>

<p>
The function you would use is nuke.to and nuke.from:
</p>

<pre python>
# WARNING: This method is highly accurate but slightly more complex.
# The 'to' and 'from' commands are primarily used for getting/setting metadata,
# but can be adapted for color conversion if needed, though the simple 
# math.pow() is usually sufficient for tile_color.

# For simplicity and performance, stick to the math.pow() method above 
# unless your pipeline uses a custom view transform that deviates significantly from sRGB.
</pre>

<p>
<span id="Reading Colors to JSON-QUESTION-Using Nuke's Native Color Functions (Advanced)-Conclusion"></span><strong id="Conclusion">Conclusion</strong>: Stick with the math.pow() approach for setting node colors. It's clean, fast, accurate enough for the visual purpose of node coloring, and relies only on the Python modules guaranteed to be available in Nuke.
</p>

<div id="Reading Colors to JSON-QUESTION"><h2 id="QUESTION" class="header"><a href="#Reading Colors to JSON-QUESTION">QUESTION</a></h2></div>

<table>
<tr>
<td>
To summarize your implementation of the Python standard library method to collect the color data, could you give me the complete script again in its entirety?
</td>
</tr>
</table>

<p>
Certainly. To summarize the pure Python standard library method, I'll provide the complete script, including:
</p>

<ul>
<li>
The OIIO image loading (as it's essential for correct Nuke-compatible float data).

<li>
The pure Python K-Means clustering function.

<li>
The dynamic K determination is removed (as it's too slow without NumPy) and replaced with a fixed K.

<li>
The necessary colorspace conversions (Linearization for clustering, and Display conversion for the Nuke node).

</ul>

<p>
This script is self-contained and avoids external dependencies like NumPy or scikit-learn.
</p>

<div id="Reading Colors to JSON-QUESTION-Complete Pure Python Color Extraction Script (Nuke-Ready)"><h3 id="Complete Pure Python Color Extraction Script (Nuke-Ready)" class="header"><a href="#Reading Colors to JSON-QUESTION-Complete Pure Python Color Extraction Script (Nuke-Ready)">Complete Pure Python Color Extraction Script (Nuke-Ready)</a></h3></div>

<p>
This script is designed to run in a standalone Python environment that has OpenImageIO (OIIO) installed for robust float image reading, but uses only core Python and the math module for all clustering and colorspace conversions.
</p>

<pre python>
import OpenImageIO as OIIO
import math
import random
import json
import os
import glob
from collections import defaultdict # Useful for efficiently building the clusters

# --- 1. Pure Python K-Means Core ---

def pure_python_kmeans(pixels, k, max_iterations=50):
    """
    Basic K-Means implementation using pure Python lists.
    
    Args:
        pixels (list of lists): The input pixel data [[R, G, B], ...].
        k (int): The number of clusters (colors).
        max_iterations (int): Maximum loops to run before stopping.
        
    Returns:
        list of lists: The final cluster centers [[R, G, B], ...].
    """
    if not pixels or len(pixels) &lt; k:
        # Return pure black if not enough pixels to meet K
        return [[0.0, 0.0, 0.0]] * k

    # Initialize K centroids randomly
    centroids = random.sample(pixels, k)
    
    for i in range(max_iterations):
        # Assignment Step: Assign each pixel to the nearest centroid
        clusters = defaultdict(list)
        
        for pixel in pixels:
            distances = []
            for centroid in centroids:
                # Euclidean distance squared
                dist_sq = sum([(p - c)**2 for p, c in zip(pixel, centroid)])
                distances.append(dist_sq)
            
            # Find the index of the closest centroid
            closest_index = distances.index(min(distances))
            clusters[closest_index].append(pixel)
            
        # Update Step: Calculate new centroids
        new_centroids = []
        is_converged = True
        
        for j in range(k):
            cluster = clusters[j]
            if cluster:
                # Calculate the mean of the cluster (the new centroid)
                # zip(*cluster) transposes the list: [[R1, G1, B1], [R2, G2, B2], ...] -&gt; [(R1, R2, ...), (G1, G2, ...), ...]
                sum_rgb = [sum(dim) for dim in zip(*cluster)]
                new_centroid = [s / len(cluster) for s in sum_rgb]
                
                # Check for convergence (simplified check of squared distance)
                if sum([(p - c)**2 for p, c in zip(new_centroid, centroids[j])]) &gt; 1e-6:
                    is_converged = False
                    
                new_centroids.append(new_centroid)
            else:
                # Handle empty cluster: re-initialize it randomly
                new_centroids.append(random.choice(pixels))
                
        centroids = new_centroids
        if is_converged:
            # print(f"K-Means converged in {i+1} iterations.")
            break
            
    return centroids


# --- 2. Image and Colorspace Functions (Requires OIIO) ---

def read_image_with_oiio(file_path):
    """Loads an image into a NumPy-like list of pixels (0.0 - 1.0+ range)."""
    img_input = OIIO.ImageInput.open(file_path)
    if not img_input:
        raise IOError(f"Could not open image with OIIO: {OIIO.geterror()}")

    spec = img_input.spec()
    
    # OIIO reads data in its native float range (0.0-1.0 or HDR range)
    # Use OIIO's read_image to get pixel data as float32
    # We use a numpy array temporarily for I/O, but reshape it to a pure list immediately
    import numpy as np # Necessary only for the OIIO numpy integration
    pixel_data_np = np.empty((spec.height, spec.width, spec.nchannels), dtype=np.float32)
    img_input.read_image(0, 0, spec.nchannels, OIIO.TypeDesc.FLOAT, pixel_data_np)
    img_input.close()
    
    # Reshape and convert to a flat list of RGB lists, discarding alpha
    pixels = pixel_data_np[:, :, :3].reshape((-1, 3)).tolist()
    return pixels


def linearize_pixels(pixels):
    """Converts a list of non-linear sRGB/Rec. 709 pixels to linear light (approx. power 2.2)."""
    linear_pixels = []
    
    for r, g, b in pixels:
        # Simple power 2.2 approximation for sRGB/Rec. 709 to linear
        # Value_Linear = Value_Non_Linear ^ 2.2
        linear_r = math.pow(max(0, r), 2.2)
        linear_g = math.pow(max(0, g), 2.2)
        linear_b = math.pow(max(0, b), 2.2)
        linear_pixels.append([linear_r, linear_g, linear_b])
        
    return linear_pixels

# --- 3. Main Extraction Loop ---

def extract_dominant_colors_sequence_pure_python(sequence_path_pattern, k_fixed=30):
    """
    Extracts dominant colors from an image sequence using pure Python K-Means.
    
    Args:
        sequence_path_pattern (str): Glob pattern for the sequence (e.g., '/path/to/img.%04d.png')
        k_fixed (int): The number of clusters to use (fixed K).
        
    Returns:
        str: JSON string containing the extracted linear RGB colors per frame.
    """
    all_dominant_colors = {}
    
    # Simple glob to find all files in the sequence
    file_list = sorted(glob.glob(sequence_path_pattern.replace('%04d', '*')))
    
    for file_path in file_list:
        try:
            # 1. Load Non-Linear Data (0.0-1.0 range)
            non_linear_pixels = read_image_with_oiio(file_path) 
            
            # 2. Linearize Data for Clustering
            linear_pixels = linearize_pixels(non_linear_pixels)
            
            # 3. Perform K-Means on Linear Data
            # Note: This step can be slow for large images/high K
            dominant_colors_linear = pure_python_kmeans(linear_pixels, k_fixed)
            
            # 4. Store Results (as linear floats)
            frame_num = os.path.basename(file_path).split('.')[-2] # Assumes frame number is before extension
            all_dominant_colors[frame_num] = dominant_colors_linear

        except Exception as e:
            print(f"Error processing {file_path}: {e}")
            continue
            
    return json.dumps(all_dominant_colors, indent=4)


# --- 4. Nuke Integration Functions (To be run INSIDE Nuke) ---

def convert_linear_to_display_color(linear_rgb_list):
    """Converts a linear RGB color to a display-ready color (for node 'tile_color')."""
    gamma_correction = 1.0 / 2.2 # The view transform approximation
    display_rgb = []
    
    for val in linear_rgb_list:
        clamped_linear_val = max(0.0, val)
        display_val = math.pow(clamped_linear_val, gamma_correction)
        clipped_display_val = min(1.0, display_val)
        display_rgb.append(clipped_display_val)
        
    return display_rgb

def float_to_packed_int(r, g, b, a=1.0):
    """Packs float RGB values (0.0-1.0) into Nuke's single tile_color integer."""
    r_int = int(r * 255)
    g_int = int(g * 255)
    b_int = int(b * 255)
    a_int = int(a * 255)
    return (a_int &lt;&lt; 24) + (b_int &lt;&lt; 16) + (g_int &lt;&lt; 8) + r_int
    
def set_node_color_from_linear_rgb(node, linear_rgb_color):
    """
    Sets a Nuke node's tile_color based on a linear RGB list, applying conversion.
    
    Args:
        node (nuke.Node): The Nuke node object.
        linear_rgb_color (list): The [R, G, B] color extracted from the JSON.
    """
    display_rgb = convert_linear_to_display_color(linear_rgb_color)
    r, g, b = display_rgb[0], display_rgb[1], display_rgb[2]
    packed_color = float_to_packed_int(r, g, b)
    
    node['tile_color'].setValue(packed_color)
</pre>

<div id="Reading Colors to JSON-QUESTION"><h2 id="QUESTION" class="header"><a href="#Reading Colors to JSON-QUESTION">QUESTION</a></h2></div>

<table>
<tr>
<td>
Can you add some functions that allow for doing this on selected Read nodes, where the enclosing directory of the imported file sequence becomes the output of a JSON file with the color data (and saved with the name of the Read image with a JSON extension)? And perhaps the script could contain functions for selecting a READ node and imported the color data collected from that READ node when the JSON export portion was saved.
</td>
</tr>
</table>

<p>
That's a logical and practical workflow! You want to embed the functionality directly into Nuke, using selected Read nodes as both the source for the image sequence and the destination for the JSON file.
</p>

<p>
I'll add the necessary Nuke integration functions to the script. We'll need a way to save the data from Nuke and another way to apply it in Nuke.
</p>

<div id="Reading Colors to JSON-QUESTION-Complete Pure Python Script with Nuke I/O"><h3 id="Complete Pure Python Script with Nuke I/O" class="header"><a href="#Reading Colors to JSON-QUESTION-Complete Pure Python Script with Nuke I/O">Complete Pure Python Script with Nuke I/O</a></h3></div>
<p>
Here is the complete script. It contains two main operational functions to be executed within Nuke:
</p>

<ul>
<li>
<span id="Reading Colors to JSON-QUESTION-Complete Pure Python Script with Nuke I/O-export_colors_from_selected_read_nodes()"></span><strong id="export_colors_from_selected_read_nodes()">export_colors_from_selected_read_nodes()</strong>: Reads the image sequence path from a selected Read node, performs K-Means clustering, and saves the resulting linear color data as a JSON file in the sequence's directory.

<li>
<span id="Reading Colors to JSON-QUESTION-Complete Pure Python Script with Nuke I/O-apply_colors_to_selected_read_node()"></span><strong id="apply_colors_to_selected_read_node()">apply_colors_to_selected_read_node()</strong>: Reads the JSON file associated with a selected Read node and applies the dominant color of the current frame to the node's tile_color.

</ul>

<p>
Python Script (my_color_tool.py)
</p>

<pre python>
import nuke
import OpenImageIO as OIIO
import math
import random
import json
import os
import glob
from collections import defaultdict

# --- CORE 1: PURE PYTHON K-MEANS ---

def pure_python_kmeans(pixels, k, max_iterations=50):
    """
    Basic K-Means implementation using pure Python lists.
    (See previous response for details on this function's logic).
    """
    if not pixels or len(pixels) &lt; k:
        return [[0.0, 0.0, 0.0]] * k

    # 
    centroids = random.sample(pixels, k)
    
    for i in range(max_iterations):
        clusters = defaultdict(list)
        
        for pixel in pixels:
            distances = []
            for centroid in centroids:
                dist_sq = sum([(p - c)**2 for p, c in zip(pixel, centroid)])
                distances.append(dist_sq)
            
            closest_index = distances.index(min(distances))
            clusters[closest_index].append(pixel)
            
        new_centroids = []
        is_converged = True
        
        for j in range(k):
            cluster = clusters[j]
            if cluster:
                sum_rgb = [sum(dim) for dim in zip(*cluster)]
                new_centroid = [s / len(cluster) for s in sum_rgb]
                
                if sum([(p - c)**2 for p, c in zip(new_centroid, centroids[j])]) &gt; 1e-6:
                    is_converged = False
                    
                new_centroids.append(new_centroid)
            else:
                new_centroids.append(random.choice(pixels))
                
        centroids = new_centroids
        if is_converged:
            break
            
    return centroids


# --- CORE 2: IMAGE AND COLORSPACE FUNCTIONS ---

def read_image_with_oiio(file_path):
    """Loads an image using OpenImageIO for robust float handling."""
    try:
        img_input = OIIO.ImageInput.open(file_path)
        if not img_input:
            raise IOError(f"Could not open image with OIIO: {OIIO.geterror()}")

        spec = img_input.spec()
        # Use numpy temporarily for the OIIO read, then convert to a list
        import numpy as np 
        pixel_data_np = np.empty((spec.height, spec.width, spec.nchannels), dtype=np.float32)
        img_input.read_image(0, 0, spec.nchannels, OIIO.TypeDesc.FLOAT, pixel_data_np)
        img_input.close()
        
        # Reshape and convert to a flat list of RGB lists
        pixels = pixel_data_np[:, :, :3].reshape((-1, 3)).tolist()
        return pixels
    except ImportError:
        nuke.message("Error: NumPy is required temporarily by OpenImageIO for this function, even if not used in the core K-Means logic.")
        return []
    except Exception as e:
        nuke.message(f"OIIO Read Error: {e}")
        return []


def linearize_pixels(pixels):
    """Converts a list of non-linear sRGB/Rec. 709 pixels to linear light (approx. power 2.2)."""
    linear_pixels = []
    for r, g, b in pixels:
        linear_r = math.pow(max(0, r), 2.2)
        linear_g = math.pow(max(0, g), 2.2)
        linear_b = math.pow(max(0, b), 2.2)
        linear_pixels.append([linear_r, linear_g, linear_b])
    return linear_pixels


# --- CORE 3: NUKE I/O FUNCTIONS ---

def get_sequence_info(read_node):
    """Gets the file path, directory, and JSON path from a Read node."""
    file_path_full = read_node['file'].value()
    
    # Handle single frame vs sequence (%04d)
    if '%' in file_path_full:
        # Get the directory and the base file name pattern (e.g., 'image.0001.exr' -&gt; 'image.%04d.exr')
        dir_path = os.path.dirname(file_path_full)
        base_name_pattern = os.path.basename(file_path_full)
        
        # Determine the name for the JSON file (based on the base name without frame numbers)
        # e.g., 'image.%04d.exr' -&gt; 'image.exr' -&gt; 'image.json'
        json_file_name = base_name_pattern.split('.')[0] + ".json"
        
        # Create a list of all frames to process (globbing the directory)
        file_list = sorted(glob.glob(os.path.join(dir_path, base_name_pattern.replace('%04d', '*'))))
    else:
        # Handle single-frame file
        dir_path = os.path.dirname(file_path_full)
        base_name = os.path.basename(file_path_full)
        json_file_name = os.path.splitext(base_name)[0] + ".json"
        file_list = [file_path_full]

    json_path = os.path.join(dir_path, json_file_name)
    
    return file_list, dir_path, json_path

# -------------------------------------------------------------
# FUNCTION 1: EXPORT COLORS FROM SELECTED READ NODES
# -------------------------------------------------------------

def export_colors_from_selected_read_nodes(k_fixed=30):
    """
    Reads selected Read nodes, extracts dominant colors for the sequence,
    and exports a JSON file to the sequence's directory.
    """
    selected_nodes = nuke.selectedNodes('Read')
    if not selected_nodes:
        nuke.message("Please select one or more Read nodes.")
        return

    for node in selected_nodes:
        file_list, dir_path, json_path = get_sequence_info(node)
        
        if not file_list:
            nuke.warning(f"Skipping {node.name()}: Could not find files at {node['file'].value()}")
            continue

        all_dominant_colors = {}
        
        nuke.message(f"Processing {len(file_list)} frames for {node.name()}...")

        for file_path in file_list:
            try:
                # 1. Load, Linearize, and Cluster
                non_linear_pixels = read_image_with_oiio(file_path)
                linear_pixels = linearize_pixels(non_linear_pixels)
                dominant_colors_linear = pure_python_kmeans(linear_pixels, k_fixed)
                
                # 2. Store Results (as linear floats)
                # Frame number extraction is often the part between the last dot and the extension
                frame_num = os.path.basename(file_path).split('.')[-2] 
                all_dominant_colors[frame_num] = dominant_colors_linear

            except Exception as e:
                nuke.warning(f"Error processing frame {file_path}: {e}")
                continue
                
        # 3. Save JSON file to disk
        try:
            with open(json_path, 'w') as f:
                json.dump(all_dominant_colors, f, indent=4)
            nuke.message(f"Successfully exported {k_fixed} linear colors per frame to:\n{json_path}")
        except Exception as e:
            nuke.critical(f"Failed to save JSON file at {json_path}: {e}")


# -------------------------------------------------------------
# FUNCTION 2: APPLY COLORS TO SELECTED READ NODE
# -------------------------------------------------------------

def convert_linear_to_display_color(linear_rgb_list):
    """Converts a linear RGB color to a display-ready color (for node 'tile_color')."""
    gamma_correction = 1.0 / 2.2
    display_rgb = []
    
    for val in linear_rgb_list:
        clamped_linear_val = max(0.0, val)
        display_val = math.pow(clamped_linear_val, gamma_correction)
        clipped_display_val = min(1.0, display_val)
        display_rgb.append(clipped_display_val)
        
    return display_rgb

def float_to_packed_int(r, g, b, a=1.0):
    """Packs float RGB values (0.0-1.0) into Nuke's single tile_color integer."""
    r_int = int(r * 255)
    g_int = int(g * 255)
    b_int = int(b * 255)
    a_int = int(a * 255)
    return (a_int &lt;&lt; 24) + (b_int &lt;&lt; 16) + (g_int &lt;&lt; 8) + r_int

def apply_colors_to_selected_read_node():
    """
    Reads the associated JSON color data and sets the tile_color of the 
    selected Read node based on the most dominant color of the current frame.
    """
    selected_nodes = nuke.selectedNodes('Read')
    if not selected_nodes:
        nuke.message("Please select one or more Read nodes.")
        return

    current_frame = nuke.frame()
    
    for node in selected_nodes:
        file_list, dir_path, json_path = get_sequence_info(node)
        
        if not os.path.exists(json_path):
            nuke.message(f"Color JSON file not found for {node.name()}:\n{json_path}\n\nRun 'Export Colors' first.")
            continue
            
        try:
            with open(json_path, 'r') as f:
                color_data = json.load(f)
            
            # Nuke frames can have leading zeros. Ensure the key matches the JSON.
            frame_key = str(current_frame) 
            
            # Find the first key that matches, handling padding (e.g., '1' vs '0001')
            matching_key = None
            if frame_key in color_data:
                matching_key = frame_key
            else:
                for key in color_data:
                    if key.lstrip('0') == frame_key:
                        matching_key = key
                        break
            
            if not matching_key:
                nuke.warning(f"No color data found for frame {current_frame} in {json_path}")
                continue

            # The most dominant color is the first one in the list [0] from K-Means
            linear_rgb_color = color_data[matching_key][0]
            
            # Set the node's tile_color
            set_node_color_from_linear_rgb(node, linear_rgb_color)
            
        except Exception as e:
            nuke.critical(f"Failed to apply color to {node.name()}: {e}")

# Note: These functions must be sourced or manually executed within a Nuke session.
# For a production setup, you would add these to your Nuke menu.py file.
</pre>

<div id="Reading Colors to JSON-QUESTION"><h2 id="QUESTION" class="header"><a href="#Reading Colors to JSON-QUESTION">QUESTION</a></h2></div>

<table>
<tr>
<td>
Does the standard Python library for Nuke include OpenImageIO?
</td>
</tr>
</table>

<p>
No, the Python module for OpenImageIO (OIIO) is not part of the standard Python library that ships with Nuke.
</p>

<p>
The standard Python library refers to the modules included with the Python language itself (like math, os, json, random, etc.). OpenImageIO is a third-party library built specifically for VFX/image handling.
</p>

<div id="Reading Colors to JSON-QUESTION-OIIO in the Nuke Ecosystem"><h3 id="OIIO in the Nuke Ecosystem" class="header"><a href="#Reading Colors to JSON-QUESTION-OIIO in the Nuke Ecosystem">OIIO in the Nuke Ecosystem</a></h3></div>

<p>
While OIIO is not a standard Python library module, it is compiled and shipped with Nuke as a core component of the application. However, this is usually the C++ library, not the Python bindings.
</p>

<ul>
<li>
<span id="Reading Colors to JSON-QUESTION-OIIO in the Nuke Ecosystem-Nuke Includes the OIIO C++ Library"></span><strong id="Nuke Includes the OIIO C++ Library">Nuke Includes the OIIO C++ Library</strong>: Nuke uses the OIIO core libraries internally to handle image file I/O for its Read and Write nodes.

<li>
<span id="Reading Colors to JSON-QUESTION-OIIO in the Nuke Ecosystem-Python Bindings are Separate"></span><strong id="Python Bindings are Separate">Python Bindings are Separate</strong>: To use OIIO directly in a Python script via import OpenImageIO, you generally need the Python bindings (OpenImageIO module) to be installed and available in Nuke's Python environment. While Foundry includes many third-party libraries for Python (like nuke, PySide2, OpenColorIO, and certain versions of OpenEXR), the OIIO Python bindings (import OpenImageIO) are often not included by default, or their availability can change between Nuke versions and operating systems.

</ul>

<div id="Reading Colors to JSON-QUESTION-OIIO in the Nuke Ecosystem-The functionality in your script that uses:"><h4 id="The functionality in your script that uses:" class="header"><a href="#Reading Colors to JSON-QUESTION-OIIO in the Nuke Ecosystem-The functionality in your script that uses:">The functionality in your script that uses:</a></h4></div>

<pre python>
import OpenImageIO as OIIO 
# ... and uses OIIO.ImageInput, OIIO.TypeDesc, etc.
</pre>

<p>
... is the part that is most likely to require a separate setup or installation (e.g., adding a custom OIIO build to your PYTHONPATH), which contradicts your goal of using only the standard environment.
</p>

<p>
<span id="Reading Colors to JSON-QUESTION-OIIO in the Nuke Ecosystem-The functionality in your script that uses:-Exception"></span><strong id="Exception">Exception</strong>: The search results indicate that Nuke includes a host of third-party libraries (e.g., Nuke 15.0v1 includes OpenImageIO 2.3.13.0). If Nuke's internal build of OIIO includes the Python bindings and correctly exposes them to the Nuke Python interpreter, then your import OpenImageIO line will work.
</p>

<p>
Given your restriction on adding libraries, it's safer to assume the OIIO Python module is not reliably available unless your specific studio pipeline ensures it is present.
</p>

<div id="Reading Colors to JSON-QUESTION"><h2 id="QUESTION" class="header"><a href="#Reading Colors to JSON-QUESTION">QUESTION</a></h2></div>

<p>
With this restriction, can you give a version of the script adapted to work without OpenImageIO?
Yes, absolutely. If you cannot rely on the OpenImageIO Python bindings, the most reliable pure Python alternative for reading pixel data in Nuke is to use Nuke's native Python API to render a single, processed frame of the image sequence into memory.
</p>

<p>
Since Nuke is already doing the heavy lifting of reading the file, managing the colorspace, and processing the pixels for display, we'll harness that existing capability via an in-memory Shuffle and Crop operation.
</p>

<p>
This script uses only the standard Python library modules (math, os, json, etc.) and the core Nuke API (nuke).
</p>

<div id="Reading Colors to JSON-QUESTION-Complete Pure Python Script (Nuke API Method)"><h3 id="Complete Pure Python Script (Nuke API Method)" class="header"><a href="#Reading Colors to JSON-QUESTION-Complete Pure Python Script (Nuke API Method)">Complete Pure Python Script (Nuke API Method)</a></h3></div>

<p>
This script contains the same two primary functions designed to be run from within a Nuke session.
</p>

<p>
Python Script (OIIO-Free)
</p>

<pre python>
import nuke
import math
import random
import json
import os
import glob
from collections import defaultdict

# --- CORE 1: PURE PYTHON K-MEANS ---

def pure_python_kmeans(pixels, k, max_iterations=50):
    """
    Basic K-Means implementation using pure Python lists.
    (This function remains unchanged as it uses only standard Python).
    """
    if not pixels or len(pixels) &lt; k:
        return [[0.0, 0.0, 0.0]] * k

    centroids = random.sample(pixels, k)
    
    for i in range(max_iterations):
        clusters = defaultdict(list)
        
        for pixel in pixels:
            distances = []
            for centroid in centroids:
                dist_sq = sum([(p - c)**2 for p, c in zip(pixel, centroid)])
                distances.append(dist_sq)
            
            closest_index = distances.index(min(distances))
            clusters[closest_index].append(pixel)
            
        new_centroids = []
        is_converged = True
        
        for j in range(k):
            cluster = clusters[j]
            if cluster:
                sum_rgb = [sum(dim) for dim in zip(*cluster)]
                new_centroid = [s / len(cluster) for s in sum_rgb]
                
                if sum([(p - c)**2 for p, c in zip(new_centroid, centroids[j])]) &gt; 1e-6:
                    is_converged = False
                    
                new_centroids.append(new_centroid)
            else:
                new_centroids.append(random.choice(pixels))
                
        centroids = new_centroids
        if is_converged:
            break
            
    return centroids


# --- CORE 2: IMAGE AND COLORSPACE FUNCTIONS (OIIO Replaced) ---

def get_linear_pixels_from_node(read_node, frame):
    """
    Reads linear pixel data directly from a Nuke node for a single frame.
    This replaces the OpenImageIO dependency.
    """
    # Create a temporary, invisible output pipe to sample the pixels
    crop = nuke.nodes.Crop(inputs=[read_node], postage_stamp=False)
    crop['box'].setValue([0, 0, read_node.width(), read_node.height()])
    
    # Force the Read node to the correct colorspace (which should be Linear by default)
    # The Read node's internal processing handles the file-to-linear conversion.
    
    pixels = []
    width = read_node.width()
    height = read_node.height()
    
    try:
        # Request the channel data (R, G, B) as a flat list of floats
        # This data is always in Nuke's internal **linear** working space.
        data = crop.sample('red', 0, 0, width, height, frame=frame)
        data += crop.sample('green', 0, 0, width, height, frame=frame)
        data += crop.sample('blue', 0, 0, width, height, frame=frame)
        
        # Reshape the flat data (R... R, G... G, B... B) into [R, G, B] triplets
        # The length of 'data' is 3 * (width * height)
        num_pixels = width * height
        
        for i in range(num_pixels):
            r = data[i]
            g = data[i + num_pixels]
            b = data[i + 2 * num_pixels]
            pixels.append([r, g, b])
            
    finally:
        # Clean up the temporary node immediately
        nuke.delete(crop)
        
    # No need for a separate 'linearize_pixels' function, as nuke.sample() returns 
    # the data already in the scene's linear working space!
    return pixels

# --- CORE 3: NUKE I/O FUNCTIONS ---

def get_sequence_info(read_node):
    """Gets the file path, directory, and JSON path from a Read node."""
    file_path_full = read_node['file'].value()
    
    if '%' in file_path_full:
        dir_path = os.path.dirname(file_path_full)
        base_name_pattern = os.path.basename(file_path_full)
        
        # Determine the name for the JSON file 
        json_file_name = base_name_pattern.split('.')[0] + ".json"
        
        # Get frame range from the Read node itself
        start = int(read_node['first'].value())
        end = int(read_node['last'].value())
        frame_list = range(start, end + 1)
    else:
        # Handle single-frame file
        dir_path = os.path.dirname(file_path_full)
        base_name = os.path.basename(file_path_full)
        json_file_name = os.path.splitext(base_name)[0] + ".json"
        frame_list = [int(nuke.root()['first_frame'].value())] # Use current project start frame

    json_path = os.path.join(dir_path, json_file_name)
    
    return frame_list, dir_path, json_path

# -------------------------------------------------------------
# FUNCTION 1: EXPORT COLORS FROM SELECTED READ NODES
# -------------------------------------------------------------

def export_colors_from_selected_read_nodes(k_fixed=30):
    """
    Reads selected Read nodes, extracts dominant linear colors for the sequence,
    and exports a JSON file using the Nuke API.
    """
    selected_nodes = nuke.selectedNodes('Read')
    if not selected_nodes:
        nuke.message("Please select one or more Read nodes.")
        return

    for node in selected_nodes:
        frame_list, dir_path, json_path = get_sequence_info(node)
        all_dominant_colors = {}
        
        if not frame_list:
            nuke.warning(f"Skipping {node.name()}: Invalid frame range.")
            continue

        nuke.message(f"Processing {len(frame_list)} frames for {node.name()}...")

        # Set Read node frame to ensure the file path is correct for sampling
        original_frame = nuke.frame() 
        
        for frame in frame_list:
            nuke.frame(frame)
            try:
                # 1. Get Linear Pixel Data using Nuke's API
                linear_pixels = get_linear_pixels_from_node(node, frame)
                
                # 2. Perform K-Means on Linear Data
                dominant_colors_linear = pure_python_kmeans(linear_pixels, k_fixed)
                
                # 3. Store Results (as linear floats)
                all_dominant_colors[str(frame)] = dominant_colors_linear

            except Exception as e:
                nuke.warning(f"Error processing frame {frame} for {node.name()}: {e}")
                continue
        
        # Restore the original frame
        nuke.frame(original_frame)
                
        # 4. Save JSON file to disk
        try:
            with open(json_path, 'w') as f:
                json.dump(all_dominant_colors, f, indent=4)
            nuke.message(f"Successfully exported {k_fixed} linear colors per frame to:\n{json_path}")
        except Exception as e:
            nuke.critical(f"Failed to save JSON file at {json_path}: {e}")


# -------------------------------------------------------------
# FUNCTION 2: APPLY COLORS TO SELECTED READ NODE (Unchanged)
# -------------------------------------------------------------

def convert_linear_to_display_color(linear_rgb_list):
    """Converts a linear RGB color to a display-ready color (for node 'tile_color')."""
    gamma_correction = 1.0 / 2.2
    display_rgb = []
    
    for val in linear_rgb_list:
        clamped_linear_val = max(0.0, val)
        display_val = math.pow(clamped_linear_val, gamma_correction)
        clipped_display_val = min(1.0, display_val)
        display_rgb.append(clipped_display_val)
        
    return display_rgb

def float_to_packed_int(r, g, b, a=1.0):
    """Packs float RGB values (0.0-1.0) into Nuke's single tile_color integer."""
    r_int = int(r * 255)
    g_int = int(g * 255)
    b_int = int(b * 255)
    a_int = int(a * 255)
    return (a_int &lt;&lt; 24) + (b_int &lt;&lt; 16) + (g_int &lt;&lt; 8) + r_int

def set_node_color_from_linear_rgb(node, linear_rgb_color):
    """Sets a Nuke node's tile_color based on a linear RGB list, applying conversion."""
    display_rgb = convert_linear_to_display_color(linear_rgb_color)
    r, g, b = display_rgb[0], display_rgb[1], display_rgb[2]
    packed_color = float_to_packed_int(r, g, b)
    node['tile_color'].setValue(packed_color)

def apply_colors_to_selected_read_node():
    """
    Reads the associated JSON color data and sets the tile_color of the 
    selected Read node based on the most dominant color of the current frame.
    """
    selected_nodes = nuke.selectedNodes('Read')
    if not selected_nodes:
        nuke.message("Please select one or more Read nodes.")
        return

    current_frame = nuke.frame()
    
    for node in selected_nodes:
        frame_list, dir_path, json_path = get_sequence_info(node)
        
        if not os.path.exists(json_path):
            nuke.message(f"Color JSON file not found for {node.name()}:\n{json_path}\n\nRun 'Export Colors' first.")
            continue
            
        try:
            with open(json_path, 'r') as f:
                color_data = json.load(f)
            
            frame_key = str(current_frame) 
            
            # Find the key, handling padded vs unpadded frame numbers
            matching_key = None
            if frame_key in color_data:
                matching_key = frame_key
            else:
                for key in color_data:
                    if key.lstrip('0') == frame_key.lstrip('0'):
                        matching_key = key
                        break
            
            if not matching_key:
                nuke.warning(f"No color data found for frame {current_frame} in {json_path}")
                continue

            # The most dominant color is the first one in the list [0] from K-Means
            linear_rgb_color = color_data[matching_key][0]
            
            # Set the node's tile_color with colorspace conversion
            set_node_color_from_linear_rgb(node, linear_rgb_color)
            
        except Exception as e:
            nuke.critical(f"Failed to apply color to {node.name()}: {e}")
</pre>

<div id="Reading Colors to JSON-QUESTION"><h2 id="QUESTION" class="header"><a href="#Reading Colors to JSON-QUESTION">QUESTION</a></h2></div>

<table>
<tr>
<td>
How much faster than the regular looped 'sample' method would the K-Means method be (using the standard Nuke Python library modules)?
</td>
</tr>
</table>

<p>
That's a great question about performance! However, there's a misunderstanding of how the two methods relate in the context of the script provided.
</p>

<p>
The K-Means method is not an alternative to the pixel sampling method; it's a process that requires the pixel sampling method.
</p>

<p>
Therefore, the K-Means process will be significantly slower than simply sampling pixels, as it involves a complex mathematical clustering calculation on top of the pixel extraction time.
</p>

<div id="Reading Colors to JSON-QUESTION-Comparison of Processing Time"><h3 id="Comparison of Processing Time" class="header"><a href="#Reading Colors to JSON-QUESTION-Comparison of Processing Time">Comparison of Processing Time</a></h3></div>

<p>
Here's a breakdown of the time complexity for the two processes:
</p>

<table>
<tr>
<td>
Process
</td>
<td>
Core Operation
</td>
<td>
Purpose
</td>
<td>
Time Complexity (Relative)
</td>
</tr>
<tr>
<td>
1. Sampling Pixels
</td>
<td>
crop.sample() / Looping
</td>
<td>
Reads all N pixels from a frame into memory.
</td>
<td>
Baseline (Requires O(N) time)
</td>
</tr>
<tr>
<td>
2. K-Means Clustering
</td>
<td>
Iterative centroid calculation
</td>
<td>
Compresses the N pixels into K dominant colors.
</td>
<td>
Much Slower (Requires ‚âàO(T‚ãÖN‚ãÖK) time)
</td>
</tr>
</table>

<p>
Where:
</p>
<ul>
<li>
N is the total number of pixels (Width √ó Height).

<li>
K is the number of clusters (e.g., 30).

<li>
T is the number of iterations (e.g., 50).

</ul>

<div id="Reading Colors to JSON-QUESTION-The Bottleneck: Pixel Extraction"><h3 id="The Bottleneck: Pixel Extraction" class="header"><a href="#Reading Colors to JSON-QUESTION-The Bottleneck: Pixel Extraction">The Bottleneck: Pixel Extraction</a></h3></div>

<p>
The operation crop.sample() is the fundamental I/O and processing step for getting pixel data from the Nuke graph into Python memory. Both the regular looped sample (if you were just logging every pixel) and the K-Means method must perform this step.
</p>

<p>
In the provided script:
</p>

<ul>
<li>
get_linear_pixels_from_node() calls crop.sample() and converts the data structure. This is O(N).

<li>
pure_python_kmeans() then processes this list of N pixels. This is the slowest part of the entire analysis, at about O(T‚ãÖN‚ãÖK).

</ul>

<div id="Reading Colors to JSON-QUESTION-The Bottleneck: Pixel Extraction-Conclusion on Speed"><h4 id="Conclusion on Speed" class="header"><a href="#Reading Colors to JSON-QUESTION-The Bottleneck: Pixel Extraction-Conclusion on Speed">Conclusion on Speed</a></h4></div>

<p>
If you are comparing:
</p>
<ul>
<li>
A basic loop to read and discard all pixels (e.g., just to prove you can access them)

<li>
The full K-Means extraction process (reading pixels + clustering)

</ul>

<p>
The K-Means process will be orders of magnitude slower because it performs complex iterative math (T iterations, checking N points against K centers) on the very large list of N pixels extracted from the node.
</p>

<p>
Using pure Python (without NumPy/SciPy/Scikit-learn) makes the K-Means calculation even slower than it would be in a fully optimized environment, as the math must be done iteratively on lists instead of vectorized operations on arrays.
</p>

<p>
The purpose of K-Means is data reduction (finding the average color), not speed. The speed gain comes later, where you store a small JSON file per frame, saving you from having to run this slow analysis ever again.
</p>

<div id="Reading Colors to JSON-QUESTION"><h2 id="QUESTION" class="header"><a href="#Reading Colors to JSON-QUESTION">QUESTION</a></h2></div>

<table>
<tr>
<td>
Then can you suggest an alternative script (but much the same functionality) where we look through image pixels sequentially, and store the found colors? I would think that skipping pixels (every other pixel or every fourth pixel) would suffice to find all colors in an image sequence (as found in a selected Read Node).
</td>
</tr>
</table>

<p>
That's an excellent approach to drastically improve performance while still gathering color information! You are moving from a complex, iterative clustering method (K-Means) to a fast sampling/histogramming method.
</p>

<p>
The core change will be replacing the slow, iterative pure_python_kmeans function with a much faster Sequential Sampling routine that uses a simple data structure to aggregate unique colors (or near-unique colors).
</p>

<p>
Here is the alternative script incorporating:
</p>
<ul>
<li>
<span id="Reading Colors to JSON-QUESTION-Sequential Pixel Sampling"></span><strong id="Sequential Pixel Sampling">Sequential Pixel Sampling</strong>: Looping through pixels with a stride (e.g., skipping every 4 

</ul>
<p>
th pixel) to reduce N.
</p>
<ul>
<li>
<span id="Reading Colors to JSON-QUESTION-Color Aggregation (Quantization)"></span><strong id="Color Aggregation (Quantization)">Color Aggregation (Quantization)</strong>: To prevent the script from storing millions of slightly different float values, we'll implement a simple form of color quantization (rounding) to group near-identical colors.

<li>
<span id="Reading Colors to JSON-QUESTION-Frequency Analysis"></span><strong id="Frequency Analysis">Frequency Analysis</strong>: The final colors will be sorted by frequency (how many times they were sampled) to identify the "dominant" color, similar to what K-Means provides.

</ul>

<p>
This completely eliminates the O(T‚ãÖN‚ãÖK) complexity of K-Means.
</p>

<div id="Reading Colors to JSON-QUESTION-Alternative Pure Python Script: Sequential Sampling &amp; Quantization"><h3 id="Alternative Pure Python Script: Sequential Sampling &amp; Quantization" class="header"><a href="#Reading Colors to JSON-QUESTION-Alternative Pure Python Script: Sequential Sampling &amp; Quantization">Alternative Pure Python Script: Sequential Sampling &amp; Quantization</a></h3></div>

<p>
This script uses the same Nuke API functions for pixel extraction and colorspace conversion, but replaces the K-Means logic with a much faster sampling and counting method.
</p>

<pre python>
import nuke
import math
import random
import json
import os
import glob
from collections import defaultdict, Counter # Counter is great for frequency analysis

# --- CORE 1: COLOR SAMPLING &amp; QUANTIZATION ---

# Define the quantization level: 
# This determines how close two float colors must be to be counted as the same.
# Higher value = fewer unique colors found (faster, but less accurate for subtle colors).
# A power of 10 is usually a good choice, e.g., 100 rounds to 2 decimal places.
QUANTIZE_FACTOR = 1000

def sample_and_aggregate_colors(linear_pixels, pixel_stride=4, color_quantization_factor=QUANTIZE_FACTOR):
    """
    Sequentially samples pixels, quantizes the linear RGB values, and counts their frequency.
    
    Args:
        linear_pixels (list of lists): The input linear pixel data [[R, G, B], ...].
        pixel_stride (int): Samples every Nth pixel (e.g., 4 = samples 1/4 of pixels).
        color_quantization_factor (int): Multiplier used for rounding float colors to integers.
        
    Returns:
        list of lists: Dominant linear colors, sorted by frequency (most frequent first).
    """
    pixel_count = len(linear_pixels)
    if pixel_count == 0:
        return [[0.0, 0.0, 0.0]]

    quantized_colors = []
    
    # 1. Sample Pixels and Quantize
    for i in range(0, pixel_count, pixel_stride):
        r, g, b = linear_pixels[i]
        
        # Quantization: Multiply by a factor, round to integer, convert to a hashable tuple
        q_r = int(round(r * color_quantization_factor))
        q_g = int(round(g * color_quantization_factor))
        q_b = int(round(b * color_quantization_factor))
        
        quantized_colors.append((q_r, q_g, q_b))
        
    # 2. Count Frequencies and Sort
    # Counter is highly optimized for this.
    color_frequencies = Counter(quantized_colors)
    
    # Sort by frequency (the count, which is the second item in the tuple) in descending order
    sorted_frequencies = color_frequencies.most_common() 
    
    # 3. De-quantize back to linear float RGB and return
    dominant_colors_linear = []
    for (q_r, q_g, q_b), count in sorted_frequencies:
        # Divide by the factor to get the float color back
        linear_r = q_r / color_quantization_factor
        linear_g = q_g / color_quantization_factor
        linear_b = q_b / color_quantization_factor
        
        dominant_colors_linear.append([linear_r, linear_g, linear_b])

    # Return only the top 'k' colors (e.g., first 30) for a consistent JSON output
    # We use a fixed limit (e.g., 30) for consistency with the K-Means script's JSON format.
    # The first element [0] is always the single most dominant color.
    return dominant_colors_linear[:30]


# --- CORE 2 &amp; 3: NUKE API / I/O FUNCTIONS (Mostly Unchanged) ---

def get_linear_pixels_from_node(read_node, frame):
    """Reads ALL linear pixel data directly from a Nuke node (OIIO replacement)."""
    # This function is unchanged from the OIIO-free script
    crop = nuke.nodes.Crop(inputs=[read_node], postage_stamp=False)
    crop['box'].setValue([0, 0, read_node.width(), read_node.height()])
    
    pixels = []
    width = read_node.width()
    height = read_node.height()
    
    try:
        data = crop.sample('red', 0, 0, width, height, frame=frame)
        data += crop.sample('green', 0, 0, width, height, frame=frame)
        data += crop.sample('blue', 0, 0, width, height, frame=frame)
        
        num_pixels = width * height
        
        for i in range(num_pixels):
            r = data[i]
            g = data[i + num_pixels]
            b = data[i + 2 * num_pixels]
            pixels.append([r, g, b])
            
    finally:
        nuke.delete(crop)
        
    return pixels

def get_sequence_info(read_node):
    # This function is unchanged
    file_path_full = read_node['file'].value()
    # ... [rest of get_sequence_info implementation] ...
    # (Leaving the rest of the implementation short for brevity)
    
    if '%' in file_path_full:
        dir_path = os.path.dirname(file_path_full)
        base_name_pattern = os.path.basename(file_path_full)
        json_file_name = base_name_pattern.split('.')[0] + ".json"
        start = int(read_node['first'].value())
        end = int(read_node['last'].value())
        frame_list = range(start, end + 1)
    else:
        dir_path = os.path.dirname(file_path_full)
        base_name = os.path.basename(file_path_full)
        json_file_name = os.path.splitext(base_name)[0] + ".json"
        frame_list = [int(nuke.root()['first_frame'].value())]

    json_path = os.path.join(dir_path, json_file_name)
    return frame_list, dir_path, json_path


# -------------------------------------------------------------
# FUNCTION 1: EXPORT COLORS FROM SELECTED READ NODES
# -------------------------------------------------------------

def export_colors_from_selected_read_nodes(pixel_stride=16):
    """
    Reads selected Read nodes, extracts dominant linear colors using sampling,
    and exports a JSON file.
    """
    selected_nodes = nuke.selectedNodes('Read')
    if not selected_nodes:
        nuke.message("Please select one or more Read nodes.")
        return

    for node in selected_nodes:
        frame_list, dir_path, json_path = get_sequence_info(node)
        all_dominant_colors = {}
        
        if not frame_list:
            nuke.warning(f"Skipping {node.name()}: Invalid frame range.")
            continue

        nuke.message(f"Processing {len(frame_list)} frames for {node.name()} with stride {pixel_stride}...")

        original_frame = nuke.frame() 
        
        for frame in frame_list:
            nuke.frame(frame)
            try:
                # 1. Get ALL Linear Pixel Data from Nuke API
                linear_pixels = get_linear_pixels_from_node(node, frame)
                
                # 2. Sample, Quantize, and Aggregate Dominant Colors
                # Note: We now use the pixel_stride here. For a 4K image (8 million pixels), 
                # a stride of 16 reduces the aggregated list size to 500,000 pixels.
                dominant_colors_linear = sample_and_aggregate_colors(linear_pixels, pixel_stride=pixel_stride)
                
                # 3. Store Results (as linear floats)
                all_dominant_colors[str(frame)] = dominant_colors_linear

            except Exception as e:
                nuke.warning(f"Error processing frame {frame} for {node.name()}: {e}")
                continue
        
        nuke.frame(original_frame)
                
        # 4. Save JSON file to disk
        try:
            with open(json_path, 'w') as f:
                json.dump(all_dominant_colors, f, indent=4)
            nuke.message(f"Successfully exported {len(dominant_colors_linear)} dominant linear colors per frame to:\n{json_path}")
        except Exception as e:
            nuke.critical(f"Failed to save JSON file at {json_path}: {e}")
</pre>

<div id="Reading Colors to JSON-QUESTION-Performance Analysis"><h3 id="Performance Analysis" class="header"><a href="#Reading Colors to JSON-QUESTION-Performance Analysis">Performance Analysis</a></h3></div>

<p>
This new method is dramatically faster than the K-Means approach because:
</p>

<ul>
<li>
Elimination of Iteration: We're no longer looping 50 times to refine centroids. We just do one pass.

<li>
Reduction of Data N: The pixel_stride parameter (e.g., set to 16 in the example) directly cuts the number of items being processed and counted by a factor of 16.

<li>
Optimization with Counter: The collections.Counter module is a highly optimized C implementation in Python, making the counting step extremely fast compared to manual list manipulation.

</ul>

<p>
The main time spent is now in get_linear_pixels_from_node() (Nuke's I/O) and the single loop inside sample_and_aggregate_colors(). This is the best performance you can achieve without stepping outside the standard Nuke Python environment.
</p>

<div id="Reading Colors to JSON-QUESTION"><h2 id="QUESTION" class="header"><a href="#Reading Colors to JSON-QUESTION">QUESTION</a></h2></div>

<table>
<tr>
<td>
I don't really understand where the K-Means method would be of advantage. Could you explain why that might be used over a brute force looped 'sample'? Would I be right in thinking it's a way of getting dominant color values until extra colors with minimal representation would be omitted in the cause of minimal data collection?
</td>
</tr>
</table>

<p>
That's an insightful way to put it! You're absolutely right: the primary advantage of K-Means over a brute-force sampled approach is quality-controlled data reduction, which ensures minimal representation colors are omitted in favor of the truly dominant ones.
</p>

<p>
Here is a breakdown of why K-Means is often preferred, despite being slower.
</p>

<div id="Reading Colors to JSON-K-Means vs. Sampled Quantization"><h2 id="K-Means vs. Sampled Quantization" class="header"><a href="#Reading Colors to JSON-K-Means vs. Sampled Quantization">K-Means vs. Sampled Quantization</a></h2></div>
 
<div id="Reading Colors to JSON-K-Means vs. Sampled Quantization-The Goal of K-Means: Finding True Averages"><h3 id="The Goal of K-Means: Finding True Averages" class="header"><a href="#Reading Colors to JSON-K-Means vs. Sampled Quantization-The Goal of K-Means: Finding True Averages">The Goal of K-Means: Finding True Averages</a></h3></div>

<p>
Your brute-force sampled method (with quantization) is essentially creating a histogram of the pixel values. It tells you that "Red 0.8" appeared 10,000 times, and "Red 0.801" appeared 9,999 times.
</p>

<div id="Reading Colors to JSON-K-Means vs. Sampled Quantization-The Goal of K-Means: Finding True Averages-K-Means (Clustering) goes further:"><h4 id="K-Means (Clustering) goes further:" class="header"><a href="#Reading Colors to JSON-K-Means vs. Sampled Quantization-The Goal of K-Means: Finding True Averages-K-Means (Clustering) goes further:">K-Means (Clustering) goes further:</a></h4></div>

<ul>
<li>
<span id="Reading Colors to JSON-K-Means vs. Sampled Quantization-The Goal of K-Means: Finding True Averages-K-Means (Clustering) goes further:-It calculates the mathematical average"></span><strong id="It calculates the mathematical average">It calculates the mathematical average</strong>: K-Means will recognize that "Red 0.8" and "Red 0.801" are visually the same, and it will combine them into a single, representative color (a centroid). If your image has a large, smooth color gradient (e.g., a blue sky fading from light blue to dark blue), the K-Means algorithm will find 30 distinct, representative shades of blue that are the average of thousands of similar colors along that gradient.

<li>
<span id="Reading Colors to JSON-K-Means vs. Sampled Quantization-The Goal of K-Means: Finding True Averages-K-Means (Clustering) goes further:-It adapts to the data"></span><strong id="It adapts to the data">It adapts to the data</strong>: It doesn't rely on a fixed, arbitrary quantization factor (QUANTIZE_FACTOR). Instead, it dynamically adjusts the centroid positions to minimize the total error (distortion) across the entire image.

</ul>
  
<table>
<tr>
<td>
Scenario
</td>
<td>
Sampled Quantization (Your Method)
</td>
<td>
K-Means Clustering
</td>
</tr>
<tr>
<td>
Smooth Gradient (e.g., sky)
</td>
<td>
You might get 50 different "unique" blues, all very close, because the quantization factor isn't fine enough to group them, or too fine, grouping too few.
</td>
<td>
You explicitly ask for 30 colors (K=30). K-Means guarantees those 30 colors are the most mathematically representative averages of the entire gradient.
</td>
</tr>
<tr>
<td>
Noise/Grain
</td>
<td>
Random noise added to a uniform wall can create thousands of unique, low-frequency colors that clutter your list.
</td>
<td>
The algorithm treats the noisy pixel as a slight deviation from the true average color of the wall, pulling the centroid toward the pure color and effectively filtering the noise.
</td>
</tr>
<tr>
<td>
Dominance
</td>
<td>
Dominance is based purely on the number of times a rounded color value appeared.
</td>
<td>
Dominance is based on the size of the cluster that the centroid represents‚Äîit's a measure of volume and influence, not just raw count.
</td>
</tr>
</table>
 
<div id="Reading Colors to JSON-K-Means vs. Sampled Quantization-When to Use Which Method"><h3 id="When to Use Which Method" class="header"><a href="#Reading Colors to JSON-K-Means vs. Sampled Quantization-When to Use Which Method">When to Use Which Method</a></h3></div>

<table>
<tr>
<td>
Method
</td>
<td>
Performance (Time)
</td>
<td>
Accuracy/Quality of Dominance
</td>
<td>
Best Use Case
</td>
</tr>
<tr>
<td>
Sampled Quantization (Your Method)
</td>
<td>
Very Fast (Best performance)
</td>
<td>
Good enough, but sensitive to noise and quantization factor.
</td>
<td>
When you must have speed, have a low-noise image, or just need the general "ballpark" colors.
</td>
</tr>
<tr>
<td>
K-Means Clustering
</td>
<td>
Slow (High computational cost)
</td>
<td>
High Quality (Mathematically optimized representation).
</td>
<td>
When you need the most accurate, mathematically proven set of dominant colors for critical analysis or art direction.
</td>
</tr>
</table>

<p>
You are absolutely correct that both methods aim to find dominant color values while omitting colors with minimal representation. The difference lies in how they achieve this:
</p>

<ul>
<li>
Sampled Quantization uses a brute-force count and filter based on fixed buckets (quantization).

<li>
K-Means uses an iterative optimization process to mathematically define the best possible buckets (centroids) for a desired K number of colors.

<li>
In a professional VFX pipeline, K-Means (or similar clustering) is often preferred for analysis tasks because the mathematical guarantee of finding the most representative average is worth the upfront computational cost.

</ul>

<div id="Reading Colors to JSON-QUESTION"><h2 id="QUESTION" class="header"><a href="#Reading Colors to JSON-QUESTION">QUESTION</a></h2></div>

<table>
<tr>
<td>
My images will have very strict 'flat' and limited color palette (maybe 50 colors max). I think a brute force approach would be preferred. Do you agree?
</td>
</tr>
</table>

<p>
Yes, I absolutely agree. Given your constraint of having images with a very strict, flat, and limited color palette (50 colors max), the brute-force sampling and quantization approach is highly preferred over K-Means clustering.
</p>

<p>
Here's why, and the one crucial step to ensure its success:
</p>

<div id="Reading Colors to JSON-QUESTION-Why Brute-Force is Better for a Limited Palette"><h3 id="Why Brute-Force is Better for a Limited Palette" class="header"><a href="#Reading Colors to JSON-QUESTION-Why Brute-Force is Better for a Limited Palette">Why Brute-Force is Better for a Limited Palette</a></h3></div>

<p>
The advantage of K-Means‚Äîits ability to find the average representative color in a gradient or noisy area‚Äîbecomes a disadvantage when your palette is already flat and strictly limited.
</p>

<ul>
<li>
<span id="Reading Colors to JSON-QUESTION-Why Brute-Force is Better for a Limited Palette-Speed Dominates"></span><strong id="Speed Dominates">Speed Dominates</strong>: Since your goal is minimal data collection and your images are simple, the performance gain from avoiding the iterative K-Means algorithm far outweighs any marginal quality difference. The brute-force method is significantly faster.

<li>
<span id="Reading Colors to JSON-QUESTION-Why Brute-Force is Better for a Limited Palette-No Averaging Needed"></span><strong id="No Averaging Needed">No Averaging Needed</strong>: Because your colors are "flat" (meaning there are large areas of uniform color rather than smooth gradients), there is little need for K-Means to calculate an average. If a wall is pure Red 0.8, K-Means will find Red 0.8. Your sampled approach will also find Red 0.8. K-Means offers no benefit here, only delay.

<li>
<span id="Reading Colors to JSON-QUESTION-Why Brute-Force is Better for a Limited Palette-Strictly Limited Data"></span><strong id="Strictly Limited Data">Strictly Limited Data</strong>: With a known limit of 50 colors, your brute-force method functions as a perfect histogram generator. It will accurately count the frequency of each of those 50 colors, giving you the true dominance order without the computational overhead of clustering.

</ul>

<div id="Reading Colors to JSON-QUESTION-The Crucial Step: Adjusting Quantization"><h3 id="The Crucial Step: Adjusting Quantization" class="header"><a href="#Reading Colors to JSON-QUESTION-The Crucial Step: Adjusting Quantization">The Crucial Step: Adjusting Quantization</a></h3></div>

<p>
The only potential pitfall with the brute-force approach is setting the quantization factor correctly. Since you have a strict palette, you want the quantization to be fine enough to distinguish between your 50 colors, but coarse enough to group any minor floating-point variations (which happen with compression/decompression).
</p>

<p>
In the previous script:
</p>

<pre>
QUANTIZE_FACTOR=1000
</pre>

<p>
This factor rounds float values (like 0.12345) to an integer based on three decimal places (123).
</p>
<ul>
<li>
If your "flat" colors are exactly uniform (e.g., exactly [0.8, 0.1, 0.1]), a high factor like 10,000 is safe.

<li>
If your "flat" colors have minor variations due to image processing (e.g., one pixel is 0.800001 and the next is 0.799998), a factor of 1000 is usually sufficient to group these into the same bucket.

</ul>

<p>
Recommendation: Stick with a high but reasonable factor like 1000 or 10000 to ensure you capture the discrete, intended colors. You can then also increase the pixel_stride (e.g., from 16 to 32 or 64) for even greater speed, as the colors are guaranteed to be consistent across large areas.
</p>

<div id="Reading Colors to JSON-QUESTION"><h2 id="QUESTION" class="header"><a href="#Reading Colors to JSON-QUESTION">QUESTION</a></h2></div>

<table>
<tr>
<td>
Would adding an artificially hight count limit here help catch a user error where a regular image is accidentally analyzed? Maybe 200 to allow for exceptional cases?
</td>
</tr>
</table>

<p>
That's an excellent idea for robustness and error detection. Since you know your images should contain at most ‚âà50 unique colors, setting a hard upper limit, such as 200, provides a critical safety check without unduly restricting your clean, flat-colored footage.
</p>

<p>
I absolutely recommend adding this limit.
</p>

<div id="Reading Colors to JSON-QUESTION-Recommended Script Adjustment"><h3 id="Recommended Script Adjustment" class="header"><a href="#Reading Colors to JSON-QUESTION-Recommended Script Adjustment">Recommended Script Adjustment</a></h3></div>

<p>
You should modify the sample_and_aggregate_colors function to accept an optional max_colors argument and apply the slice at the end. A limit of 200 is generous enough to handle a highly noisy "flat" plate but restrictive enough to prevent analyzing a full 16.7 million-color image that was accidentally dragged into the graph.
</p>

<p>
Here is the modified function with the added safeguard:
</p>

<pre python>
# Define a robust limit for safety check
SAFEGUARD_COLOR_LIMIT = 200

def sample_and_aggregate_colors(linear_pixels, pixel_stride=16, color_quantization_factor=1000, max_colors=SAFEGUARD_COLOR_LIMIT):
    """
    Sequentially samples pixels, quantizes the linear RGB values, counts frequency,
    and includes a safeguard against analyzing standard, highly-varied images.
    """
    pixel_count = len(linear_pixels)
    if pixel_count == 0:
        return [[0.0, 0.0, 0.0]]

    quantized_colors = []
    
    # 1. Sample Pixels and Quantize
    for i in range(0, pixel_count, pixel_stride):
        r, g, b = linear_pixels[i]
        
        # Quantization: Multiply by a factor, round to integer, convert to a hashable tuple
        q_r = int(round(r * color_quantization_factor))
        q_g = int(round(g * color_quantization_factor))
        q_b = int(round(b * color_quantization_factor))
        
        quantized_colors.append((q_r, q_g, q_b))
        
    # 2. Count Frequencies and Sort
    color_frequencies = Counter(quantized_colors)
    sorted_frequencies = color_frequencies.most_common() 
    
    # 3. De-quantize back to linear float RGB
    dominant_colors_linear = []
    for (q_r, q_g, q_b), count in sorted_frequencies:
        # Divide by the factor to get the float color back
        linear_r = q_r / color_quantization_factor
        linear_g = q_g / color_quantization_factor
        linear_b = q_b / color_quantization_factor
        
        dominant_colors_linear.append([linear_r, linear_g, linear_b])

    # 4. SAFETY CHECK AND LIMITATION:
    # This limits the output to prevent massive JSON files if a user runs the script
    # on a photographic image (which might yield thousands of unique, quantized colors).
    if len(dominant_colors_linear) &gt; max_colors:
        nuke.warning(f"Warning: Image contained {len(dominant_colors_linear)} unique colors. Limiting export to the top {max_colors} colors to prevent excessively large data files.")
        
    return dominant_colors_linear[:max_colors]
</pre>

<div id="Reading Colors to JSON-QUESTION-Benefits of the max_colors=200 Safeguard"><h3 id="Benefits of the max_colors=200 Safeguard" class="header"><a href="#Reading Colors to JSON-QUESTION-Benefits of the max_colors=200 Safeguard">Benefits of the max_colors=200 Safeguard</a></h3></div>

<table>
<tr>
<td>
Benefit
</td>
<td>
Explanation
</td>
</tr>
<tr>
<td>
Error Prevention
</td>
<td>
If a complex, photographic image is used, the brute-force method can still generate thousands of unique, quantized colors, leading to a huge JSON file and a very slow export. Limiting it to 200 prevents this file size bloat.
</td>
</tr>
<tr>
<td>
User Feedback
</td>
<td>
The added nuke.warning provides immediate feedback that the source footage does not conform to the expected "flat color" template.
</td>
</tr>
<tr>
<td>
Guaranteed Structure
</td>
<td>
The downstream function apply_colors_to_selected_read_node is guaranteed to be working with a predictable, finite data structure, enhancing stability.
</td>
</tr>
</table>

<p>
Setting the limit to 200 is a perfect compromise: it's well above your expected 50 colors, but far below the thousands you'd get from a regular production image.
</p>

<div id="Reading Colors to JSON-QUESTION"><h2 id="QUESTION" class="header"><a href="#Reading Colors to JSON-QUESTION">QUESTION</a></h2></div>

<table>
<tr>
<td>
Can you add that updated function into the latest script (so that I don't introduce errors)?
</td>
</tr>
</table>

<p>
Certainly. Here is the complete, updated script.
</p>

<p>
This version replaces the slow K-Means logic with the fast Sampled Quantization method, uses Nuke's Python API (no OpenImageIO dependency), and includes the max_colors=200 safeguard to prevent errors from accidentally analyzing high-detail images.
</p>

<p>
Complete Nuke Script (Sampled Quantization)
</p>

<pre python>
import nuke
import math
import random
import json
import os
import glob
from collections import defaultdict, Counter 

# --- CONFIGURATION CONSTANTS ---
PIXEL_STRIDE = 16          # Samples 1/16th of the pixels for speed
QUANTIZE_FACTOR = 1000     # Groups colors by rounding to 3 decimal places (0.001)
SAFEGUARD_COLOR_LIMIT = 200 # Max number of unique colors to save in the JSON

# --- CORE 1: COLOR SAMPLING &amp; QUANTIZATION (UPDATED) ---

def sample_and_aggregate_colors(linear_pixels, pixel_stride=PIXEL_STRIDE, color_quantization_factor=QUANTIZE_FACTOR, max_colors=SAFEGUARD_COLOR_LIMIT):
    """
    Sequentially samples pixels, quantizes the linear RGB values, counts frequency,
    and includes a safeguard against analyzing standard, highly-varied images.
    """
    pixel_count = len(linear_pixels)
    if pixel_count == 0:
        return [[0.0, 0.0, 0.0]]

    quantized_colors = []
    
    # 1. Sample Pixels and Quantize
    for i in range(0, pixel_count, pixel_stride):
        r, g, b = linear_pixels[i]
        
        # Quantization: Multiply by a factor, round to integer, convert to a hashable tuple
        q_r = int(round(r * color_quantization_factor))
        q_g = int(round(g * color_quantization_factor))
        q_b = int(round(b * color_quantization_factor))
        
        quantized_colors.append((q_r, q_g, q_b))
        
    # 2. Count Frequencies and Sort (Most dominant first)
    color_frequencies = Counter(quantized_colors)
    sorted_frequencies = color_frequencies.most_common() 
    
    # 3. De-quantize back to linear float RGB
    dominant_colors_linear = []
    for (q_r, q_g, q_b), count in sorted_frequencies:
        # Divide by the factor to get the float color back
        linear_r = q_r / color_quantization_factor
        linear_g = q_g / color_quantization_factor
        linear_b = q_b / color_quantization_factor
        
        dominant_colors_linear.append([linear_r, linear_g, linear_b])

    # 4. SAFETY CHECK AND LIMITATION:
    # If the number of unique colors exceeds the safeguard limit, issue a warning 
    # and only return the top 'max_colors'.
    if len(dominant_colors_linear) &gt; max_colors:
        nuke.warning(f"Warning: Image contained {len(dominant_colors_linear)} unique colors. Limiting export to the top {max_colors} colors to prevent excessively large data files.")
        
    # Returns all colors found (sorted by dominance), sliced to the safety limit.
    return dominant_colors_linear[:max_colors]


# --- CORE 2: NUKE API PIXEL EXTRACTION ---

def get_linear_pixels_from_node(read_node, frame):
    """
    Reads ALL linear pixel data directly from a Nuke node for a single frame.
    Uses nuke.sample() which is in the scene's linear working space.
    """
    # Create a temporary, invisible output pipe to sample the pixels
    crop = nuke.nodes.Crop(inputs=[read_node], postage_stamp=False)
    crop['box'].setValue([0, 0, read_node.width(), read_node.height()])
    
    pixels = []
    width = read_node.width()
    height = read_node.height()
    
    try:
        # Request the channel data (R, G, B) as a flat list of floats
        data = crop.sample('red', 0, 0, width, height, frame=frame)
        data += crop.sample('green', 0, 0, width, height, frame=frame)
        data += crop.sample('blue', 0, 0, width, height, frame=frame)
        
        num_pixels = width * height
        
        # Reshape the flat data (R...R, G...G, B...B) into [R, G, B] triplets
        for i in range(num_pixels):
            r = data[i]
            g = data[i + num_pixels]
            b = data[i + 2 * num_pixels]
            pixels.append([r, g, b])
            
    finally:
        # Clean up the temporary node immediately
        nuke.delete(crop)
        
    return pixels

def get_sequence_info(read_node):
    """Gets the file path, directory, and JSON path from a Read node."""
    file_path_full = read_node['file'].value()
    
    if '%' in file_path_full:
        dir_path = os.path.dirname(file_path_full)
        base_name_pattern = os.path.basename(file_path_full)
        json_file_name = base_name_pattern.split('.')[0] + ".json"
        start = int(read_node['first'].value())
        end = int(read_node['last'].value())
        frame_list = range(start, end + 1)
    else:
        dir_path = os.path.dirname(file_path_full)
        base_name = os.path.basename(file_path_full)
        json_file_name = os.path.splitext(base_name)[0] + ".json"
        frame_list = [int(nuke.root()['first_frame'].value())]

    json_path = os.path.join(dir_path, json_file_name)
    return frame_list, dir_path, json_path


# -------------------------------------------------------------
# FUNCTION 1: EXPORT COLORS FROM SELECTED READ NODES
# -------------------------------------------------------------

def export_colors_from_selected_read_nodes():
    """
    Reads selected Read nodes, extracts dominant linear colors using sampling,
    and exports a JSON file.
    """
    selected_nodes = nuke.selectedNodes('Read')
    if not selected_nodes:
        nuke.message("Please select one or more Read nodes.")
        return

    for node in selected_nodes:
        frame_list, dir_path, json_path = get_sequence_info(node)
        all_dominant_colors = {}
        
        if not frame_list:
            nuke.warning(f"Skipping {node.name()}: Invalid frame range.")
            continue

        nuke.message(f"Processing {len(frame_list)} frames for {node.name()} with stride {PIXEL_STRIDE}...")

        original_frame = nuke.frame() 
        
        for frame in frame_list:
            nuke.frame(frame)
            try:
                # 1. Get ALL Linear Pixel Data from Nuke API
                linear_pixels = get_linear_pixels_from_node(node, frame)
                
                # 2. Sample, Quantize, and Aggregate Dominant Colors
                dominant_colors_linear = sample_and_aggregate_colors(linear_pixels)
                
                # 3. Store Results (as linear floats)
                all_dominant_colors[str(frame)] = dominant_colors_linear

            except Exception as e:
                nuke.warning(f"Error processing frame {frame} for {node.name()}: {e}")
                continue
        
        nuke.frame(original_frame)
                
        # 4. Save JSON file to disk
        try:
            with open(json_path, 'w') as f:
                json.dump(all_dominant_colors, f, indent=4)
            nuke.message(f"Successfully exported {len(dominant_colors_linear)} dominant linear colors per frame to:\n{json_path}")
        except Exception as e:
            nuke.critical(f"Failed to save JSON file at {json_path}: {e}")


# -------------------------------------------------------------
# FUNCTION 2: APPLY COLORS TO SELECTED READ NODE
# -------------------------------------------------------------

def convert_linear_to_display_color(linear_rgb_list):
    """Converts a linear RGB color to a display-ready color (for node 'tile_color')."""
    gamma_correction = 1.0 / 2.2
    display_rgb = []
    
    for val in linear_rgb_list:
        clamped_linear_val = max(0.0, val)
        display_val = math.pow(clamped_linear_val, gamma_correction)
        clipped_display_val = min(1.0, display_val)
        display_rgb.append(clipped_display_val)
        
    return display_rgb

def float_to_packed_int(r, g, b, a=1.0):
    """Packs float RGB values (0.0-1.0) into Nuke's single tile_color integer."""
    r_int = int(r * 255)
    g_int = int(g * 255)
    b_int = int(b * 255)
    a_int = int(a * 255)
    return (a_int &lt;&lt; 24) + (b_int &lt;&lt; 16) + (g_int &lt;&lt; 8) + r_int

def set_node_color_from_linear_rgb(node, linear_rgb_color):
    """Sets a Nuke node's tile_color based on a linear RGB list, applying conversion."""
    display_rgb = convert_linear_to_display_color(linear_rgb_color)
    r, g, b = display_rgb[0], display_rgb[1], display_rgb[2]
    packed_color = float_to_packed_int(r, g, b)
    node['tile_color'].setValue(packed_color)

def apply_colors_to_selected_read_node():
    """
    Reads the associated JSON color data and sets the tile_color of the 
    selected Read node based on the most dominant color of the current frame.
    """
    selected_nodes = nuke.selectedNodes('Read')
    if not selected_nodes:
        nuke.message("Please select one or more Read nodes.")
        return

    current_frame = nuke.frame()
    
    for node in selected_nodes:
        frame_list, dir_path, json_path = get_sequence_info(node)
        
        if not os.path.exists(json_path):
            nuke.message(f"Color JSON file not found for {node.name()}:\n{json_path}\n\nRun 'Export Colors' first.")
            continue
            
        try:
            with open(json_path, 'r') as f:
                color_data = json.load(f)
            
            frame_key = str(current_frame) 
            
            # Find the key, handling padded vs unpadded frame numbers
            matching_key = None
            if frame_key in color_data:
                matching_key = frame_key
            else:
                for key in color_data:
                    if key.lstrip('0') == frame_key.lstrip('0'):
                        matching_key = key
                        break
            
            if not matching_key:
                nuke.warning(f"No color data found for frame {current_frame} in {json_path}")
                continue

            # The most dominant color is always the first one in the list [0]
            linear_rgb_color = color_data[matching_key][0]
            
            # Set the node's tile_color with colorspace conversion
            set_node_color_from_linear_rgb(node, linear_rgb_color)
            
        except Exception as e:
            nuke.critical(f"Failed to apply color to {node.name()}: {e}")
            
</pre>

<div id="Reading Colors to JSON-QUESTION"><h2 id="QUESTION" class="header"><a href="#Reading Colors to JSON-QUESTION">QUESTION</a></h2></div>

<table>
<tr>
<td>
I'm using 'sample' method to query the values of images in Nuke. I'm saving JSON dictionaries of color values. Is there any way to make something as fast as Nuke's CurveTool? My guess is that Python is a bottleneck.
</td>
</tr>
</table>


<p>
You're correct; when querying thousands or millions of pixels across multiple frames, Python is almost certainly the bottleneck compared to Nuke's internal C++ processing, like the CurveTool. The sample() method requires Nuke to translate data from its native C++ memory structure into Python objects, which is an expensive process repeated for every pixel you sample.
</p>

<p>
To achieve speeds closer to the CurveTool, you must leverage Nuke's internal vectorized processing using the nuke.buffers module or OIIO (OpenImageIO), which are specifically designed to minimize the Python bottleneck by processing large blocks of data at once.
</p>

<p>
Here are the two fastest approaches, which avoid the per-pixel Python overhead:
</p>

<div id="Reading Colors to JSON-QUESTION-Using nuke.buffers.getImage() (The Fastest Nuke-Native Way)"><h4 id="Using nuke.buffers.getImage() (The Fastest Nuke-Native Way)" class="header"><a href="#Reading Colors to JSON-QUESTION-Using nuke.buffers.getImage() (The Fastest Nuke-Native Way)">Using nuke.buffers.getImage() (The Fastest Nuke-Native Way)</a></h4></div>

<p>
The nuke.buffers module allows you to directly access the raw pixel data from the image plane in a NumPy array. This bypasses the Nuke/Python conversion loop and lets NumPy (which is C-based) handle the large-scale data manipulation very quickly.
</p>

<div id="Reading Colors to JSON-QUESTION-Workflow"><h4 id="Workflow" class="header"><a href="#Reading Colors to JSON-QUESTION-Workflow">Workflow</a></h4></div>
<ul>
<li>
<span id="Reading Colors to JSON-QUESTION-Workflow-Get the Plane Data"></span><strong id="Get the Plane Data">Get the Plane Data</strong>: Use nuke.buffers.getImage() to dump the pixel data for a specified region and frame into a NumPy array.

<li>
<span id="Reading Colors to JSON-QUESTION-Workflow-Process with NumPy"></span><strong id="Process with NumPy">Process with NumPy</strong>: Use NumPy's highly optimized methods to calculate statistics (mean, variance, mode) or filter the data.

<li>
<span id="Reading Colors to JSON-QUESTION-Workflow-Convert to JSON"></span><strong id="Convert to JSON">Convert to JSON</strong>: Process the final statistical results into your JSON dictionary.

</ul>

<p>
Example Code
</p>
<pre python>
<table>
<tr>
<td>
UNTESTED
</td>
</tr>
</table>

import nuke
import numpy as np

def fast_sample_data(node, frame, channels=['red', 'green', 'blue']):
    # 1. Ensure the node is calculated at the specific frame
    nuke.toNode(node).frame(frame)
    
    # 2. Get the bounding box of the image (Area Of Interest)
    format = nuke.toNode(node).format()
    bbox = (format.x(), format.y(), format.r(), format.t())
    
    # 3. Dump the pixel data into a NumPy array
    try:
        data = nuke.buffers.getImage(
            node, 
            channels, 
            bbox[0], bbox[1], bbox[2], bbox[3]
        )
    except Exception as e:
        print(f"Error getting image data: {e}")
        return {}

    # 4. Process the data using NumPy for speed
    # 'data' is a 2D array: [pixel_index, channel_index]
    
    # Example: Calculate the mean color value for the entire image
    mean_values = np.mean(data, axis=0)
    
    # 5. Build the final JSON-like dictionary
    results = {}
    for i, channel in enumerate(channels):
        results[channel] = float(mean_values[i])
        
    return results

# Example Usage:
# stats = fast_sample_data('Read1', 101)
# print(stats) 
</pre>

<div id="Reading Colors to JSON-QUESTION-Using Nuke's CurveTool Itself (The &quot;No Code&quot; Solution)"><h3 id="Using Nuke's CurveTool Itself (The &quot;No Code&quot; Solution)" class="header"><a href="#Reading Colors to JSON-QUESTION-Using Nuke's CurveTool Itself (The &quot;No Code&quot; Solution)">Using Nuke's CurveTool Itself (The "No Code" Solution)</a></h3></div>

<p>
The CurveTool is a powerful and very fast way to generate statistical data (average, min/max, deviation) from an image sequence. Since you're looking for something as fast as it, you can simply use the CurveTool node's output data instead of writing your own sampling script.
</p>

<div id="Reading Colors to JSON-QUESTION-Using Nuke's CurveTool Itself (The &quot;No Code&quot; Solution)-‚öôÔ∏è Workflow"><h4 id="‚öôÔ∏è Workflow" class="header"><a href="#Reading Colors to JSON-QUESTION-Using Nuke's CurveTool Itself (The &quot;No Code&quot; Solution)-‚öôÔ∏è Workflow">‚öôÔ∏è Workflow</a></h4></div>
<ul>
<li>
Insert CurveTool: Add a CurveTool node after the image sequence you want to sample.

<li>
Configure: Set the "ROI" (Region of Interest) and select the statistics you want to track (e.g., Average, Max).

<li>
Analyze: Press the "Analyze" button. The CurveTool writes all the requested data to its own knobs (e.g., average.r, average.g).

<li>
Extract Data: Use a simple Python script to read the animated values from these knobs and save them to your JSON file.

</ul>

<div id="Reading Colors to JSON-QUESTION-Using Nuke's CurveTool Itself (The &quot;No Code&quot; Solution)-Extraction Code (Very Fast)"><h4 id="Extraction Code (Very Fast)" class="header"><a href="#Reading Colors to JSON-QUESTION-Using Nuke's CurveTool Itself (The &quot;No Code&quot; Solution)-Extraction Code (Very Fast)">Extraction Code (Very Fast)</a></h4></div>

<p>
This approach is fast because the heavy pixel processing is done by the C++ CurveTool, and Python only has to read a few animated knob values per frame.
</p>

<pre python>
<table>
<tr>
<td>
UNTESTED
</td>
</tr>
</table>

import nuke

def extract_curvetool_data(curvetool_node_name, start_frame, end_frame):
    node = nuke.toNode(curvetool_node_name)
    if not node or node.Class() != 'CurveTool':
        print(f"Node {curvetool_node_name} not found or not a CurveTool.")
        return []

    json_data = []
    
    # The statistics knobs are animated, we just read the values
    for frame in range(start_frame, end_frame + 1):
        nuke.frame(frame)
        
        frame_data = {
            "frame": frame,
            "r_avg": node.knob('average.r').value(),
            "g_avg": node.knob('average.g').value(),
            "b_avg": node.knob('average.b').value()
            # Add other stats as needed
        }
        json_data.append(frame_data)
        
    return json_data

# Example Usage:
# all_stats = extract_curvetool_data('CurveTool1', 1, 100)
# # You can now save 'all_stats' to your JSON file
</pre>

<p>
The second method is often the simplest and fastest solution, as it offloads all the heavy computational work to the optimized Nuke/C++ core.
</p>

<div id="Reading Colors to JSON-QUESTION"><h2 id="QUESTION" class="header"><a href="#Reading Colors to JSON-QUESTION">QUESTION</a></h2></div>

<table>
<tr>
<td>
I have images with a maximum of 50 identical colors. I want to go through and list these values in a dictionary (perhaps JSON). Can I leverage this technique to gather the set of colors?
</td>
</tr>
</table>

<p>
Yes, you can absolutely leverage the technique of processing pixel data in large blocks to efficiently gather the set of up to 50 unique colors, and it will be significantly faster than using repeated nuke.sample().
</p>

<p>
The recommended approach is to use NumPy via the nuke.buffers module (Method 1 from the previous answer), as it allows you to get all pixel data into an array at once, making it very quick to find all unique colors.
</p>

<p>
Efficiently Gathering Unique Colors (The NumPy Method)
This method avoids the Python per-pixel loop entirely, which is essential for speed.
</p>

<div id="Reading Colors to JSON-QUESTION-üêç Workflow"><h4 id="üêç Workflow" class="header"><a href="#Reading Colors to JSON-QUESTION-üêç Workflow">üêç Workflow</a></h4></div>

<ul>
<li>
Get Raw Data: Extract the entire image's pixel data into a NumPy array using nuke.buffers.getImage().

<li>
Combine Channels: Reshape or combine the Red, Green, and Blue channels for each pixel into a single, unique identifier (or just treat each pixel as a 3-element vector).

<li>
Find Unique Rows: Use the highly optimized NumPy function np.unique to quickly find every unique color vector present in the image.

<li>
Format Output: Convert the resulting unique color array into your desired Python dictionary or JSON format.

</ul>

<p>
Example Code
</p>
<pre python>
<table>
<tr>
<td>
UNTESTED
</td>
</tr>
</table>

import nuke
import numpy as np

def get_unique_colors(node_name, frame, max_colors=50):
    node = nuke.toNode(node_name)
    if not node:
        return {"error": f"Node '{node_name}' not found."}

    # 1. Prepare for data extraction
    # Ensure the node is evaluated at the frame
    nuke.frame(frame)
    
    # Get the bounding box (ROI)
    format = node.format()
    bbox = (format.x(), format.y(), format.r(), format.t())
    channels = ['red', 'green', 'blue']

    # 2. Dump the pixel data into a NumPy array
    try:
        # data will be a 2D array: [Number of Pixels, Number of Channels]
        data = nuke.buffers.getImage(
            node_name, 
            channels, 
            bbox[0], bbox[1], bbox[2], bbox[3]
        )
    except Exception as e:
        return {"error": f"Failed to get buffer data: {e}"}

    # 3. Find unique colors efficiently
    # np.unique with axis=0 finds unique ROWS (i.e., unique [R, G, B] vectors)
    # The 'return_counts=True' is optional but useful for verifying.
    unique_colors, counts = np.unique(data, axis=0, return_counts=True)
    
    if len(unique_colors) &gt; max_colors:
        # Optimization: If it has more than 50 colors, stop and return an error/warning
        return {"warning": f"Image has {len(unique_colors)} unique colors, exceeding the limit of {max_colors}."}
    
    # 4. Format the output dictionary
    color_dict = {}
    for i, color_vector in enumerate(unique_colors):
        # Convert NumPy float array to a list/tuple of standard Python floats
        r, g, b = color_vector.tolist()
        
        # Use the RGB tuple as the key (or format as a string for JSON keys)
        # Using a string key here for JSON compatibility
        color_key = f"({r:.6f}, {g:.6f}, {b:.6f})"
        color_dict[color_key] = {
            "r": r,
            "g": g,
            "b": b,
            "count": int(counts[i]) # Number of pixels with this color
        }

    return {"unique_colors": color_dict}

# Example Usage (assuming you have a node named 'Read1'):
# results = get_unique_colors('Read1', 1)
# print(results)
</pre>


<p>
This NumPy-based approach performs the massive data reduction (finding unique elements) using highly optimized C code, making it dramatically faster than sampling or looping in Python.
</p>



<p>
<a href="pmount_index.html">PMount</a>
</p>

<p>
<a href="index.html">Index</a>
</p>

</body>
</html>
