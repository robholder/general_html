<!DOCTYPE html>
<html>
<head>
<link rel="Stylesheet" type="text/css" href="style.css">
<link rel="alternate" type="application/rss+xml" title="RSS" href="rss.xml">
<title>pm_project_reading_colors_to_json</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
</head>
<body>

<p>
<a href="index.html">Index</a>
</p>

<p>
<a href="pmount_index.html">PMount</a>
</p>

<div id="Reading Colors to JSON"><h1 id="Reading Colors to JSON" class="header"><a href="#Reading Colors to JSON">Reading Colors to JSON</a></h1></div>

<div id="Reading Colors to JSON-Jump Here for the Real Start"><h2 id="Jump Here for the Real Start" class="header"><a href="#Reading Colors to JSON-Jump Here for the Real Start">Jump Here for the Real Start</a></h2></div>
<ul>
<li>
<a href="pm_project_reading_colors_to_json.html#PyOpenImageIO (PyIOP)">PyOpenImageIO (PyIOP)</a>

</ul>
<div id="Contents" class="toc"><h3 id="Contents" class="header"><a href="#Contents">Contents</a></h3></div>
<ul>
<li>
<a href="pm_project_reading_colors_to_json.html#Reading Colors to JSON">Reading Colors to JSON</a>

<ul>
<li>
<a href="pm_project_reading_colors_to_json.html#QUESTION">QUESTION</a>

<li>
<a href="pm_project_reading_colors_to_json.html#Recommended Solution: External Python with NumPy + Scikit-learn">Recommended Solution: External Python with NumPy + Scikit-learn</a>

<ul>
<li>
<a href="pm_project_reading_colors_to_json.html#The Strategy: K-Means Clustering">The Strategy: K-Means Clustering</a>

<li>
<a href="pm_project_reading_colors_to_json.html#The Tools">The Tools</a>

</ul>
<li>
<a href="pm_project_reading_colors_to_json.html#Python Code Example (K-Means)">Python Code Example (K-Means)</a>

<li>
<a href="pm_project_reading_colors_to_json.html#The OpenImageIO (OIIO) Option">The OpenImageIO (OIIO) Option</a>

<ul>
<li>
<a href="pm_project_reading_colors_to_json.html#OIIO Replacement for cv2.imread:">OIIO Replacement for cv2.imread:</a>

</ul>
<li>
<a href="pm_project_reading_colors_to_json.html#QUESTION">QUESTION</a>

<li>
<a href="pm_project_reading_colors_to_json.html#The Strategy: The Elbow Method üìê">The Strategy: The Elbow Method üìê</a>

<li>
<a href="pm_project_reading_colors_to_json.html#Modified Python Code with Dynamic K-Selection">Modified Python Code with Dynamic K-Selection</a>

<li>
<a href="pm_project_reading_colors_to_json.html#QUESTION">QUESTION</a>

<li>
<a href="pm_project_reading_colors_to_json.html#The Necessary Adjustment to the Code">The Necessary Adjustment to the Code</a>

<li>
<a href="pm_project_reading_colors_to_json.html#Corrected Code Snippet using OIIO (Recommended)">Corrected Code Snippet using OIIO (Recommended)</a>

<li>
<a href="pm_project_reading_colors_to_json.html#Corrected Code Snippet using OpenCV (If Used)">Corrected Code Snippet using OpenCV (If Used)</a>

<ul>
<li>
<a href="pm_project_reading_colors_to_json.html#Crucial Takeaway:">Crucial Takeaway:</a>

</ul>
<li>
<a href="pm_project_reading_colors_to_json.html#QUESTION">QUESTION</a>

<ul>
<li>
<a href="pm_project_reading_colors_to_json.html#Key Points for VFX Work:">Key Points for VFX Work:</a>

</ul>
<li>
<a href="pm_project_reading_colors_to_json.html#QUESTION">QUESTION</a>

<ul>
<li>
<a href="pm_project_reading_colors_to_json.html#How Nuke Read Nodes Affect Pixel Values">How Nuke Read Nodes Affect Pixel Values</a>

<li>
<a href="pm_project_reading_colors_to_json.html#The Inaccuracy of Extracting Color Post-Read Node">The Inaccuracy of Extracting Color Post-Read Node</a>

<ul>
<li>
<a href="pm_project_reading_colors_to_json.html#Conclusion for Your Workflow">Conclusion for Your Workflow</a>

<li>
<a href="pm_project_reading_colors_to_json.html#Recommended Solution: Use Linear Data for Clustering">Recommended Solution: Use Linear Data for Clustering</a>

</ul>
<li>
<a href="pm_project_reading_colors_to_json.html#Code Adjustment for Linearization">Code Adjustment for Linearization</a>

</ul>
<li>
<a href="pm_project_reading_colors_to_json.html#QUESTION">QUESTION</a>

<ul>
<li>
<a href="pm_project_reading_colors_to_json.html#Alternative: Pure Python K-Means Clustering">Alternative: Pure Python K-Means Clustering</a>

</ul>
<li>
<a href="pm_project_reading_colors_to_json.html#Pure Python K-Means Function">Pure Python K-Means Function</a>

<ul>
<li>
<a href="pm_project_reading_colors_to_json.html#Replacing the Elbow Method (Pure Python)">Replacing the Elbow Method (Pure Python)</a>

<ul>
<li>
<a href="pm_project_reading_colors_to_json.html#Heuristic Alternative: Fixed K-range and a Simple Threshold">Heuristic Alternative: Fixed K-range and a Simple Threshold</a>

</ul>
<li>
<a href="pm_project_reading_colors_to_json.html#Pure Python Colorspace and JSON Handling">Pure Python Colorspace and JSON Handling</a>

<li>
<a href="pm_project_reading_colors_to_json.html#Summary of Changes:">Summary of Changes:</a>

</ul>
<li>
<a href="pm_project_reading_colors_to_json.html#QUESTION">QUESTION</a>

<ul>
<li>
<a href="pm_project_reading_colors_to_json.html#Pure Python Colorspace Conversion">Pure Python Colorspace Conversion</a>

<li>
<a href="pm_project_reading_colors_to_json.html#Using Nuke's Native Color Functions (Advanced)">Using Nuke's Native Color Functions (Advanced)</a>

</ul>
<li>
<a href="pm_project_reading_colors_to_json.html#QUESTION">QUESTION</a>

<ul>
<li>
<a href="pm_project_reading_colors_to_json.html#Complete Pure Python Color Extraction Script (Nuke-Ready)">Complete Pure Python Color Extraction Script (Nuke-Ready)</a>

</ul>
<li>
<a href="pm_project_reading_colors_to_json.html#QUESTION">QUESTION</a>

<ul>
<li>
<a href="pm_project_reading_colors_to_json.html#Complete Pure Python Script with Nuke I/O">Complete Pure Python Script with Nuke I/O</a>

</ul>
<li>
<a href="pm_project_reading_colors_to_json.html#QUESTION">QUESTION</a>

<ul>
<li>
<a href="pm_project_reading_colors_to_json.html#OIIO in the Nuke Ecosystem">OIIO in the Nuke Ecosystem</a>

<ul>
<li>
<a href="pm_project_reading_colors_to_json.html#The functionality in your script that uses:">The functionality in your script that uses:</a>

</ul>
</ul>
<li>
<a href="pm_project_reading_colors_to_json.html#QUESTION">QUESTION</a>

<ul>
<li>
<a href="pm_project_reading_colors_to_json.html#Complete Pure Python Script (Nuke API Method)">Complete Pure Python Script (Nuke API Method)</a>

</ul>
<li>
<a href="pm_project_reading_colors_to_json.html#QUESTION">QUESTION</a>

<ul>
<li>
<a href="pm_project_reading_colors_to_json.html#Comparison of Processing Time">Comparison of Processing Time</a>

<li>
<a href="pm_project_reading_colors_to_json.html#The Bottleneck: Pixel Extraction">The Bottleneck: Pixel Extraction</a>

<ul>
<li>
<a href="pm_project_reading_colors_to_json.html#Conclusion on Speed">Conclusion on Speed</a>

</ul>
</ul>
<li>
<a href="pm_project_reading_colors_to_json.html#QUESTION">QUESTION</a>

<ul>
<li>
<a href="pm_project_reading_colors_to_json.html#Alternative Pure Python Script: Sequential Sampling &amp; Quantization">Alternative Pure Python Script: Sequential Sampling &amp; Quantization</a>

<li>
<a href="pm_project_reading_colors_to_json.html#Performance Analysis">Performance Analysis</a>

</ul>
<li>
<a href="pm_project_reading_colors_to_json.html#QUESTION">QUESTION</a>

<li>
<a href="pm_project_reading_colors_to_json.html#K-Means vs. Sampled Quantization">K-Means vs. Sampled Quantization</a>

<ul>
<li>
<a href="pm_project_reading_colors_to_json.html#The Goal of K-Means: Finding True Averages">The Goal of K-Means: Finding True Averages</a>

<ul>
<li>
<a href="pm_project_reading_colors_to_json.html#K-Means (Clustering) goes further:">K-Means (Clustering) goes further:</a>

</ul>
<li>
<a href="pm_project_reading_colors_to_json.html#When to Use Which Method">When to Use Which Method</a>

</ul>
<li>
<a href="pm_project_reading_colors_to_json.html#QUESTION">QUESTION</a>

<ul>
<li>
<a href="pm_project_reading_colors_to_json.html#Why Brute-Force is Better for a Limited Palette">Why Brute-Force is Better for a Limited Palette</a>

<li>
<a href="pm_project_reading_colors_to_json.html#The Crucial Step: Adjusting Quantization">The Crucial Step: Adjusting Quantization</a>

</ul>
<li>
<a href="pm_project_reading_colors_to_json.html#QUESTION">QUESTION</a>

<ul>
<li>
<a href="pm_project_reading_colors_to_json.html#Recommended Script Adjustment">Recommended Script Adjustment</a>

<li>
<a href="pm_project_reading_colors_to_json.html#Benefits of the max_colors=200 Safeguard">Benefits of the max_colors=200 Safeguard</a>

</ul>
<li>
<a href="pm_project_reading_colors_to_json.html#QUESTION">QUESTION</a>

</ul>
<li>
<a href="pm_project_reading_colors_to_json.html#Further Dev">Further Dev</a>

<ul>
<li>
<a href="pm_project_reading_colors_to_json.html#Summary">Summary</a>

<ul>
<li>
<a href="pm_project_reading_colors_to_json.html#Usage notes and integration">Usage notes and integration</a>

</ul>
</ul>
<li>
<a href="pm_project_reading_colors_to_json.html#QUESTION">QUESTION</a>

<ul>
<li>
<a href="pm_project_reading_colors_to_json.html#Using nuke.buffers.getImage() (The Fastest Nuke-Native Way)">Using nuke.buffers.getImage() (The Fastest Nuke-Native Way)</a>

</ul>
<li>
<a href="pm_project_reading_colors_to_json.html#Using Nuke's CurveTool Itself (The &quot;No Code&quot; Solution)">Using Nuke's CurveTool Itself (The "No Code" Solution)</a>

<ul>
<li>
<a href="pm_project_reading_colors_to_json.html#‚öôÔ∏è Workflow">‚öôÔ∏è Workflow</a>

<li>
<a href="pm_project_reading_colors_to_json.html#Extraction Code (Very Fast)">Extraction Code (Very Fast)</a>

</ul>
<li>
<a href="pm_project_reading_colors_to_json.html#QUESTION">QUESTION</a>

<ul>
<li>
<a href="pm_project_reading_colors_to_json.html#üêç Workflow">üêç Workflow</a>

</ul>
<li>
<a href="pm_project_reading_colors_to_json.html#PyOpenImageIO (PyIOP)">PyOpenImageIO (PyIOP)</a>

<ul>
<li>
<a href="pm_project_reading_colors_to_json.html#üêç Workflow">üêç Workflow</a>

<li>
<a href="pm_project_reading_colors_to_json.html#Corrected Example Code">Corrected Example Code</a>

<ul>
<li>
<a href="pm_project_reading_colors_to_json.html#‚úÖ Strengths">‚úÖ Strengths</a>

</ul>
<li>
<a href="pm_project_reading_colors_to_json.html#üõ† Suggestions for Improvement">üõ† Suggestions for Improvement</a>

<ul>
<li>
<a href="pm_project_reading_colors_to_json.html#Avoid Overwriting Temp Files">Avoid Overwriting Temp Files</a>

<li>
<a href="pm_project_reading_colors_to_json.html#Limit Memory Usage for Large Images">Limit Memory Usage for Large Images</a>

<li>
<a href="pm_project_reading_colors_to_json.html#Optional: Sort Colors by Frequency">Optional: Sort Colors by Frequency</a>

<li>
<a href="pm_project_reading_colors_to_json.html#Add Support for Alpha or Other Channels">Add Support for Alpha or Other Channels</a>

<li>
<a href="pm_project_reading_colors_to_json.html#üß™ Optional Enhancements">üß™ Optional Enhancements</a>

</ul>
</ul>
<li>
<a href="pm_project_reading_colors_to_json.html#Understanding data.shape">Understanding data.shape</a>

<ul>
<li>
<a href="pm_project_reading_colors_to_json.html#Example:">Example:</a>

</ul>
<li>
[[#What data.shape[-1|What data.shape[-1] means]]

<ul>
<li>
[[#Why use it in reshape(-1, data.shape[-1|Why use it in reshape(-1, data.shape[-1])]]

<li>
<a href="pm_project_reading_colors_to_json.html#Example:">Example:</a>

</ul>
<li>
<a href="pm_project_reading_colors_to_json.html#QUESTION">QUESTION</a>

<ul>
<li>
<a href="pm_project_reading_colors_to_json.html#Step 1. Imagine the raw image data">Step 1. Imagine the raw image data</a>

<li>
<a href="pm_project_reading_colors_to_json.html#Step 2. Reshape to a flat list of pixels">Step 2. Reshape to a flat list of pixels</a>

<li>
<a href="pm_project_reading_colors_to_json.html#Step 3. Why data.shape[-1">Step 3. Why data.shape[-1</a>]

<li>
<a href="pm_project_reading_colors_to_json.html#‚ú® So in your head:">‚ú® So in your head:</a>

<li>
<a href="pm_project_reading_colors_to_json.html#Step 1. A 2√ó2 RGBA image">Step 1. A 2√ó2 RGBA image</a>

<li>
<a href="pm_project_reading_colors_to_json.html#Step 2. Reshape to a flat list of pixels">Step 2. Reshape to a flat list of pixels</a>

<li>
<a href="pm_project_reading_colors_to_json.html#Step 3. Slice off the alpha">Step 3. Slice off the alpha</a>

<li>
<a href="pm_project_reading_colors_to_json.html#‚ú® So the flow is:">‚ú® So the flow is:</a>

</ul>
<li>
<a href="pm_project_reading_colors_to_json.html#np.unique">np.unique</a>

<ul>
<li>
<a href="pm_project_reading_colors_to_json.html#Step 1. Our reshaped RGB pixel list">Step 1. Our reshaped RGB pixel list</a>

<li>
<a href="pm_project_reading_colors_to_json.html#Step 2. Apply np.unique">Step 2. Apply np.unique</a>

<li>
<a href="pm_project_reading_colors_to_json.html#Step 3. Try with duplicates">Step 3. Try with duplicates</a>

<li>
<a href="pm_project_reading_colors_to_json.html#‚ú® That‚Äôs exactly what your function is doing under the hood:">‚ú® That‚Äôs exactly what your function is doing under the hood:</a>

</ul>
<li>
<a href="pm_project_reading_colors_to_json.html#Sorting by Frequency">Sorting by Frequency</a>

<ul>
<li>
<a href="pm_project_reading_colors_to_json.html#Step 1. Our pixel list with duplicates">Step 1. Our pixel list with duplicates</a>

<li>
<a href="pm_project_reading_colors_to_json.html#Step 2. Find unique colors and counts">Step 2. Find unique colors and counts</a>

<li>
<a href="pm_project_reading_colors_to_json.html#Step 3. Sort by frequency">Step 3. Sort by frequency</a>

<li>
<a href="pm_project_reading_colors_to_json.html#Step 4. Wrap into a dictionary (like your function)">Step 4. Wrap into a dictionary (like your function)</a>

<li>
<a href="pm_project_reading_colors_to_json.html#‚ú® That‚Äôs the whole pipeline:">‚ú® That‚Äôs the whole pipeline:</a>

</ul>
<li>
<a href="pm_project_reading_colors_to_json.html#Why a Temp File">Why a Temp File</a>

<li>
<a href="pm_project_reading_colors_to_json.html#QUESTION">QUESTION</a>

<ul>
<li>
<a href="pm_project_reading_colors_to_json.html#Why the temp file is there">Why the temp file is there</a>

<li>
<a href="pm_project_reading_colors_to_json.html#Could we skip the file?">Could we skip the file?</a>

<ul>
<li>
<a href="pm_project_reading_colors_to_json.html#Trade‚Äëoffs">Trade‚Äëoffs</a>

<li>
<a href="pm_project_reading_colors_to_json.html#‚ú® In practice:">‚ú® In practice:</a>

</ul>
</ul>
<li>
<a href="pm_project_reading_colors_to_json.html#Batch Processing Frames">Batch Processing Frames</a>

<ul>
<li>
<a href="pm_project_reading_colors_to_json.html#üîÑ What Batch Processing Could Mean">üîÑ What Batch Processing Could Mean</a>

<li>
<a href="pm_project_reading_colors_to_json.html#üõ† How to Implement It">üõ† How to Implement It</a>

<ul>
<li>
<a href="pm_project_reading_colors_to_json.html#üìä Example Usage">üìä Example Usage</a>

<li>
<a href="pm_project_reading_colors_to_json.html#üìä Example Usage">üìä Example Usage</a>

<li>
<a href="pm_project_reading_colors_to_json.html#‚ú® Why This Works Well for You">‚ú® Why This Works Well for You</a>

</ul>
</ul>
<li>
<a href="pm_project_reading_colors_to_json.html#Sort by Frequency">Sort by Frequency</a>

<ul>
<li>
<a href="pm_project_reading_colors_to_json.html#üîÑ Updated Function with Sorting">üîÑ Updated Function with Sorting</a>

<li>
<a href="pm_project_reading_colors_to_json.html#üìä Example Usage">üìä Example Usage</a>

</ul>
</ul>
<div id="Reading Colors to JSON-QUESTION"><h2 id="QUESTION" class="header"><a href="#Reading Colors to JSON-QUESTION">QUESTION</a></h2></div>

<table>
<tr>
<td>
I want to get a list of common colors in a sequence of images. I'm bringing in sequences of images with around 30 colors per sequence (they are comic book style). I want to make a json list of RGB colors. I can do this in Nuke using the 'sample' method in Python, but it's slow. Can I use some other tool (inside or outside Nuke) to achieve this more efficiently and faster. For example I've heard the 'Open Image IO' may be faster. I'm open to any ideas.
</td>
</tr>
</table>

<p>
The slow performance of Nuke's Python sample method is a common issue because it incurs overhead from the Nuke evaluation engine. To achieve this more efficiently and faster, you should move the image processing outside of the Nuke Python API and use high-performance, dedicated Python libraries.
</p>

<p>
The best approach for finding a limited set of dominant colors is to use a Machine Learning/Statistical Clustering method, specifically K-Means Clustering, which is highly optimized in popular scientific libraries.
</p>

<p>
Here is the most efficient solution, combining fast I/O and statistical processing:
</p>

<div id="Reading Colors to JSON-Recommended Solution: External Python with NumPy + Scikit-learn"><h2 id="Recommended Solution: External Python with NumPy + Scikit-learn" class="header"><a href="#Reading Colors to JSON-Recommended Solution: External Python with NumPy + Scikit-learn">Recommended Solution: External Python with NumPy + Scikit-learn</a></h2></div>

<p>
This method is significantly faster because it uses optimized C/C++ backend libraries (like NumPy) for file reading and computation, bypassing Nuke's slower internal Python wrappers.
</p>

<div id="Reading Colors to JSON-Recommended Solution: External Python with NumPy + Scikit-learn-The Strategy: K-Means Clustering"><h3 id="The Strategy: K-Means Clustering" class="header"><a href="#Reading Colors to JSON-Recommended Solution: External Python with NumPy + Scikit-learn-The Strategy: K-Means Clustering">The Strategy: K-Means Clustering</a></h3></div>

<p>
Since your images are "comic book style" with a naturally limited number of distinct colors (around 30), K-Means Clustering is the ideal algorithm.
</p>
<ul>
<li>
<span id="Reading Colors to JSON-Recommended Solution: External Python with NumPy + Scikit-learn-The Strategy: K-Means Clustering-How it works"></span><strong id="How it works">How it works</strong>: It groups all the pixels in an image into K clusters (where K is the number of colors you expect, e.g., K=30). The center of each cluster (centroid) is one of your dominant colors.

<li>
<span id="Reading Colors to JSON-Recommended Solution: External Python with NumPy + Scikit-learn-The Strategy: K-Means Clustering-Speed"></span><strong id="Speed">Speed</strong>: Libraries like Scikit-learn or OpenCV implement K-Means using highly optimized algorithms that can process millions of pixels quickly.

</ul>
<div id="Reading Colors to JSON-Recommended Solution: External Python with NumPy + Scikit-learn-The Tools"><h3 id="The Tools" class="header"><a href="#Reading Colors to JSON-Recommended Solution: External Python with NumPy + Scikit-learn-The Tools">The Tools</a></h3></div>

<table>
<tr>
<td>
Tool
</td>
<td>
Purpose	Why it's Fast
</td>
<td>
&nbsp;
</td>
</tr>
<tr>
<td>
NumPy
</td>
<td>
High-performance, low-level array manipulation.
</td>
<td>
Core of Python scientific computing, operates on pre-compiled C/C++ code.
</td>
</tr>
<tr>
<td>
OpenCV (cv2) or PIL/Pillow
</td>
<td>
Fast image loading and pixel access.
</td>
<td>
Optimized for reading and writing large image data efficiently.
</td>
</tr>
<tr>
<td>
Scikit-learn (sklearn)
</td>
<td>
Implements the fast K-Means Clustering algorithm.
</td>
<td>
Excellent, robust, and fast statistical library.
</td>
</tr>
<tr>
<td>
OpenImageIO (OIIO)
</td>
<td>
Optional. The Python bindings are great for VFX file formats (EXR, TIFF, etc.), offering fast, consistent I/O, as you suspected.
</td>
<td>
Specifically designed for high-performance reading of image sequences in a production pipeline.
</td>
</tr>
</table>

<div id="Reading Colors to JSON-Python Code Example (K-Means)"><h2 id="Python Code Example (K-Means)" class="header"><a href="#Reading Colors to JSON-Python Code Example (K-Means)">Python Code Example (K-Means)</a></h2></div>

<p>
This script iterates through your sequence, processes each frame for K=30 colors, and aggregates the results.
</p>

<pre python>
import numpy as np
import cv2  # You could use Pillow or OIIO instead of OpenCV
from sklearn.cluster import MiniBatchKMeans
import json
import glob
import os

def extract_dominant_colors_sequence(sequence_path, num_colors=30, frames=None):
    """
    Extracts the dominant RGB colors from a sequence of images using MiniBatchKMeans.
    
    Args:
        sequence_path (str): File path pattern (e.g., '/path/to/img.%04d.exr').
        num_colors (int): The target number of dominant colors (K).
        frames (list): Optional list of frame numbers to process (e.g., [1, 10, 20]).
    
    Returns:
        dict: A dictionary mapping frame numbers to a list of RGB color lists.
    """
    
    all_dominant_colors = {}
    
    if not frames:
        # Simple glob to find all files if frame list isn't provided
        # NOTE: This part needs careful handling for frame number parsing if not using Nuke's native Read node features
        print("Warning: Processing a basic file glob. Consider using OIIO for robust sequence handling.")
        file_list = sorted(glob.glob(sequence_path.replace('%04d', '*')))
        
        # Simple frame number extraction for file_list:
        frame_list = [os.path.basename(f).split('.')[1] for f in file_list]
        file_frame_pairs = zip(file_list, frame_list)
        
    else:
        # Use nuke.frame() functionality replacement for a range of frames
        def get_frame_path(frame_num):
            # Assumes the sequence path uses Nuke-style padding (e.g., %04d)
            return sequence_path % frame_num
        
        file_frame_pairs = [(get_frame_path(f), str(f)) for f in frames]


    for file_path, frame_num_str in file_frame_pairs:
        try:
            # 1. Load Image (using OpenCV for speed)
            # flag=1 loads as BGR (color), flag=-1 loads all channels including alpha
            img = cv2.imread(file_path)
            if img is None:
                print(f"Skipping frame {frame_num_str}: Could not load image.")
                continue

            # Convert BGR (OpenCV default) to RGB
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            
            # 2. Reshape and Normalize Pixel Data
            # Reshape the image array to a list of pixels (N rows, 3 columns for R, G, B)
            pixels = img.reshape((-1, 3))
            pixels = np.float32(pixels)

            # 3. Apply K-Means Clustering (using MiniBatchKMeans for even more speed)
            # This finds 'num_colors' representative colors
            kmeans = MiniBatchKMeans(n_clusters=num_colors, n_init='auto', random_state=42)
            kmeans.fit(pixels)
            
            # The cluster centers are the dominant colors
            dominant_colors = kmeans.cluster_centers_

            # 4. Format Output: Convert float RGB (0-255) to integer list
            # The result is a list of [R, G, B] lists
            color_list = dominant_colors.astype(int).tolist()
            
            all_dominant_colors[frame_num_str] = color_list
            print(f"Processed frame {frame_num_str}: Found {len(color_list)} colors.")

        except Exception as e:
            print(f"Error processing frame {frame_num_str}: {e}")
            
    # 5. Output to JSON
    json_output = json.dumps(all_dominant_colors, indent=4)
    return json_output

# --- Example Usage ---
# NOTE: Replace 'read_node_file_path' with the actual file sequence path.
# Example path with Nuke-style padding:
# read_node_file_path = '/path/to/my/comic/scene_1.shot_01.%04d.exr'
# frames_to_process = list(range(101, 201))  # Process frames 101 through 200

# # Run the function
# json_data = extract_dominant_colors_sequence(read_node_file_path, num_colors=30, frames=frames_to_process)

# # Save the JSON file
# with open('comic_palette.json', 'w') as f:
#     f.write(json_data)

</pre>

<div id="Reading Colors to JSON-The OpenImageIO (OIIO) Option"><h2 id="The OpenImageIO (OIIO) Option" class="header"><a href="#Reading Colors to JSON-The OpenImageIO (OIIO) Option">The OpenImageIO (OIIO) Option</a></h2></div>

<p>
You specifically mentioned OpenImageIO. This is an excellent idea, especially if your pipeline uses complex file formats like OpenEXR, TIFF, or DPX.
</p>

<ul>
<li>
<span id="Reading Colors to JSON-The OpenImageIO (OIIO) Option-Integration"></span><strong id="Integration">Integration</strong>: Use the Python bindings for OIIO (import OpenImageIO as OIIO).

<li>
<span id="Reading Colors to JSON-The OpenImageIO (OIIO) Option-Fast Reading"></span><strong id="Fast Reading">Fast Reading</strong>: Replace cv2.imread(file_path) with OIIO code to read the image data into a NumPy array (which OIIO handles natively).

<li>
<span id="Reading Colors to JSON-The OpenImageIO (OIIO) Option-Process"></span><strong id="Process">Process</strong>: Pass the NumPy array directly to the K-Means clustering step (as shown above).

</ul>
<p>
This separates the fast I/O (OIIO) from the fast computation (NumPy/Scikit-learn), maximizing efficiency.
</p>

<div id="Reading Colors to JSON-The OpenImageIO (OIIO) Option-OIIO Replacement for cv2.imread:"><h3 id="OIIO Replacement for cv2.imread:" class="header"><a href="#Reading Colors to JSON-The OpenImageIO (OIIO) Option-OIIO Replacement for cv2.imread:">OIIO Replacement for cv2.imread:</a></h3></div>

<pre python>
import OpenImageIO as OIIO
import numpy as np
# ... (rest of your imports)

def read_image_with_oiio(file_path):
    """Loads an image into a NumPy array using OpenImageIO."""
    
    # 1. Read the image and spec
    img_input = OIIO.ImageInput.open(file_path)
    if not img_input:
        raise IOError(f"Could not open image with OIIO: {OIIO.geterror()}")

    spec = img_input.spec()
    
    # 2. Create NumPy array and read pixel data
    # Read as float type and get R, G, B channels
    pixel_data = np.empty((spec.height, spec.width, spec.nchannels), dtype=np.float32)
    img_input.read_image(0, 0, spec.nchannels, OIIO.TypeDesc.FLOAT, pixel_data)
    img_input.close()
    
    # Only return the first 3 channels (R, G, B), dropping Alpha if present
    return (pixel_data[:, :, :3] * 255.0).astype(np.uint8) # Convert to 0-255 range for clustering
</pre>

<p>
You would then call this function within your main loop and feed the resulting array to the clustering steps.
</p>

<div id="Reading Colors to JSON-QUESTION"><h2 id="QUESTION" class="header"><a href="#Reading Colors to JSON-QUESTION">QUESTION</a></h2></div>

<table>
<tr>
<td>
I won't know the number of colors for each image sequence, but I know they will be relatively low in number. How can I change the code to expect any number of colors?
</td>
</tr>
</table>

<p>
That's a key challenge when using K-Means Clustering! If you don't know the optimal number of clusters (K), you need an automated way to estimate it for each frame.
</p>

<p>
You can modify the code to use the Elbow Method to dynamically determine the best K for a given image, making the process adaptive to your comic book style sequences.
</p>

<div id="Reading Colors to JSON-The Strategy: The Elbow Method üìê"><h2 id="The Strategy: The Elbow Method üìê" class="header"><a href="#Reading Colors to JSON-The Strategy: The Elbow Method üìê">The Strategy: The Elbow Method üìê</a></h2></div>

<p>
The Elbow Method is a heuristic used to find the optimal number of clusters (K) in K-Means.
</p>

<ul>
<li>
<span id="Reading Colors to JSON-The Strategy: The Elbow Method üìê-Calculate Distortion"></span><strong id="Calculate Distortion">Calculate Distortion</strong>: Run K-Means multiple times, increasing K (e.g., from 5 to 50). For each run, record the distortion (the sum of squared distances between each point and its assigned centroid).

<li>
<span id="Reading Colors to JSON-The Strategy: The Elbow Method üìê-Find the &quot;Elbow&quot;"></span><strong id="Find the &quot;Elbow&quot;">Find the "Elbow"</strong>: Plot K (x-axis) vs. Distortion (y-axis). The distortion will decrease sharply as K increases, then slow down dramatically. The point where the curve bends, resembling an "elbow," is the optimal number of clusters, as adding more clusters provides diminishing returns.

</ul>
<p>
Since your color counts are low (around 30), we can define a reasonable search range for K.
</p>

<div id="Reading Colors to JSON-Modified Python Code with Dynamic K-Selection"><h2 id="Modified Python Code with Dynamic K-Selection" class="header"><a href="#Reading Colors to JSON-Modified Python Code with Dynamic K-Selection">Modified Python Code with Dynamic K-Selection</a></h2></div>

<p>
This modified script uses the scipy.spatial.distance.cdist function (often necessary for distortion calculation) and the MiniBatchKMeans inertia (which is equivalent to distortion) over a range of possible K values.
</p>

<pre python>
import numpy as np
import cv2  # or OIIO, as discussed
from sklearn.cluster import MiniBatchKMeans
from scipy.spatial.distance import cdist
import json
import glob
import os

def determine_optimal_k(pixels_data, k_range=(5, 50)):
    """
    Uses the Elbow Method (via inertia) to find the optimal number of clusters (K).
    
    Args:
        pixels_data (np.ndarray): Reshaped image pixels (N, 3).
        k_range (tuple): (min K, max K) to search within.
        
    Returns:
        int: The estimated optimal number of clusters.
    """
    k_min, k_max = k_range
    inertias = []
    
    # Run MiniBatchKMeans for a range of K values
    for k in range(k_min, k_max + 1, 5): # Check every 5th K for speed
        kmeans = MiniBatchKMeans(n_clusters=k, n_init='auto', random_state=42)
        kmeans.fit(pixels_data)
        inertias.append(kmeans.inertia_)
    
    # --- Simplified Elbow Detection ---
    # This is often done visually, but we can approximate by looking for the 
    # point where the rate of decrease in inertia slows down.
    
    # Calculate the percentage change in inertia
    changes = np.diff(inertias)
    percent_drops = np.abs(changes / inertias[:-1])
    
    # Find the index where the drop (gain in information) falls below a threshold (e.g., 5%)
    # This threshold may need adjustment based on your image style.
    threshold = 0.05 
    
    # Add 1 to the index because diff/changes array is one element shorter
    elbow_index = np.where(percent_drops &lt; threshold)[0]
    
    if len(elbow_index) &gt; 0:
        # The elbow is typically the first index where the drop slows down
        return k_range[0] + (elbow_index[0] + 1) * 5 
    else:
        # If no elbow is clearly found, default to the max K in the range
        return k_max

def extract_dominant_colors_sequence_dynamic(sequence_path, k_search_range=(5, 50), frames=None):
    """
    Extracts dominant colors, dynamically determining the optimal K for each frame.
    """
    all_dominant_colors = {}
    
    # --- Setup file/frame list logic (omitted for brevity, assume same as before) ---
    # NOTE: You'd implement your file iteration here. For this example, we'll
    # assume the input is a list of file paths named 'file_frame_pairs'.
    
    # Placeholder for file iteration based on a single sample frame:
    
    # Example to load the first file in the sequence as a test:
    # file_list = sorted(glob.glob(sequence_path.replace('%04d', '*')))
    # file_frame_pairs = zip(file_list, [os.path.basename(f).split('.')[1] for f in file_list])
    
    # --- Main Loop ---
    # We will assume a single image file for demonstration
    file_path = "path/to/single/image.jpg" # Replace with actual file iteration
    frame_num_str = "001" 
    
    # 1. Load Image
    img = cv2.imread(file_path) 
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    pixels = img.reshape((-1, 3))
    pixels = np.float32(pixels)

    # 2. Determine Optimal K
    optimal_k = determine_optimal_k(pixels, k_search_range)
    print(f"Frame {frame_num_str}: Optimal K estimated at {optimal_k}")

    # 3. Final K-Means with Optimal K
    kmeans = MiniBatchKMeans(n_clusters=optimal_k, n_init='auto', random_state=42)
    kmeans.fit(pixels)
    dominant_colors = kmeans.cluster_centers_

    # 4. Format Output
    color_list = dominant_colors.astype(int).tolist()
    all_dominant_colors[frame_num_str] = color_list
        
    json_output = json.dumps(all_dominant_colors, indent=4)
    return json_output

# --- Considerations ---
# 1. The range for K_RANGE (5, 50) is a guess. Adjust based on how many
#    unique colors your images actually contain.
# 2. The ELBOW DETECTION THRESHOLD (0.05) is a heuristic. It may need tuning.
# 3. For faster processing, you might want to run the K-Means on a 
#    *downscaled* version of the image (e.g., resize to 500x500 pixels) 
#    before calculating the elbow and the final colors.
</pre>

<div id="Reading Colors to JSON-QUESTION"><h2 id="QUESTION" class="header"><a href="#Reading Colors to JSON-QUESTION">QUESTION</a></h2></div>

<table>
<tr>
<td>
I believe the RGB values would need to be in the same units/representation as used in Nuke - zero to one for standard black to white (while HDR images are possible). Would I forego the conversion to 0 - 255?
</td>
</tr>
</table>

<p>
Yes, you are absolutely correct and should forego the conversion to the 0-255 range if you intend to use the resulting RGB values directly within Nuke (or any standard VFX application). üí°
</p>

<p>
Nuke and OpenEXR/HDR pipelines natively operate on floating-point color values where standard dynamic range is represented by 0.0 to 1.0.
</p>

<div id="Reading Colors to JSON-The Necessary Adjustment to the Code"><h2 id="The Necessary Adjustment to the Code" class="header"><a href="#Reading Colors to JSON-The Necessary Adjustment to the Code">The Necessary Adjustment to the Code</a></h2></div>

<p>
You need to ensure two things in your processing pipeline:
</p>
<ul>
<li>
<span id="Reading Colors to JSON-The Necessary Adjustment to the Code-Image Read"></span><strong id="Image Read">Image Read</strong>: The image data is loaded directly as floating-point numbers.

<li>
<span id="Reading Colors to JSON-The Necessary Adjustment to the Code-K-Means Processing"></span><strong id="K-Means Processing">K-Means Processing</strong>: The clustering algorithm runs on the 0.0‚àí1.0 floating-point data.

<li>
<span id="Reading Colors to JSON-The Necessary Adjustment to the Code-Output"></span><strong id="Output">Output</strong>: The final cluster centers are outputted as floating-point numbers.

</ul>
<p>
If you are using OpenImageIO (OIIO), this is much simpler as it defaults to reading data as float (or whatever the file precision is) and gives you the 0.0‚àí1.0 data naturally. If you are using OpenCV or Pillow, you must explicitly ensure the conversion to float and division by 255 does not happen, or handle the data type correctly.
</p>

<div id="Reading Colors to JSON-Corrected Code Snippet using OIIO (Recommended)"><h2 id="Corrected Code Snippet using OIIO (Recommended)" class="header"><a href="#Reading Colors to JSON-Corrected Code Snippet using OIIO (Recommended)">Corrected Code Snippet using OIIO (Recommended)</a></h2></div>

<p>
Since OIIO is designed for VFX, it's the most reliable for float data.
</p>

<pre python>
import OpenImageIO as OIIO
import numpy as np
# ... (rest of your imports)

def read_image_with_oiio(file_path):
    """Loads an image into a NumPy array using OpenImageIO as float32 (0.0 - 1.0 range)."""
    
    img_input = OIIO.ImageInput.open(file_path)
    if not img_input:
        raise IOError(f"Could not open image with OIIO: {OIIO.geterror()}")

    spec = img_input.spec()
    
    # Read pixel data directly as float32
    # OIIO reads data in its native float range (0.0-1.0 or HDR range for EXR)
    pixel_data = np.empty((spec.height, spec.width, spec.nchannels), dtype=np.float32)
    img_input.read_image(0, 0, spec.nchannels, OIIO.TypeDesc.FLOAT, pixel_data)
    img_input.close()
    
    # Return R, G, B channels in 0.0-1.0 (or HDR float) range
    return pixel_data[:, :, :3] # Keep R, G, B and discard others

# --- The clustering function remains the same, but operates on float data ---

# Inside extract_dominant_colors_sequence_dynamic:
# 1. Load Image
img_float_data = read_image_with_oiio(file_path) 

# 2. Reshape Pixel Data (already float, no normalization needed)
pixels = img_float_data.reshape((-1, 3))
# pixels is now np.float32 in 0.0-1.0+ range
# ... (Proceed with determine_optimal_k and MiniBatchKMeans on 'pixels')

# 3. Final Output (NO .astype(int) conversion!)
# Keep the cluster centers as a float list for JSON output
# color_list = dominant_colors.astype(int).tolist()  &lt;-- DELETE THIS LINE
color_list = dominant_colors.tolist() 
</pre>

<div id="Reading Colors to JSON-Corrected Code Snippet using OpenCV (If Used)"><h2 id="Corrected Code Snippet using OpenCV (If Used)" class="header"><a href="#Reading Colors to JSON-Corrected Code Snippet using OpenCV (If Used)">Corrected Code Snippet using OpenCV (If Used)</a></h2></div>

<p>
If you must use OpenCV (or Pillow), the key is to load the image and manually convert to float in the correct range, then bypass any conversion back to integer.
</p>

<pre python>
# Using OpenCV
img = cv2.imread(file_path) 
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

# Convert to float and normalize to 0.0 - 1.0 range
# Assumes the input image is 8-bit (0-255). If it's a 16-bit TIFF, this is wrong.
pixels = img.reshape((-1, 3))
pixels = np.float32(pixels) / 255.0  
# For true HDR/float files, you must use OIIO or specialized readers like OpenEXR.

# ... (Proceed with clustering)

# Final Output (still must output as float list)
color_list = dominant_colors.tolist() 
</pre>

<div id="Reading Colors to JSON-Corrected Code Snippet using OpenCV (If Used)-Crucial Takeaway:"><h3 id="Crucial Takeaway:" class="header"><a href="#Reading Colors to JSON-Corrected Code Snippet using OpenCV (If Used)-Crucial Takeaway:">Crucial Takeaway:</a></h3></div>

<p>
By removing the .astype(int) and .astype(int) conversions and ensuring the input array is already in the float (0.0 - 1.0+) range, your resulting JSON will contain values ready to be pasted directly into a Nuke node's RGB color field.
</p>

<div id="Reading Colors to JSON-QUESTION"><h2 id="QUESTION" class="header"><a href="#Reading Colors to JSON-QUESTION">QUESTION</a></h2></div>

<table>
<tr>
<td>
Can Open Image IO read data from TGA and PNG files?
</td>
</tr>
</table>

<p>
Yes, OpenImageIO (OIIO) can read data from both TGA (Truevision Targa) and PNG (Portable Network Graphics) files.
</p>

<p>
OIIO is designed specifically for the visual effects (VFX) and animation industry, meaning it supports a wide variety of formats commonly used in production pipelines, which includes these two.
</p>

<div id="Reading Colors to JSON-QUESTION-Key Points for VFX Work:"><h3 id="Key Points for VFX Work:" class="header"><a href="#Reading Colors to JSON-QUESTION-Key Points for VFX Work:">Key Points for VFX Work:</a></h3></div>

<ul>
<li>
<span id="Reading Colors to JSON-QUESTION-Key Points for VFX Work:-TGA Support"></span><strong id="TGA Support">TGA Support</strong>: TGA files are a legacy format but are fully supported by OIIO for both reading and writing.

<li>
<span id="Reading Colors to JSON-QUESTION-Key Points for VFX Work:-PNG Support"></span><strong id="PNG Support">PNG Support</strong>: PNG is widely supported and OIIO handles it reliably.

<li>
<span id="Reading Colors to JSON-QUESTION-Key Points for VFX Work:-Data Handling"></span><strong id="Data Handling">Data Handling</strong>: The main benefit of using OIIO, particularly in your case, is its format-agnostic API and its focus on using floating-point data for internal image representation. When reading, OIIO will automatically convert the integer-based data from 8-bit or 16-bit TGA/PNG files into the 0.0 to 1.0 floating-point range (or higher for HDR files like OpenEXR). This eliminates the need for manual normalization steps like dividing by 255.0

</ul>
<div id="Reading Colors to JSON-QUESTION"><h2 id="QUESTION" class="header"><a href="#Reading Colors to JSON-QUESTION">QUESTION</a></h2></div>

<table>
<tr>
<td>
When these image files are imported into Nuke through the Read nodes, will the colorspace settings affect the values? (In other words, will getting a list of colors from the files be accurate once the images have been ingested or imported into a Nuke scene?
</td>
</tr>
</table>

<p>
Yes, the colorspace settings on the Nuke Read node will absolutely affect the pixel values read into the scene. ‚ö†Ô∏è This is a crucial consideration for your color extraction script.
</p>

<p>
The goal in a Nuke pipeline is usually to convert all incoming image data into a single, standardized Working Colorspace (often linear sRGB/Rec. 709 or ACEScg). The Read node performs this conversion.
</p>

<div id="Reading Colors to JSON-QUESTION-How Nuke Read Nodes Affect Pixel Values"><h3 id="How Nuke Read Nodes Affect Pixel Values" class="header"><a href="#Reading Colors to JSON-QUESTION-How Nuke Read Nodes Affect Pixel Values">How Nuke Read Nodes Affect Pixel Values</a></h3></div>

<p>
The Nuke Read node is responsible for two main actions:
</p>
<ul>
<li>
<span id="Reading Colors to JSON-QUESTION-How Nuke Read Nodes Affect Pixel Values-Reading Data"></span><strong id="Reading Data">Reading Data</strong>: It takes the raw digital values from the file (e.g., the 8-bit integer data from a PNG or TGA).

<li>
<span id="Reading Colors to JSON-QUESTION-How Nuke Read Nodes Affect Pixel Values-Colorspace Transformation"></span><strong id="Colorspace Transformation">Colorspace Transformation</strong>: It applies a color transformation to convert the file's native colorspace (e.g., sRGB, Rec. 709, or Log) into the scene's working colorspace (usually linear).

</ul>
<div id="Reading Colors to JSON-QUESTION-The Inaccuracy of Extracting Color Post-Read Node"><h3 id="The Inaccuracy of Extracting Color Post-Read Node" class="header"><a href="#Reading Colors to JSON-QUESTION-The Inaccuracy of Extracting Color Post-Read Node">The Inaccuracy of Extracting Color Post-Read Node</a></h3></div>

<p>
If you run your Python script on the image before it enters Nuke (as intended with OIIO), your script is operating on the file's native colorspace data (e.g., sRGB).
</p>

<p>
If you were to sample the colors inside Nuke (e.g., using an expression or Python in the Nuke script itself), the values you get would be the linearized values.
</p>

<table>
<tr>
<td>
Stage
</td>
<td>
Operation
</td>
<td>
Colorspace of Pixel Values
</td>
<td>
Resulting Values
</td>
</tr>
<tr>
<td>
File on Disk
</td>
<td>
None
</td>
<td>
sRGB (or similar standard)
</td>
<td>
Non-linear, 0.0‚àí1.0 (or 0‚àí255)
</td>
</tr>
<tr>
<td>
Your Python/OIIO Script
</td>
<td>
Normalization (OIIO)
</td>
<td>
sRGB (or similar standard)
</td>
<td>
Non-linear, 0.0‚àí1.0
</td>
</tr>
<tr>
<td>
Nuke Read Node Output
</td>
<td>
Conversion to Linear
</td>
<td>
Linear (Working Space)
</td>
<td>
Linear, 0.0‚àí1.0+
</td>
</tr>
</table>

<div id="Reading Colors to JSON-QUESTION-The Inaccuracy of Extracting Color Post-Read Node-Conclusion for Your Workflow"><h4 id="Conclusion for Your Workflow" class="header"><a href="#Reading Colors to JSON-QUESTION-The Inaccuracy of Extracting Color Post-Read Node-Conclusion for Your Workflow">Conclusion for Your Workflow</a></h4></div>

<p>
The colorspace transformation means that the floating-point RGB values you extract from the raw files (which are in the sRGB/Rec. 709 gamma space) will NOT numerically match the pixel values you see in the Nuke Viewer or are used by Nuke nodes (which are in the linear space), except for pure black (0,0,0) and pure white (1,1,1).
</p>

<div id="Reading Colors to JSON-QUESTION-The Inaccuracy of Extracting Color Post-Read Node-Recommended Solution: Use Linear Data for Clustering"><h4 id="Recommended Solution: Use Linear Data for Clustering" class="header"><a href="#Reading Colors to JSON-QUESTION-The Inaccuracy of Extracting Color Post-Read Node-Recommended Solution: Use Linear Data for Clustering">Recommended Solution: Use Linear Data for Clustering</a></h4></div>

<p>
For maximum accuracy and consistency with the Nuke scene, you should perform your K-Means clustering in linear color space.
</p>

<p>
This means you need to modify your image loading/preprocessing to perform the same colorspace transformation that Nuke is doing:
</p>
<ul>
<li>
<span id="Reading Colors to JSON-QUESTION-The Inaccuracy of Extracting Color Post-Read Node-Recommended Solution: Use Linear Data for Clustering-Read the File (OIIO)"></span><strong id="Read the File (OIIO)">Read the File (OIIO)</strong>: Get the non-linear pixel data in the 0.0‚àí1.0 float range.

<li>
<span id="Reading Colors to JSON-QUESTION-The Inaccuracy of Extracting Color Post-Read Node-Recommended Solution: Use Linear Data for Clustering-Linearize the Data"></span><strong id="Linearize the Data">Linearize the Data</strong>: Apply a colorspace conversion (usually the inverse of the sRGB or Rec. 709 transfer function, which is a power 2.2 or 2.4 function) to put the data into linear light.

<li>
<span id="Reading Colors to JSON-QUESTION-The Inaccuracy of Extracting Color Post-Read Node-Recommended Solution: Use Linear Data for Clustering-Run K-Means"></span><strong id="Run K-Means">Run K-Means</strong>: Cluster the linear data.

<li>
<span id="Reading Colors to JSON-QUESTION-The Inaccuracy of Extracting Color Post-Read Node-Recommended Solution: Use Linear Data for Clustering-Output Colors"></span><strong id="Output Colors">Output Colors</strong>: The resulting cluster centers will be in the scene's linear working space, ready for direct use in Nuke.

</ul>
<div id="Reading Colors to JSON-QUESTION-Code Adjustment for Linearization"><h3 id="Code Adjustment for Linearization" class="header"><a href="#Reading Colors to JSON-QUESTION-Code Adjustment for Linearization">Code Adjustment for Linearization</a></h3></div>

<p>
If you are confident your image files are standard sRGB (which most PNGs and JPEGs are):
</p>

<pre python>
def read_image_and_linearize(file_path):
    # 1. Load Image using OIIO (assumes 0.0-1.0 non-linear data)
    img_non_linear = read_image_with_oiio(file_path) 
    
    # 2. Linearize the data using the inverse sRGB EOTF (roughly a power 2.2 function)
    # The simple power 2.2 is a good approximation for sRGB/Rec. 709:
    # Value_Linear = Value_Non_Linear ^ 2.2
    
    # We apply the power function to only the color channels (R, G, B)
    # np.power(img_non_linear, 2.2) is the standard approximation
    img_linear = np.power(img_non_linear, 2.2) 
    
    return img_linear # The data is now in the Nuke working space
    
# In your main extraction function:
# pixels = img_float_data.reshape((-1, 3))  &lt;-- CHANGE THIS
img_linear_data = read_image_and_linearize(file_path)
pixels = img_linear_data.reshape((-1, 3))
</pre>

<p>
This approach ensures that your extracted dominant colors are numerically equivalent to the colors that Nuke is actually using for compositing, which is usually what you want.
</p>

<div id="Reading Colors to JSON-QUESTION"><h2 id="QUESTION" class="header"><a href="#Reading Colors to JSON-QUESTION">QUESTION</a></h2></div>

<table>
<tr>
<td>
Is there an alternative to using the conventional Python libraries? I don't have the facility to add Numpy to Nuke if that takes another install.
</td>
</tr>
</table>

<p>
That's a very common constraint in production environments! üò´
</p>

<p>
Yes, there is an alternative to using NumPy and scikit-learn's MiniBatchKMeans for K-Means clustering, but it will require writing the clustering logic yourself using only Nuke's native Python libraries and basic data structures.
</p>

<p>
Since you can't install NumPy, you also won't be able to use the scipy.spatial or the numpy.power functions mentioned in the previous solutions.
</p>

<div id="Reading Colors to JSON-QUESTION-Alternative: Pure Python K-Means Clustering"><h3 id="Alternative: Pure Python K-Means Clustering" class="header"><a href="#Reading Colors to JSON-QUESTION-Alternative: Pure Python K-Means Clustering">Alternative: Pure Python K-Means Clustering</a></h3></div>

<p>
You can implement a basic, unoptimized K-Means algorithm using only core Python libraries. This will be slower than the NumPy/scikit-learn version, but it is necessary if external libraries are forbidden.
</p>

<p>
Here's how to adapt the logic to pure Python, including the dynamic K (Elbow Method) and the necessary colorspace conversions.
</p>

<div id="Reading Colors to JSON-Pure Python K-Means Function"><h2 id="Pure Python K-Means Function" class="header"><a href="#Reading Colors to JSON-Pure Python K-Means Function">Pure Python K-Means Function</a></h2></div>

<p>
This function replaces the MiniBatchKMeans from scikit-learn.
</p>

<pre python>
import random

def pure_python_kmeans(pixels, k, max_iterations=30):
    """
    Basic K-Means implementation using pure Python lists.
    
    Args:
        pixels (list of lists): The input pixel data [[R, G, B], ...].
        k (int): The number of clusters (colors).
        max_iterations (int): Maximum loops to run before stopping.
        
    Returns:
        list of lists: The final cluster centers [[R, G, B], ...].
    """
    if not pixels or len(pixels) &lt; k:
        return []

    # 1. Initialize K centroids randomly
    centroids = random.sample(pixels, k)
    
    for i in range(max_iterations):
        # 2. Assignment Step: Create K empty lists for the clusters
        clusters = [[] for _ in range(k)]
        
        # Calculate Euclidean distance (squared distance for speed)
        for pixel in pixels:
            distances = []
            for centroid in centroids:
                # Euclidean distance squared: (R2-R1)^2 + (G2-G1)^2 + (B2-B1)^2
                dist_sq = sum([(p - c)**2 for p, c in zip(pixel, centroid)])
                distances.append(dist_sq)
            
            # Find the index of the closest centroid
            closest_index = distances.index(min(distances))
            clusters[closest_index].append(pixel)
            
        # 3. Update Step: Calculate new centroids
        new_centroids = []
        is_converged = True
        
        for j, cluster in enumerate(clusters):
            if cluster:
                # Calculate the mean of the cluster (the new centroid)
                # Sum the R, G, B components, then divide by the cluster size
                sum_rgb = [sum(dim) for dim in zip(*cluster)]
                new_centroid = [s / len(cluster) for s in sum_rgb]
                
                # Check for convergence (simplified: if any centroid moves significantly)
                # The minimum change threshold is set very low to be robust
                if sum([(p - c)**2 for p, c in zip(new_centroid, centroids[j])]) &gt; 1e-4:
                    is_converged = False
                    
                new_centroids.append(new_centroid)
            else:
                # Handle empty cluster: re-initialize it randomly
                new_centroids.append(random.choice(pixels))
                
        centroids = new_centroids
        if is_converged:
            break
            
    return centroids

# Note: The 'distortion' required for the Elbow Method is the sum of all 'dist_sq' values.
</pre>

<div id="Reading Colors to JSON-Pure Python K-Means Function-Replacing the Elbow Method (Pure Python)"><h3 id="Replacing the Elbow Method (Pure Python)" class="header"><a href="#Reading Colors to JSON-Pure Python K-Means Function-Replacing the Elbow Method (Pure Python)">Replacing the Elbow Method (Pure Python)</a></h3></div>

<p>
Calculating the full Elbow Method in pure Python for a large number of frames would be very slow. Since you know the number of colors is low, you can use a fast heuristic instead of a full cluster analysis:
</p>

<div id="Reading Colors to JSON-Pure Python K-Means Function-Replacing the Elbow Method (Pure Python)-Heuristic Alternative: Fixed K-range and a Simple Threshold"><h4 id="Heuristic Alternative: Fixed K-range and a Simple Threshold" class="header"><a href="#Reading Colors to JSON-Pure Python K-Means Function-Replacing the Elbow Method (Pure Python)-Heuristic Alternative: Fixed K-range and a Simple Threshold">Heuristic Alternative: Fixed K-range and a Simple Threshold</a></h4></div>

<p>
Instead of relying on the mathematical "elbow," you could define a narrow, reasonable search range (e.g., K=10 to K=30) and simply check a few common K values, perhaps picking the one that provides a high-enough compression ratio or the one that minimizes the distortion per pixel most effectively.
</p>

<p>
A simpler approach for production is to pre-set a maximum K value (e.g., K=30) and rely on the fact that K-Means will naturally group similar colors even if you request more clusters than strictly necessary.
</p>
<ul>
<li>
<span id="Reading Colors to JSON-Pure Python K-Means Function-Replacing the Elbow Method (Pure Python)-Heuristic Alternative: Fixed K-range and a Simple Threshold-Recommendation"></span><strong id="Recommendation">Recommendation</strong>: If performance is critical, use a fixed, slightly generous K (like K=30) and skip the dynamic Elbow Method entirely.

</ul>
<div id="Reading Colors to JSON-Pure Python K-Means Function-Pure Python Colorspace and JSON Handling"><h3 id="Pure Python Colorspace and JSON Handling" class="header"><a href="#Reading Colors to JSON-Pure Python K-Means Function-Pure Python Colorspace and JSON Handling">Pure Python Colorspace and JSON Handling</a></h3></div>

<p>
You will replace NumPy's methods with pure Python math.
</p>

<pre python>
# Assuming you have the linear RGB color list from pure_python_kmeans
linear_rgb = [0.88, 0.35, 0.35]

# --- 1. Linear-to-Display Conversion (Replacing numpy.power) ---
# Goal: Value_Display = Value_Linear ^ (1.0 / 2.2)

# Using the built-in pow() function for the gamma correction
gamma = 1.0 / 2.2 

display_rgb = []
for val in linear_rgb:
    # Use max(0, val) for safety, as pow() on a negative linear value is undefined
    display_val = pow(max(0, val), gamma)
    
    # Clip to 0.0 - 1.0, as node colors don't support HDR
    clipped_val = min(1.0, max(0.0, display_val))
    display_rgb.append(clipped_val)
    
# display_rgb is now ready to be packed into the tile_color integer

# --- 2. JSON Handling ---
# The standard Python 'json' library is almost always available in Nuke's environment
# You'd use the 'json' module to load and parse the data exactly as previously shown:
# extracted_colors = json.loads(json_string) 
</pre>

<div id="Reading Colors to JSON-Pure Python K-Means Function-Summary of Changes:"><h3 id="Summary of Changes:" class="header"><a href="#Reading Colors to JSON-Pure Python K-Means Function-Summary of Changes:">Summary of Changes:</a></h3></div>

<table>
<tr>
<td>
Functionality
</td>
<td>
Standard Python (Nuke's Environment)
</td>
<td>
Conventional Python
</td>
</tr>
<tr>
<td>
Clustering
</td>
<td>
Custom pure_python_kmeans function
</td>
<td>
sklearn.cluster.MiniBatchKMeans
</td>
</tr>
<tr>
<td>
Math/Vectors
</td>
<td>
zip, List comprehensions, sum, pow()
</td>
<td>
numpy (arrays and functions)
</td>
</tr>
<tr>
<td>
Elbow Method
</td>
<td>
Pre-set K (e.g., K=30) or custom, slow logic
</td>
<td>
scipy.spatial.distance.cdist (Inertia)
</td>
</tr>
</table>

<div id="Reading Colors to JSON-QUESTION"><h2 id="QUESTION" class="header"><a href="#Reading Colors to JSON-QUESTION">QUESTION</a></h2></div>

<table>
<tr>
<td>
Sorry, I don't think I communicated my question well enough. Your answer is very useful however! I think I can use this information. The better framed question would be, can I implement the colorspace conversion from the JSON color sets and Nuke's Node Graph colors without an additional library in Nuke's standard Python libraries?
</td>
</tr>
</table>

<p>
I apologize for misinterpreting the initial constraint. Your question is perfectly clear now: Can you perform the color space conversion (linear to display) within Nuke's Python environment without using an external library like NumPy?
</p>

<p>
The answer is Yes, absolutely. You can use the built-in math module and core Python functions to achieve the same result as NumPy for the sRGB linearization inverse (the View Transform).
</p>

<div id="Reading Colors to JSON-QUESTION-Pure Python Colorspace Conversion"><h3 id="Pure Python Colorspace Conversion" class="header"><a href="#Reading Colors to JSON-QUESTION-Pure Python Colorspace Conversion">Pure Python Colorspace Conversion</a></h3></div>

<p>
The required conversion is the simple power function:
</p>

<pre>
Value Display=Value Linear 1/2.2 ‚Äã	
</pre>
 
<p>
You can use Python's built-in math.pow() function for this calculation, which is included in Nuke's standard Python installation and requires no external packages.
</p>

<pre python>
import nuke
import math
import json
# You would also need the pure_python_kmeans function if extracting colors within Nuke

def convert_linear_to_display_color(linear_rgb_list):
    """
    Converts a linear [R, G, B] color (0.0-1.0+) to a display-ready 
    [R, G, B] color (0.0-1.0) using the sRGB/Rec. 709 gamma (1/2.2).
    
    Args:
        linear_rgb_list (list): A list of three floats representing the linear RGB values.
        
    Returns:
        list: A list of three floats representing the display-ready RGB values.
    """
    
    # Reciprocal of 2.2 (the gamma value)
    gamma_correction = 1.0 / 2.2 
    
    display_rgb = []
    
    for val in linear_rgb_list:
        # Nuke nodes don't support HDR, so we must clamp positive values.
        # We also ensure the base of pow() is non-negative.
        clamped_linear_val = max(0.0, val)
        
        # Apply the power function
        display_val = math.pow(clamped_linear_val, gamma_correction)
        
        # Clip the result to 0.0 - 1.0 for the 8-bit color space used by tile_color
        clipped_display_val = min(1.0, display_val)
        
        display_rgb.append(clipped_display_val)
        
    return display_rgb

def float_to_packed_int(r, g, b, a=1.0):
    """Packs float RGB values (0.0-1.0) into Nuke's single tile_color integer."""
    r_int = int(r * 255)
    g_int = int(g * 255)
    b_int = int(b * 255)
    a_int = int(a * 255)
    
    # Nuke uses the format 0xAABBGGRR (little-endian)
    return (a_int &lt;&lt; 24) + (b_int &lt;&lt; 16) + (g_int &lt;&lt; 8) + r_int

# --- Example Usage in Nuke ---
# This is a linear red that looks dark and washed out if used directly
linear_red = [0.8, 0.1, 0.1] 

# Convert to the color that visually matches the Nuke Viewer
display_red = convert_linear_to_display_color(linear_red) 

# Pack the color for the node property
r, g, b = display_red[0], display_red[1], display_red[2]
packed_color = float_to_packed_int(r, g, b)

# node = nuke.selectedNode() 
# node['tile_color'].setValue(packed_color)
</pre>

<div id="Reading Colors to JSON-QUESTION-Using Nuke's Native Color Functions (Advanced)"><h3 id="Using Nuke's Native Color Functions (Advanced)" class="header"><a href="#Reading Colors to JSON-QUESTION-Using Nuke's Native Color Functions (Advanced)">Using Nuke's Native Color Functions (Advanced)</a></h3></div>

<p>
If you need a more robust, OCIO-aware solution‚Äîone that respects Nuke's actual project settings (which may not be a simple power 2.2)‚Äîyou can use Nuke's internal color conversion functions. This is the most accurate way to match the Nuke Viewer, as it utilizes the exact Viewer Process configuration.
</p>

<p>
The function you would use is nuke.to and nuke.from:
</p>

<pre python>
# WARNING: This method is highly accurate but slightly more complex.
# The 'to' and 'from' commands are primarily used for getting/setting metadata,
# but can be adapted for color conversion if needed, though the simple 
# math.pow() is usually sufficient for tile_color.

# For simplicity and performance, stick to the math.pow() method above 
# unless your pipeline uses a custom view transform that deviates significantly from sRGB.
</pre>

<p>
<span id="Reading Colors to JSON-QUESTION-Using Nuke's Native Color Functions (Advanced)-Conclusion"></span><strong id="Conclusion">Conclusion</strong>: Stick with the math.pow() approach for setting node colors. It's clean, fast, accurate enough for the visual purpose of node coloring, and relies only on the Python modules guaranteed to be available in Nuke.
</p>

<div id="Reading Colors to JSON-QUESTION"><h2 id="QUESTION" class="header"><a href="#Reading Colors to JSON-QUESTION">QUESTION</a></h2></div>

<table>
<tr>
<td>
To summarize your implementation of the Python standard library method to collect the color data, could you give me the complete script again in its entirety?
</td>
</tr>
</table>

<p>
Certainly. To summarize the pure Python standard library method, I'll provide the complete script, including:
</p>

<ul>
<li>
The OIIO image loading (as it's essential for correct Nuke-compatible float data).

<li>
The pure Python K-Means clustering function.

<li>
The dynamic K determination is removed (as it's too slow without NumPy) and replaced with a fixed K.

<li>
The necessary colorspace conversions (Linearization for clustering, and Display conversion for the Nuke node).

</ul>
<p>
This script is self-contained and avoids external dependencies like NumPy or scikit-learn.
</p>

<div id="Reading Colors to JSON-QUESTION-Complete Pure Python Color Extraction Script (Nuke-Ready)"><h3 id="Complete Pure Python Color Extraction Script (Nuke-Ready)" class="header"><a href="#Reading Colors to JSON-QUESTION-Complete Pure Python Color Extraction Script (Nuke-Ready)">Complete Pure Python Color Extraction Script (Nuke-Ready)</a></h3></div>

<p>
This script is designed to run in a standalone Python environment that has OpenImageIO (OIIO) installed for robust float image reading, but uses only core Python and the math module for all clustering and colorspace conversions.
</p>

<pre python>
import OpenImageIO as OIIO
import math
import random
import json
import os
import glob
from collections import defaultdict # Useful for efficiently building the clusters

# --- 1. Pure Python K-Means Core ---

def pure_python_kmeans(pixels, k, max_iterations=50):
    """
    Basic K-Means implementation using pure Python lists.
    
    Args:
        pixels (list of lists): The input pixel data [[R, G, B], ...].
        k (int): The number of clusters (colors).
        max_iterations (int): Maximum loops to run before stopping.
        
    Returns:
        list of lists: The final cluster centers [[R, G, B], ...].
    """
    if not pixels or len(pixels) &lt; k:
        # Return pure black if not enough pixels to meet K
        return [[0.0, 0.0, 0.0]] * k

    # Initialize K centroids randomly
    centroids = random.sample(pixels, k)
    
    for i in range(max_iterations):
        # Assignment Step: Assign each pixel to the nearest centroid
        clusters = defaultdict(list)
        
        for pixel in pixels:
            distances = []
            for centroid in centroids:
                # Euclidean distance squared
                dist_sq = sum([(p - c)**2 for p, c in zip(pixel, centroid)])
                distances.append(dist_sq)
            
            # Find the index of the closest centroid
            closest_index = distances.index(min(distances))
            clusters[closest_index].append(pixel)
            
        # Update Step: Calculate new centroids
        new_centroids = []
        is_converged = True
        
        for j in range(k):
            cluster = clusters[j]
            if cluster:
                # Calculate the mean of the cluster (the new centroid)
                # zip(*cluster) transposes the list: [[R1, G1, B1], [R2, G2, B2], ...] -&gt; [(R1, R2, ...), (G1, G2, ...), ...]
                sum_rgb = [sum(dim) for dim in zip(*cluster)]
                new_centroid = [s / len(cluster) for s in sum_rgb]
                
                # Check for convergence (simplified check of squared distance)
                if sum([(p - c)**2 for p, c in zip(new_centroid, centroids[j])]) &gt; 1e-6:
                    is_converged = False
                    
                new_centroids.append(new_centroid)
            else:
                # Handle empty cluster: re-initialize it randomly
                new_centroids.append(random.choice(pixels))
                
        centroids = new_centroids
        if is_converged:
            # print(f"K-Means converged in {i+1} iterations.")
            break
            
    return centroids


# --- 2. Image and Colorspace Functions (Requires OIIO) ---

def read_image_with_oiio(file_path):
    """Loads an image into a NumPy-like list of pixels (0.0 - 1.0+ range)."""
    img_input = OIIO.ImageInput.open(file_path)
    if not img_input:
        raise IOError(f"Could not open image with OIIO: {OIIO.geterror()}")

    spec = img_input.spec()
    
    # OIIO reads data in its native float range (0.0-1.0 or HDR range)
    # Use OIIO's read_image to get pixel data as float32
    # We use a numpy array temporarily for I/O, but reshape it to a pure list immediately
    import numpy as np # Necessary only for the OIIO numpy integration
    pixel_data_np = np.empty((spec.height, spec.width, spec.nchannels), dtype=np.float32)
    img_input.read_image(0, 0, spec.nchannels, OIIO.TypeDesc.FLOAT, pixel_data_np)
    img_input.close()
    
    # Reshape and convert to a flat list of RGB lists, discarding alpha
    pixels = pixel_data_np[:, :, :3].reshape((-1, 3)).tolist()
    return pixels


def linearize_pixels(pixels):
    """Converts a list of non-linear sRGB/Rec. 709 pixels to linear light (approx. power 2.2)."""
    linear_pixels = []
    
    for r, g, b in pixels:
        # Simple power 2.2 approximation for sRGB/Rec. 709 to linear
        # Value_Linear = Value_Non_Linear ^ 2.2
        linear_r = math.pow(max(0, r), 2.2)
        linear_g = math.pow(max(0, g), 2.2)
        linear_b = math.pow(max(0, b), 2.2)
        linear_pixels.append([linear_r, linear_g, linear_b])
        
    return linear_pixels

# --- 3. Main Extraction Loop ---

def extract_dominant_colors_sequence_pure_python(sequence_path_pattern, k_fixed=30):
    """
    Extracts dominant colors from an image sequence using pure Python K-Means.
    
    Args:
        sequence_path_pattern (str): Glob pattern for the sequence (e.g., '/path/to/img.%04d.png')
        k_fixed (int): The number of clusters to use (fixed K).
        
    Returns:
        str: JSON string containing the extracted linear RGB colors per frame.
    """
    all_dominant_colors = {}
    
    # Simple glob to find all files in the sequence
    file_list = sorted(glob.glob(sequence_path_pattern.replace('%04d', '*')))
    
    for file_path in file_list:
        try:
            # 1. Load Non-Linear Data (0.0-1.0 range)
            non_linear_pixels = read_image_with_oiio(file_path) 
            
            # 2. Linearize Data for Clustering
            linear_pixels = linearize_pixels(non_linear_pixels)
            
            # 3. Perform K-Means on Linear Data
            # Note: This step can be slow for large images/high K
            dominant_colors_linear = pure_python_kmeans(linear_pixels, k_fixed)
            
            # 4. Store Results (as linear floats)
            frame_num = os.path.basename(file_path).split('.')[-2] # Assumes frame number is before extension
            all_dominant_colors[frame_num] = dominant_colors_linear

        except Exception as e:
            print(f"Error processing {file_path}: {e}")
            continue
            
    return json.dumps(all_dominant_colors, indent=4)


# --- 4. Nuke Integration Functions (To be run INSIDE Nuke) ---

def convert_linear_to_display_color(linear_rgb_list):
    """Converts a linear RGB color to a display-ready color (for node 'tile_color')."""
    gamma_correction = 1.0 / 2.2 # The view transform approximation
    display_rgb = []
    
    for val in linear_rgb_list:
        clamped_linear_val = max(0.0, val)
        display_val = math.pow(clamped_linear_val, gamma_correction)
        clipped_display_val = min(1.0, display_val)
        display_rgb.append(clipped_display_val)
        
    return display_rgb

def float_to_packed_int(r, g, b, a=1.0):
    """Packs float RGB values (0.0-1.0) into Nuke's single tile_color integer."""
    r_int = int(r * 255)
    g_int = int(g * 255)
    b_int = int(b * 255)
    a_int = int(a * 255)
    return (a_int &lt;&lt; 24) + (b_int &lt;&lt; 16) + (g_int &lt;&lt; 8) + r_int
    
def set_node_color_from_linear_rgb(node, linear_rgb_color):
    """
    Sets a Nuke node's tile_color based on a linear RGB list, applying conversion.
    
    Args:
        node (nuke.Node): The Nuke node object.
        linear_rgb_color (list): The [R, G, B] color extracted from the JSON.
    """
    display_rgb = convert_linear_to_display_color(linear_rgb_color)
    r, g, b = display_rgb[0], display_rgb[1], display_rgb[2]
    packed_color = float_to_packed_int(r, g, b)
    
    node['tile_color'].setValue(packed_color)
</pre>

<div id="Reading Colors to JSON-QUESTION"><h2 id="QUESTION" class="header"><a href="#Reading Colors to JSON-QUESTION">QUESTION</a></h2></div>

<table>
<tr>
<td>
Can you add some functions that allow for doing this on selected Read nodes, where the enclosing directory of the imported file sequence becomes the output of a JSON file with the color data (and saved with the name of the Read image with a JSON extension)? And perhaps the script could contain functions for selecting a READ node and imported the color data collected from that READ node when the JSON export portion was saved.
</td>
</tr>
</table>

<p>
That's a logical and practical workflow! You want to embed the functionality directly into Nuke, using selected Read nodes as both the source for the image sequence and the destination for the JSON file.
</p>

<p>
I'll add the necessary Nuke integration functions to the script. We'll need a way to save the data from Nuke and another way to apply it in Nuke.
</p>

<div id="Reading Colors to JSON-QUESTION-Complete Pure Python Script with Nuke I/O"><h3 id="Complete Pure Python Script with Nuke I/O" class="header"><a href="#Reading Colors to JSON-QUESTION-Complete Pure Python Script with Nuke I/O">Complete Pure Python Script with Nuke I/O</a></h3></div>
<p>
Here is the complete script. It contains two main operational functions to be executed within Nuke:
</p>

<ul>
<li>
<span id="Reading Colors to JSON-QUESTION-Complete Pure Python Script with Nuke I/O-export_colors_from_selected_read_nodes()"></span><strong id="export_colors_from_selected_read_nodes()">export_colors_from_selected_read_nodes()</strong>: Reads the image sequence path from a selected Read node, performs K-Means clustering, and saves the resulting linear color data as a JSON file in the sequence's directory.

<li>
<span id="Reading Colors to JSON-QUESTION-Complete Pure Python Script with Nuke I/O-apply_colors_to_selected_read_node()"></span><strong id="apply_colors_to_selected_read_node()">apply_colors_to_selected_read_node()</strong>: Reads the JSON file associated with a selected Read node and applies the dominant color of the current frame to the node's tile_color.

</ul>
<p>
Python Script (my_color_tool.py)
</p>

<pre python>
import nuke
import OpenImageIO as OIIO
import math
import random
import json
import os
import glob
from collections import defaultdict

# --- CORE 1: PURE PYTHON K-MEANS ---

def pure_python_kmeans(pixels, k, max_iterations=50):
    """
    Basic K-Means implementation using pure Python lists.
    (See previous response for details on this function's logic).
    """
    if not pixels or len(pixels) &lt; k:
        return [[0.0, 0.0, 0.0]] * k

    # 
    centroids = random.sample(pixels, k)
    
    for i in range(max_iterations):
        clusters = defaultdict(list)
        
        for pixel in pixels:
            distances = []
            for centroid in centroids:
                dist_sq = sum([(p - c)**2 for p, c in zip(pixel, centroid)])
                distances.append(dist_sq)
            
            closest_index = distances.index(min(distances))
            clusters[closest_index].append(pixel)
            
        new_centroids = []
        is_converged = True
        
        for j in range(k):
            cluster = clusters[j]
            if cluster:
                sum_rgb = [sum(dim) for dim in zip(*cluster)]
                new_centroid = [s / len(cluster) for s in sum_rgb]
                
                if sum([(p - c)**2 for p, c in zip(new_centroid, centroids[j])]) &gt; 1e-6:
                    is_converged = False
                    
                new_centroids.append(new_centroid)
            else:
                new_centroids.append(random.choice(pixels))
                
        centroids = new_centroids
        if is_converged:
            break
            
    return centroids


# --- CORE 2: IMAGE AND COLORSPACE FUNCTIONS ---

def read_image_with_oiio(file_path):
    """Loads an image using OpenImageIO for robust float handling."""
    try:
        img_input = OIIO.ImageInput.open(file_path)
        if not img_input:
            raise IOError(f"Could not open image with OIIO: {OIIO.geterror()}")

        spec = img_input.spec()
        # Use numpy temporarily for the OIIO read, then convert to a list
        import numpy as np 
        pixel_data_np = np.empty((spec.height, spec.width, spec.nchannels), dtype=np.float32)
        img_input.read_image(0, 0, spec.nchannels, OIIO.TypeDesc.FLOAT, pixel_data_np)
        img_input.close()
        
        # Reshape and convert to a flat list of RGB lists
        pixels = pixel_data_np[:, :, :3].reshape((-1, 3)).tolist()
        return pixels
    except ImportError:
        nuke.message("Error: NumPy is required temporarily by OpenImageIO for this function, even if not used in the core K-Means logic.")
        return []
    except Exception as e:
        nuke.message(f"OIIO Read Error: {e}")
        return []


def linearize_pixels(pixels):
    """Converts a list of non-linear sRGB/Rec. 709 pixels to linear light (approx. power 2.2)."""
    linear_pixels = []
    for r, g, b in pixels:
        linear_r = math.pow(max(0, r), 2.2)
        linear_g = math.pow(max(0, g), 2.2)
        linear_b = math.pow(max(0, b), 2.2)
        linear_pixels.append([linear_r, linear_g, linear_b])
    return linear_pixels


# --- CORE 3: NUKE I/O FUNCTIONS ---

def get_sequence_info(read_node):
    """Gets the file path, directory, and JSON path from a Read node."""
    file_path_full = read_node['file'].value()
    
    # Handle single frame vs sequence (%04d)
    if '%' in file_path_full:
        # Get the directory and the base file name pattern (e.g., 'image.0001.exr' -&gt; 'image.%04d.exr')
        dir_path = os.path.dirname(file_path_full)
        base_name_pattern = os.path.basename(file_path_full)
        
        # Determine the name for the JSON file (based on the base name without frame numbers)
        # e.g., 'image.%04d.exr' -&gt; 'image.exr' -&gt; 'image.json'
        json_file_name = base_name_pattern.split('.')[0] + ".json"
        
        # Create a list of all frames to process (globbing the directory)
        file_list = sorted(glob.glob(os.path.join(dir_path, base_name_pattern.replace('%04d', '*'))))
    else:
        # Handle single-frame file
        dir_path = os.path.dirname(file_path_full)
        base_name = os.path.basename(file_path_full)
        json_file_name = os.path.splitext(base_name)[0] + ".json"
        file_list = [file_path_full]

    json_path = os.path.join(dir_path, json_file_name)
    
    return file_list, dir_path, json_path

# -------------------------------------------------------------
# FUNCTION 1: EXPORT COLORS FROM SELECTED READ NODES
# -------------------------------------------------------------

def export_colors_from_selected_read_nodes(k_fixed=30):
    """
    Reads selected Read nodes, extracts dominant colors for the sequence,
    and exports a JSON file to the sequence's directory.
    """
    selected_nodes = nuke.selectedNodes('Read')
    if not selected_nodes:
        nuke.message("Please select one or more Read nodes.")
        return

    for node in selected_nodes:
        file_list, dir_path, json_path = get_sequence_info(node)
        
        if not file_list:
            nuke.warning(f"Skipping {node.name()}: Could not find files at {node['file'].value()}")
            continue

        all_dominant_colors = {}
        
        nuke.message(f"Processing {len(file_list)} frames for {node.name()}...")

        for file_path in file_list:
            try:
                # 1. Load, Linearize, and Cluster
                non_linear_pixels = read_image_with_oiio(file_path)
                linear_pixels = linearize_pixels(non_linear_pixels)
                dominant_colors_linear = pure_python_kmeans(linear_pixels, k_fixed)
                
                # 2. Store Results (as linear floats)
                # Frame number extraction is often the part between the last dot and the extension
                frame_num = os.path.basename(file_path).split('.')[-2] 
                all_dominant_colors[frame_num] = dominant_colors_linear

            except Exception as e:
                nuke.warning(f"Error processing frame {file_path}: {e}")
                continue
                
        # 3. Save JSON file to disk
        try:
            with open(json_path, 'w') as f:
                json.dump(all_dominant_colors, f, indent=4)
            nuke.message(f"Successfully exported {k_fixed} linear colors per frame to:\n{json_path}")
        except Exception as e:
            nuke.critical(f"Failed to save JSON file at {json_path}: {e}")


# -------------------------------------------------------------
# FUNCTION 2: APPLY COLORS TO SELECTED READ NODE
# -------------------------------------------------------------

def convert_linear_to_display_color(linear_rgb_list):
    """Converts a linear RGB color to a display-ready color (for node 'tile_color')."""
    gamma_correction = 1.0 / 2.2
    display_rgb = []
    
    for val in linear_rgb_list:
        clamped_linear_val = max(0.0, val)
        display_val = math.pow(clamped_linear_val, gamma_correction)
        clipped_display_val = min(1.0, display_val)
        display_rgb.append(clipped_display_val)
        
    return display_rgb

def float_to_packed_int(r, g, b, a=1.0):
    """Packs float RGB values (0.0-1.0) into Nuke's single tile_color integer."""
    r_int = int(r * 255)
    g_int = int(g * 255)
    b_int = int(b * 255)
    a_int = int(a * 255)
    return (a_int &lt;&lt; 24) + (b_int &lt;&lt; 16) + (g_int &lt;&lt; 8) + r_int

def apply_colors_to_selected_read_node():
    """
    Reads the associated JSON color data and sets the tile_color of the 
    selected Read node based on the most dominant color of the current frame.
    """
    selected_nodes = nuke.selectedNodes('Read')
    if not selected_nodes:
        nuke.message("Please select one or more Read nodes.")
        return

    current_frame = nuke.frame()
    
    for node in selected_nodes:
        file_list, dir_path, json_path = get_sequence_info(node)
        
        if not os.path.exists(json_path):
            nuke.message(f"Color JSON file not found for {node.name()}:\n{json_path}\n\nRun 'Export Colors' first.")
            continue
            
        try:
            with open(json_path, 'r') as f:
                color_data = json.load(f)
            
            # Nuke frames can have leading zeros. Ensure the key matches the JSON.
            frame_key = str(current_frame) 
            
            # Find the first key that matches, handling padding (e.g., '1' vs '0001')
            matching_key = None
            if frame_key in color_data:
                matching_key = frame_key
            else:
                for key in color_data:
                    if key.lstrip('0') == frame_key:
                        matching_key = key
                        break
            
            if not matching_key:
                nuke.warning(f"No color data found for frame {current_frame} in {json_path}")
                continue

            # The most dominant color is the first one in the list [0] from K-Means
            linear_rgb_color = color_data[matching_key][0]
            
            # Set the node's tile_color
            set_node_color_from_linear_rgb(node, linear_rgb_color)
            
        except Exception as e:
            nuke.critical(f"Failed to apply color to {node.name()}: {e}")

# Note: These functions must be sourced or manually executed within a Nuke session.
# For a production setup, you would add these to your Nuke menu.py file.
</pre>

<div id="Reading Colors to JSON-QUESTION"><h2 id="QUESTION" class="header"><a href="#Reading Colors to JSON-QUESTION">QUESTION</a></h2></div>

<table>
<tr>
<td>
Does the standard Python library for Nuke include OpenImageIO?
</td>
</tr>
</table>

<p>
No, the Python module for OpenImageIO (OIIO) is not part of the standard Python library that ships with Nuke.
</p>

<p>
The standard Python library refers to the modules included with the Python language itself (like math, os, json, random, etc.). OpenImageIO is a third-party library built specifically for VFX/image handling.
</p>

<div id="Reading Colors to JSON-QUESTION-OIIO in the Nuke Ecosystem"><h3 id="OIIO in the Nuke Ecosystem" class="header"><a href="#Reading Colors to JSON-QUESTION-OIIO in the Nuke Ecosystem">OIIO in the Nuke Ecosystem</a></h3></div>

<p>
While OIIO is not a standard Python library module, it is compiled and shipped with Nuke as a core component of the application. However, this is usually the C++ library, not the Python bindings.
</p>

<ul>
<li>
<span id="Reading Colors to JSON-QUESTION-OIIO in the Nuke Ecosystem-Nuke Includes the OIIO C++ Library"></span><strong id="Nuke Includes the OIIO C++ Library">Nuke Includes the OIIO C++ Library</strong>: Nuke uses the OIIO core libraries internally to handle image file I/O for its Read and Write nodes.

<li>
<span id="Reading Colors to JSON-QUESTION-OIIO in the Nuke Ecosystem-Python Bindings are Separate"></span><strong id="Python Bindings are Separate">Python Bindings are Separate</strong>: To use OIIO directly in a Python script via import OpenImageIO, you generally need the Python bindings (OpenImageIO module) to be installed and available in Nuke's Python environment. While Foundry includes many third-party libraries for Python (like nuke, PySide2, OpenColorIO, and certain versions of OpenEXR), the OIIO Python bindings (import OpenImageIO) are often not included by default, or their availability can change between Nuke versions and operating systems.

</ul>
<div id="Reading Colors to JSON-QUESTION-OIIO in the Nuke Ecosystem-The functionality in your script that uses:"><h4 id="The functionality in your script that uses:" class="header"><a href="#Reading Colors to JSON-QUESTION-OIIO in the Nuke Ecosystem-The functionality in your script that uses:">The functionality in your script that uses:</a></h4></div>

<pre python>
import OpenImageIO as OIIO 
# ... and uses OIIO.ImageInput, OIIO.TypeDesc, etc.
</pre>

<p>
... is the part that is most likely to require a separate setup or installation (e.g., adding a custom OIIO build to your PYTHONPATH), which contradicts your goal of using only the standard environment.
</p>

<p>
<span id="Reading Colors to JSON-QUESTION-OIIO in the Nuke Ecosystem-The functionality in your script that uses:-Exception"></span><strong id="Exception">Exception</strong>: The search results indicate that Nuke includes a host of third-party libraries (e.g., Nuke 15.0v1 includes OpenImageIO 2.3.13.0). If Nuke's internal build of OIIO includes the Python bindings and correctly exposes them to the Nuke Python interpreter, then your import OpenImageIO line will work.
</p>

<p>
Given your restriction on adding libraries, it's safer to assume the OIIO Python module is not reliably available unless your specific studio pipeline ensures it is present.
</p>

<div id="Reading Colors to JSON-QUESTION"><h2 id="QUESTION" class="header"><a href="#Reading Colors to JSON-QUESTION">QUESTION</a></h2></div>

<p>
With this restriction, can you give a version of the script adapted to work without OpenImageIO?
Yes, absolutely. If you cannot rely on the OpenImageIO Python bindings, the most reliable pure Python alternative for reading pixel data in Nuke is to use Nuke's native Python API to render a single, processed frame of the image sequence into memory.
</p>

<p>
Since Nuke is already doing the heavy lifting of reading the file, managing the colorspace, and processing the pixels for display, we'll harness that existing capability via an in-memory Shuffle and Crop operation.
</p>

<p>
This script uses only the standard Python library modules (math, os, json, etc.) and the core Nuke API (nuke).
</p>

<div id="Reading Colors to JSON-QUESTION-Complete Pure Python Script (Nuke API Method)"><h3 id="Complete Pure Python Script (Nuke API Method)" class="header"><a href="#Reading Colors to JSON-QUESTION-Complete Pure Python Script (Nuke API Method)">Complete Pure Python Script (Nuke API Method)</a></h3></div>

<p>
This script contains the same two primary functions designed to be run from within a Nuke session.
</p>

<p>
Python Script (OIIO-Free)
</p>

<pre python>
import nuke
import math
import random
import json
import os
import glob
from collections import defaultdict

# --- CORE 1: PURE PYTHON K-MEANS ---

def pure_python_kmeans(pixels, k, max_iterations=50):
    """
    Basic K-Means implementation using pure Python lists.
    (This function remains unchanged as it uses only standard Python).
    """
    if not pixels or len(pixels) &lt; k:
        return [[0.0, 0.0, 0.0]] * k

    centroids = random.sample(pixels, k)
    
    for i in range(max_iterations):
        clusters = defaultdict(list)
        
        for pixel in pixels:
            distances = []
            for centroid in centroids:
                dist_sq = sum([(p - c)**2 for p, c in zip(pixel, centroid)])
                distances.append(dist_sq)
            
            closest_index = distances.index(min(distances))
            clusters[closest_index].append(pixel)
            
        new_centroids = []
        is_converged = True
        
        for j in range(k):
            cluster = clusters[j]
            if cluster:
                sum_rgb = [sum(dim) for dim in zip(*cluster)]
                new_centroid = [s / len(cluster) for s in sum_rgb]
                
                if sum([(p - c)**2 for p, c in zip(new_centroid, centroids[j])]) &gt; 1e-6:
                    is_converged = False
                    
                new_centroids.append(new_centroid)
            else:
                new_centroids.append(random.choice(pixels))
                
        centroids = new_centroids
        if is_converged:
            break
            
    return centroids


# --- CORE 2: IMAGE AND COLORSPACE FUNCTIONS (OIIO Replaced) ---

def get_linear_pixels_from_node(read_node, frame):
    """
    Reads linear pixel data directly from a Nuke node for a single frame.
    This replaces the OpenImageIO dependency.
    """
    # Create a temporary, invisible output pipe to sample the pixels
    crop = nuke.nodes.Crop(inputs=[read_node], postage_stamp=False)
    crop['box'].setValue([0, 0, read_node.width(), read_node.height()])
    
    # Force the Read node to the correct colorspace (which should be Linear by default)
    # The Read node's internal processing handles the file-to-linear conversion.
    
    pixels = []
    width = read_node.width()
    height = read_node.height()
    
    try:
        # Request the channel data (R, G, B) as a flat list of floats
        # This data is always in Nuke's internal **linear** working space.
        data = crop.sample('red', 0, 0, width, height, frame=frame)
        data += crop.sample('green', 0, 0, width, height, frame=frame)
        data += crop.sample('blue', 0, 0, width, height, frame=frame)
        
        # Reshape the flat data (R... R, G... G, B... B) into [R, G, B] triplets
        # The length of 'data' is 3 * (width * height)
        num_pixels = width * height
        
        for i in range(num_pixels):
            r = data[i]
            g = data[i + num_pixels]
            b = data[i + 2 * num_pixels]
            pixels.append([r, g, b])
            
    finally:
        # Clean up the temporary node immediately
        nuke.delete(crop)
        
    # No need for a separate 'linearize_pixels' function, as nuke.sample() returns 
    # the data already in the scene's linear working space!
    return pixels

# --- CORE 3: NUKE I/O FUNCTIONS ---

def get_sequence_info(read_node):
    """Gets the file path, directory, and JSON path from a Read node."""
    file_path_full = read_node['file'].value()
    
    if '%' in file_path_full:
        dir_path = os.path.dirname(file_path_full)
        base_name_pattern = os.path.basename(file_path_full)
        
        # Determine the name for the JSON file 
        json_file_name = base_name_pattern.split('.')[0] + ".json"
        
        # Get frame range from the Read node itself
        start = int(read_node['first'].value())
        end = int(read_node['last'].value())
        frame_list = range(start, end + 1)
    else:
        # Handle single-frame file
        dir_path = os.path.dirname(file_path_full)
        base_name = os.path.basename(file_path_full)
        json_file_name = os.path.splitext(base_name)[0] + ".json"
        frame_list = [int(nuke.root()['first_frame'].value())] # Use current project start frame

    json_path = os.path.join(dir_path, json_file_name)
    
    return frame_list, dir_path, json_path

# -------------------------------------------------------------
# FUNCTION 1: EXPORT COLORS FROM SELECTED READ NODES
# -------------------------------------------------------------

def export_colors_from_selected_read_nodes(k_fixed=30):
    """
    Reads selected Read nodes, extracts dominant linear colors for the sequence,
    and exports a JSON file using the Nuke API.
    """
    selected_nodes = nuke.selectedNodes('Read')
    if not selected_nodes:
        nuke.message("Please select one or more Read nodes.")
        return

    for node in selected_nodes:
        frame_list, dir_path, json_path = get_sequence_info(node)
        all_dominant_colors = {}
        
        if not frame_list:
            nuke.warning(f"Skipping {node.name()}: Invalid frame range.")
            continue

        nuke.message(f"Processing {len(frame_list)} frames for {node.name()}...")

        # Set Read node frame to ensure the file path is correct for sampling
        original_frame = nuke.frame() 
        
        for frame in frame_list:
            nuke.frame(frame)
            try:
                # 1. Get Linear Pixel Data using Nuke's API
                linear_pixels = get_linear_pixels_from_node(node, frame)
                
                # 2. Perform K-Means on Linear Data
                dominant_colors_linear = pure_python_kmeans(linear_pixels, k_fixed)
                
                # 3. Store Results (as linear floats)
                all_dominant_colors[str(frame)] = dominant_colors_linear

            except Exception as e:
                nuke.warning(f"Error processing frame {frame} for {node.name()}: {e}")
                continue
        
        # Restore the original frame
        nuke.frame(original_frame)
                
        # 4. Save JSON file to disk
        try:
            with open(json_path, 'w') as f:
                json.dump(all_dominant_colors, f, indent=4)
            nuke.message(f"Successfully exported {k_fixed} linear colors per frame to:\n{json_path}")
        except Exception as e:
            nuke.critical(f"Failed to save JSON file at {json_path}: {e}")


# -------------------------------------------------------------
# FUNCTION 2: APPLY COLORS TO SELECTED READ NODE (Unchanged)
# -------------------------------------------------------------

def convert_linear_to_display_color(linear_rgb_list):
    """Converts a linear RGB color to a display-ready color (for node 'tile_color')."""
    gamma_correction = 1.0 / 2.2
    display_rgb = []
    
    for val in linear_rgb_list:
        clamped_linear_val = max(0.0, val)
        display_val = math.pow(clamped_linear_val, gamma_correction)
        clipped_display_val = min(1.0, display_val)
        display_rgb.append(clipped_display_val)
        
    return display_rgb

def float_to_packed_int(r, g, b, a=1.0):
    """Packs float RGB values (0.0-1.0) into Nuke's single tile_color integer."""
    r_int = int(r * 255)
    g_int = int(g * 255)
    b_int = int(b * 255)
    a_int = int(a * 255)
    return (a_int &lt;&lt; 24) + (b_int &lt;&lt; 16) + (g_int &lt;&lt; 8) + r_int

def set_node_color_from_linear_rgb(node, linear_rgb_color):
    """Sets a Nuke node's tile_color based on a linear RGB list, applying conversion."""
    display_rgb = convert_linear_to_display_color(linear_rgb_color)
    r, g, b = display_rgb[0], display_rgb[1], display_rgb[2]
    packed_color = float_to_packed_int(r, g, b)
    node['tile_color'].setValue(packed_color)

def apply_colors_to_selected_read_node():
    """
    Reads the associated JSON color data and sets the tile_color of the 
    selected Read node based on the most dominant color of the current frame.
    """
    selected_nodes = nuke.selectedNodes('Read')
    if not selected_nodes:
        nuke.message("Please select one or more Read nodes.")
        return

    current_frame = nuke.frame()
    
    for node in selected_nodes:
        frame_list, dir_path, json_path = get_sequence_info(node)
        
        if not os.path.exists(json_path):
            nuke.message(f"Color JSON file not found for {node.name()}:\n{json_path}\n\nRun 'Export Colors' first.")
            continue
            
        try:
            with open(json_path, 'r') as f:
                color_data = json.load(f)
            
            frame_key = str(current_frame) 
            
            # Find the key, handling padded vs unpadded frame numbers
            matching_key = None
            if frame_key in color_data:
                matching_key = frame_key
            else:
                for key in color_data:
                    if key.lstrip('0') == frame_key.lstrip('0'):
                        matching_key = key
                        break
            
            if not matching_key:
                nuke.warning(f"No color data found for frame {current_frame} in {json_path}")
                continue

            # The most dominant color is the first one in the list [0] from K-Means
            linear_rgb_color = color_data[matching_key][0]
            
            # Set the node's tile_color with colorspace conversion
            set_node_color_from_linear_rgb(node, linear_rgb_color)
            
        except Exception as e:
            nuke.critical(f"Failed to apply color to {node.name()}: {e}")
</pre>

<div id="Reading Colors to JSON-QUESTION"><h2 id="QUESTION" class="header"><a href="#Reading Colors to JSON-QUESTION">QUESTION</a></h2></div>

<table>
<tr>
<td>
How much faster than the regular looped 'sample' method would the K-Means method be (using the standard Nuke Python library modules)?
</td>
</tr>
</table>

<p>
That's a great question about performance! However, there's a misunderstanding of how the two methods relate in the context of the script provided.
</p>

<p>
The K-Means method is not an alternative to the pixel sampling method; it's a process that requires the pixel sampling method.
</p>

<p>
Therefore, the K-Means process will be significantly slower than simply sampling pixels, as it involves a complex mathematical clustering calculation on top of the pixel extraction time.
</p>

<div id="Reading Colors to JSON-QUESTION-Comparison of Processing Time"><h3 id="Comparison of Processing Time" class="header"><a href="#Reading Colors to JSON-QUESTION-Comparison of Processing Time">Comparison of Processing Time</a></h3></div>

<p>
Here's a breakdown of the time complexity for the two processes:
</p>

<table>
<tr>
<td>
Process
</td>
<td>
Core Operation
</td>
<td>
Purpose
</td>
<td>
Time Complexity (Relative)
</td>
</tr>
<tr>
<td>
1. Sampling Pixels
</td>
<td>
crop.sample() / Looping
</td>
<td>
Reads all N pixels from a frame into memory.
</td>
<td>
Baseline (Requires O(N) time)
</td>
</tr>
<tr>
<td>
2. K-Means Clustering
</td>
<td>
Iterative centroid calculation
</td>
<td>
Compresses the N pixels into K dominant colors.
</td>
<td>
Much Slower (Requires ‚âàO(T‚ãÖN‚ãÖK) time)
</td>
</tr>
</table>

<p>
Where:
</p>
<ul>
<li>
N is the total number of pixels (Width √ó Height).

<li>
K is the number of clusters (e.g., 30).

<li>
T is the number of iterations (e.g., 50).

</ul>
<div id="Reading Colors to JSON-QUESTION-The Bottleneck: Pixel Extraction"><h3 id="The Bottleneck: Pixel Extraction" class="header"><a href="#Reading Colors to JSON-QUESTION-The Bottleneck: Pixel Extraction">The Bottleneck: Pixel Extraction</a></h3></div>

<p>
The operation crop.sample() is the fundamental I/O and processing step for getting pixel data from the Nuke graph into Python memory. Both the regular looped sample (if you were just logging every pixel) and the K-Means method must perform this step.
</p>

<p>
In the provided script:
</p>

<ul>
<li>
get_linear_pixels_from_node() calls crop.sample() and converts the data structure. This is O(N).

<li>
pure_python_kmeans() then processes this list of N pixels. This is the slowest part of the entire analysis, at about O(T‚ãÖN‚ãÖK).

</ul>
<div id="Reading Colors to JSON-QUESTION-The Bottleneck: Pixel Extraction-Conclusion on Speed"><h4 id="Conclusion on Speed" class="header"><a href="#Reading Colors to JSON-QUESTION-The Bottleneck: Pixel Extraction-Conclusion on Speed">Conclusion on Speed</a></h4></div>

<p>
If you are comparing:
</p>
<ul>
<li>
A basic loop to read and discard all pixels (e.g., just to prove you can access them)

<li>
The full K-Means extraction process (reading pixels + clustering)

</ul>
<p>
The K-Means process will be orders of magnitude slower because it performs complex iterative math (T iterations, checking N points against K centers) on the very large list of N pixels extracted from the node.
</p>

<p>
Using pure Python (without NumPy/SciPy/Scikit-learn) makes the K-Means calculation even slower than it would be in a fully optimized environment, as the math must be done iteratively on lists instead of vectorized operations on arrays.
</p>

<p>
The purpose of K-Means is data reduction (finding the average color), not speed. The speed gain comes later, where you store a small JSON file per frame, saving you from having to run this slow analysis ever again.
</p>

<div id="Reading Colors to JSON-QUESTION"><h2 id="QUESTION" class="header"><a href="#Reading Colors to JSON-QUESTION">QUESTION</a></h2></div>

<table>
<tr>
<td>
Then can you suggest an alternative script (but much the same functionality) where we look through image pixels sequentially, and store the found colors? I would think that skipping pixels (every other pixel or every fourth pixel) would suffice to find all colors in an image sequence (as found in a selected Read Node).
</td>
</tr>
</table>

<p>
That's an excellent approach to drastically improve performance while still gathering color information! You are moving from a complex, iterative clustering method (K-Means) to a fast sampling/histogramming method.
</p>

<p>
The core change will be replacing the slow, iterative pure_python_kmeans function with a much faster Sequential Sampling routine that uses a simple data structure to aggregate unique colors (or near-unique colors).
</p>

<p>
Here is the alternative script incorporating:
</p>
<ul>
<li>
<span id="Reading Colors to JSON-QUESTION-Sequential Pixel Sampling"></span><strong id="Sequential Pixel Sampling">Sequential Pixel Sampling</strong>: Looping through pixels with a stride (e.g., skipping every 4 

</ul>
<p>
th pixel) to reduce N.
</p>
<ul>
<li>
<span id="Reading Colors to JSON-QUESTION-Color Aggregation (Quantization)"></span><strong id="Color Aggregation (Quantization)">Color Aggregation (Quantization)</strong>: To prevent the script from storing millions of slightly different float values, we'll implement a simple form of color quantization (rounding) to group near-identical colors.

<li>
<span id="Reading Colors to JSON-QUESTION-Frequency Analysis"></span><strong id="Frequency Analysis">Frequency Analysis</strong>: The final colors will be sorted by frequency (how many times they were sampled) to identify the "dominant" color, similar to what K-Means provides.

</ul>
<p>
This completely eliminates the O(T‚ãÖN‚ãÖK) complexity of K-Means.
</p>

<div id="Reading Colors to JSON-QUESTION-Alternative Pure Python Script: Sequential Sampling &amp; Quantization"><h3 id="Alternative Pure Python Script: Sequential Sampling &amp; Quantization" class="header"><a href="#Reading Colors to JSON-QUESTION-Alternative Pure Python Script: Sequential Sampling &amp; Quantization">Alternative Pure Python Script: Sequential Sampling &amp; Quantization</a></h3></div>

<p>
This script uses the same Nuke API functions for pixel extraction and colorspace conversion, but replaces the K-Means logic with a much faster sampling and counting method.
</p>

<pre python>
import nuke
import math
import random
import json
import os
import glob
from collections import defaultdict, Counter # Counter is great for frequency analysis

# --- CORE 1: COLOR SAMPLING &amp; QUANTIZATION ---

# Define the quantization level: 
# This determines how close two float colors must be to be counted as the same.
# Higher value = fewer unique colors found (faster, but less accurate for subtle colors).
# A power of 10 is usually a good choice, e.g., 100 rounds to 2 decimal places.
QUANTIZE_FACTOR = 1000

def sample_and_aggregate_colors(linear_pixels, pixel_stride=4, color_quantization_factor=QUANTIZE_FACTOR):
    """
    Sequentially samples pixels, quantizes the linear RGB values, and counts their frequency.
    
    Args:
        linear_pixels (list of lists): The input linear pixel data [[R, G, B], ...].
        pixel_stride (int): Samples every Nth pixel (e.g., 4 = samples 1/4 of pixels).
        color_quantization_factor (int): Multiplier used for rounding float colors to integers.
        
    Returns:
        list of lists: Dominant linear colors, sorted by frequency (most frequent first).
    """
    pixel_count = len(linear_pixels)
    if pixel_count == 0:
        return [[0.0, 0.0, 0.0]]

    quantized_colors = []
    
    # 1. Sample Pixels and Quantize
    for i in range(0, pixel_count, pixel_stride):
        r, g, b = linear_pixels[i]
        
        # Quantization: Multiply by a factor, round to integer, convert to a hashable tuple
        q_r = int(round(r * color_quantization_factor))
        q_g = int(round(g * color_quantization_factor))
        q_b = int(round(b * color_quantization_factor))
        
        quantized_colors.append((q_r, q_g, q_b))
        
    # 2. Count Frequencies and Sort
    # Counter is highly optimized for this.
    color_frequencies = Counter(quantized_colors)
    
    # Sort by frequency (the count, which is the second item in the tuple) in descending order
    sorted_frequencies = color_frequencies.most_common() 
    
    # 3. De-quantize back to linear float RGB and return
    dominant_colors_linear = []
    for (q_r, q_g, q_b), count in sorted_frequencies:
        # Divide by the factor to get the float color back
        linear_r = q_r / color_quantization_factor
        linear_g = q_g / color_quantization_factor
        linear_b = q_b / color_quantization_factor
        
        dominant_colors_linear.append([linear_r, linear_g, linear_b])

    # Return only the top 'k' colors (e.g., first 30) for a consistent JSON output
    # We use a fixed limit (e.g., 30) for consistency with the K-Means script's JSON format.
    # The first element [0] is always the single most dominant color.
    return dominant_colors_linear[:30]


# --- CORE 2 &amp; 3: NUKE API / I/O FUNCTIONS (Mostly Unchanged) ---

def get_linear_pixels_from_node(read_node, frame):
    """Reads ALL linear pixel data directly from a Nuke node (OIIO replacement)."""
    # This function is unchanged from the OIIO-free script
    crop = nuke.nodes.Crop(inputs=[read_node], postage_stamp=False)
    crop['box'].setValue([0, 0, read_node.width(), read_node.height()])
    
    pixels = []
    width = read_node.width()
    height = read_node.height()
    
    try:
        data = crop.sample('red', 0, 0, width, height, frame=frame)
        data += crop.sample('green', 0, 0, width, height, frame=frame)
        data += crop.sample('blue', 0, 0, width, height, frame=frame)
        
        num_pixels = width * height
        
        for i in range(num_pixels):
            r = data[i]
            g = data[i + num_pixels]
            b = data[i + 2 * num_pixels]
            pixels.append([r, g, b])
            
    finally:
        nuke.delete(crop)
        
    return pixels

def get_sequence_info(read_node):
    # This function is unchanged
    file_path_full = read_node['file'].value()
    # ... [rest of get_sequence_info implementation] ...
    # (Leaving the rest of the implementation short for brevity)
    
    if '%' in file_path_full:
        dir_path = os.path.dirname(file_path_full)
        base_name_pattern = os.path.basename(file_path_full)
        json_file_name = base_name_pattern.split('.')[0] + ".json"
        start = int(read_node['first'].value())
        end = int(read_node['last'].value())
        frame_list = range(start, end + 1)
    else:
        dir_path = os.path.dirname(file_path_full)
        base_name = os.path.basename(file_path_full)
        json_file_name = os.path.splitext(base_name)[0] + ".json"
        frame_list = [int(nuke.root()['first_frame'].value())]

    json_path = os.path.join(dir_path, json_file_name)
    return frame_list, dir_path, json_path


# -------------------------------------------------------------
# FUNCTION 1: EXPORT COLORS FROM SELECTED READ NODES
# -------------------------------------------------------------

def export_colors_from_selected_read_nodes(pixel_stride=16):
    """
    Reads selected Read nodes, extracts dominant linear colors using sampling,
    and exports a JSON file.
    """
    selected_nodes = nuke.selectedNodes('Read')
    if not selected_nodes:
        nuke.message("Please select one or more Read nodes.")
        return

    for node in selected_nodes:
        frame_list, dir_path, json_path = get_sequence_info(node)
        all_dominant_colors = {}
        
        if not frame_list:
            nuke.warning(f"Skipping {node.name()}: Invalid frame range.")
            continue

        nuke.message(f"Processing {len(frame_list)} frames for {node.name()} with stride {pixel_stride}...")

        original_frame = nuke.frame() 
        
        for frame in frame_list:
            nuke.frame(frame)
            try:
                # 1. Get ALL Linear Pixel Data from Nuke API
                linear_pixels = get_linear_pixels_from_node(node, frame)
                
                # 2. Sample, Quantize, and Aggregate Dominant Colors
                # Note: We now use the pixel_stride here. For a 4K image (8 million pixels), 
                # a stride of 16 reduces the aggregated list size to 500,000 pixels.
                dominant_colors_linear = sample_and_aggregate_colors(linear_pixels, pixel_stride=pixel_stride)
                
                # 3. Store Results (as linear floats)
                all_dominant_colors[str(frame)] = dominant_colors_linear

            except Exception as e:
                nuke.warning(f"Error processing frame {frame} for {node.name()}: {e}")
                continue
        
        nuke.frame(original_frame)
                
        # 4. Save JSON file to disk
        try:
            with open(json_path, 'w') as f:
                json.dump(all_dominant_colors, f, indent=4)
            nuke.message(f"Successfully exported {len(dominant_colors_linear)} dominant linear colors per frame to:\n{json_path}")
        except Exception as e:
            nuke.critical(f"Failed to save JSON file at {json_path}: {e}")
</pre>

<div id="Reading Colors to JSON-QUESTION-Performance Analysis"><h3 id="Performance Analysis" class="header"><a href="#Reading Colors to JSON-QUESTION-Performance Analysis">Performance Analysis</a></h3></div>

<p>
This new method is dramatically faster than the K-Means approach because:
</p>

<ul>
<li>
Elimination of Iteration: We're no longer looping 50 times to refine centroids. We just do one pass.

<li>
Reduction of Data N: The pixel_stride parameter (e.g., set to 16 in the example) directly cuts the number of items being processed and counted by a factor of 16.

<li>
Optimization with Counter: The collections.Counter module is a highly optimized C implementation in Python, making the counting step extremely fast compared to manual list manipulation.

</ul>
<p>
The main time spent is now in get_linear_pixels_from_node() (Nuke's I/O) and the single loop inside sample_and_aggregate_colors(). This is the best performance you can achieve without stepping outside the standard Nuke Python environment.
</p>

<div id="Reading Colors to JSON-QUESTION"><h2 id="QUESTION" class="header"><a href="#Reading Colors to JSON-QUESTION">QUESTION</a></h2></div>

<table>
<tr>
<td>
I don't really understand where the K-Means method would be of advantage. Could you explain why that might be used over a brute force looped 'sample'? Would I be right in thinking it's a way of getting dominant color values until extra colors with minimal representation would be omitted in the cause of minimal data collection?
</td>
</tr>
</table>

<p>
That's an insightful way to put it! You're absolutely right: the primary advantage of K-Means over a brute-force sampled approach is quality-controlled data reduction, which ensures minimal representation colors are omitted in favor of the truly dominant ones.
</p>

<p>
Here is a breakdown of why K-Means is often preferred, despite being slower.
</p>

<div id="Reading Colors to JSON-K-Means vs. Sampled Quantization"><h2 id="K-Means vs. Sampled Quantization" class="header"><a href="#Reading Colors to JSON-K-Means vs. Sampled Quantization">K-Means vs. Sampled Quantization</a></h2></div>
 
<div id="Reading Colors to JSON-K-Means vs. Sampled Quantization-The Goal of K-Means: Finding True Averages"><h3 id="The Goal of K-Means: Finding True Averages" class="header"><a href="#Reading Colors to JSON-K-Means vs. Sampled Quantization-The Goal of K-Means: Finding True Averages">The Goal of K-Means: Finding True Averages</a></h3></div>

<p>
Your brute-force sampled method (with quantization) is essentially creating a histogram of the pixel values. It tells you that "Red 0.8" appeared 10,000 times, and "Red 0.801" appeared 9,999 times.
</p>

<div id="Reading Colors to JSON-K-Means vs. Sampled Quantization-The Goal of K-Means: Finding True Averages-K-Means (Clustering) goes further:"><h4 id="K-Means (Clustering) goes further:" class="header"><a href="#Reading Colors to JSON-K-Means vs. Sampled Quantization-The Goal of K-Means: Finding True Averages-K-Means (Clustering) goes further:">K-Means (Clustering) goes further:</a></h4></div>

<ul>
<li>
<span id="Reading Colors to JSON-K-Means vs. Sampled Quantization-The Goal of K-Means: Finding True Averages-K-Means (Clustering) goes further:-It calculates the mathematical average"></span><strong id="It calculates the mathematical average">It calculates the mathematical average</strong>: K-Means will recognize that "Red 0.8" and "Red 0.801" are visually the same, and it will combine them into a single, representative color (a centroid). If your image has a large, smooth color gradient (e.g., a blue sky fading from light blue to dark blue), the K-Means algorithm will find 30 distinct, representative shades of blue that are the average of thousands of similar colors along that gradient.

<li>
<span id="Reading Colors to JSON-K-Means vs. Sampled Quantization-The Goal of K-Means: Finding True Averages-K-Means (Clustering) goes further:-It adapts to the data"></span><strong id="It adapts to the data">It adapts to the data</strong>: It doesn't rely on a fixed, arbitrary quantization factor (QUANTIZE_FACTOR). Instead, it dynamically adjusts the centroid positions to minimize the total error (distortion) across the entire image.

</ul>
  
<table>
<tr>
<td>
Scenario
</td>
<td>
Sampled Quantization (Your Method)
</td>
<td>
K-Means Clustering
</td>
</tr>
<tr>
<td>
Smooth Gradient (e.g., sky)
</td>
<td>
You might get 50 different "unique" blues, all very close, because the quantization factor isn't fine enough to group them, or too fine, grouping too few.
</td>
<td>
You explicitly ask for 30 colors (K=30). K-Means guarantees those 30 colors are the most mathematically representative averages of the entire gradient.
</td>
</tr>
<tr>
<td>
Noise/Grain
</td>
<td>
Random noise added to a uniform wall can create thousands of unique, low-frequency colors that clutter your list.
</td>
<td>
The algorithm treats the noisy pixel as a slight deviation from the true average color of the wall, pulling the centroid toward the pure color and effectively filtering the noise.
</td>
</tr>
<tr>
<td>
Dominance
</td>
<td>
Dominance is based purely on the number of times a rounded color value appeared.
</td>
<td>
Dominance is based on the size of the cluster that the centroid represents‚Äîit's a measure of volume and influence, not just raw count.
</td>
</tr>
</table>
 
<div id="Reading Colors to JSON-K-Means vs. Sampled Quantization-When to Use Which Method"><h3 id="When to Use Which Method" class="header"><a href="#Reading Colors to JSON-K-Means vs. Sampled Quantization-When to Use Which Method">When to Use Which Method</a></h3></div>

<table>
<tr>
<td>
Method
</td>
<td>
Performance (Time)
</td>
<td>
Accuracy/Quality of Dominance
</td>
<td>
Best Use Case
</td>
</tr>
<tr>
<td>
Sampled Quantization (Your Method)
</td>
<td>
Very Fast (Best performance)
</td>
<td>
Good enough, but sensitive to noise and quantization factor.
</td>
<td>
When you must have speed, have a low-noise image, or just need the general "ballpark" colors.
</td>
</tr>
<tr>
<td>
K-Means Clustering
</td>
<td>
Slow (High computational cost)
</td>
<td>
High Quality (Mathematically optimized representation).
</td>
<td>
When you need the most accurate, mathematically proven set of dominant colors for critical analysis or art direction.
</td>
</tr>
</table>

<p>
You are absolutely correct that both methods aim to find dominant color values while omitting colors with minimal representation. The difference lies in how they achieve this:
</p>

<ul>
<li>
Sampled Quantization uses a brute-force count and filter based on fixed buckets (quantization).

<li>
K-Means uses an iterative optimization process to mathematically define the best possible buckets (centroids) for a desired K number of colors.

<li>
In a professional VFX pipeline, K-Means (or similar clustering) is often preferred for analysis tasks because the mathematical guarantee of finding the most representative average is worth the upfront computational cost.

</ul>
<div id="Reading Colors to JSON-QUESTION"><h2 id="QUESTION" class="header"><a href="#Reading Colors to JSON-QUESTION">QUESTION</a></h2></div>

<table>
<tr>
<td>
My images will have very strict 'flat' and limited color palette (maybe 50 colors max). I think a brute force approach would be preferred. Do you agree?
</td>
</tr>
</table>

<p>
Yes, I absolutely agree. Given your constraint of having images with a very strict, flat, and limited color palette (50 colors max), the brute-force sampling and quantization approach is highly preferred over K-Means clustering.
</p>

<p>
Here's why, and the one crucial step to ensure its success:
</p>

<div id="Reading Colors to JSON-QUESTION-Why Brute-Force is Better for a Limited Palette"><h3 id="Why Brute-Force is Better for a Limited Palette" class="header"><a href="#Reading Colors to JSON-QUESTION-Why Brute-Force is Better for a Limited Palette">Why Brute-Force is Better for a Limited Palette</a></h3></div>

<p>
The advantage of K-Means‚Äîits ability to find the average representative color in a gradient or noisy area‚Äîbecomes a disadvantage when your palette is already flat and strictly limited.
</p>

<ul>
<li>
<span id="Reading Colors to JSON-QUESTION-Why Brute-Force is Better for a Limited Palette-Speed Dominates"></span><strong id="Speed Dominates">Speed Dominates</strong>: Since your goal is minimal data collection and your images are simple, the performance gain from avoiding the iterative K-Means algorithm far outweighs any marginal quality difference. The brute-force method is significantly faster.

<li>
<span id="Reading Colors to JSON-QUESTION-Why Brute-Force is Better for a Limited Palette-No Averaging Needed"></span><strong id="No Averaging Needed">No Averaging Needed</strong>: Because your colors are "flat" (meaning there are large areas of uniform color rather than smooth gradients), there is little need for K-Means to calculate an average. If a wall is pure Red 0.8, K-Means will find Red 0.8. Your sampled approach will also find Red 0.8. K-Means offers no benefit here, only delay.

<li>
<span id="Reading Colors to JSON-QUESTION-Why Brute-Force is Better for a Limited Palette-Strictly Limited Data"></span><strong id="Strictly Limited Data">Strictly Limited Data</strong>: With a known limit of 50 colors, your brute-force method functions as a perfect histogram generator. It will accurately count the frequency of each of those 50 colors, giving you the true dominance order without the computational overhead of clustering.

</ul>
<div id="Reading Colors to JSON-QUESTION-The Crucial Step: Adjusting Quantization"><h3 id="The Crucial Step: Adjusting Quantization" class="header"><a href="#Reading Colors to JSON-QUESTION-The Crucial Step: Adjusting Quantization">The Crucial Step: Adjusting Quantization</a></h3></div>

<p>
The only potential pitfall with the brute-force approach is setting the quantization factor correctly. Since you have a strict palette, you want the quantization to be fine enough to distinguish between your 50 colors, but coarse enough to group any minor floating-point variations (which happen with compression/decompression).
</p>

<p>
In the previous script:
</p>

<pre>
QUANTIZE_FACTOR=1000
</pre>

<p>
This factor rounds float values (like 0.12345) to an integer based on three decimal places (123).
</p>
<ul>
<li>
If your "flat" colors are exactly uniform (e.g., exactly [0.8, 0.1, 0.1]), a high factor like 10,000 is safe.

<li>
If your "flat" colors have minor variations due to image processing (e.g., one pixel is 0.800001 and the next is 0.799998), a factor of 1000 is usually sufficient to group these into the same bucket.

</ul>
<p>
Recommendation: Stick with a high but reasonable factor like 1000 or 10000 to ensure you capture the discrete, intended colors. You can then also increase the pixel_stride (e.g., from 16 to 32 or 64) for even greater speed, as the colors are guaranteed to be consistent across large areas.
</p>

<div id="Reading Colors to JSON-QUESTION"><h2 id="QUESTION" class="header"><a href="#Reading Colors to JSON-QUESTION">QUESTION</a></h2></div>

<table>
<tr>
<td>
Would adding an artificially hight count limit here help catch a user error where a regular image is accidentally analyzed? Maybe 200 to allow for exceptional cases?
</td>
</tr>
</table>

<p>
That's an excellent idea for robustness and error detection. Since you know your images should contain at most ‚âà50 unique colors, setting a hard upper limit, such as 200, provides a critical safety check without unduly restricting your clean, flat-colored footage.
</p>

<p>
I absolutely recommend adding this limit.
</p>

<div id="Reading Colors to JSON-QUESTION-Recommended Script Adjustment"><h3 id="Recommended Script Adjustment" class="header"><a href="#Reading Colors to JSON-QUESTION-Recommended Script Adjustment">Recommended Script Adjustment</a></h3></div>

<p>
You should modify the sample_and_aggregate_colors function to accept an optional max_colors argument and apply the slice at the end. A limit of 200 is generous enough to handle a highly noisy "flat" plate but restrictive enough to prevent analyzing a full 16.7 million-color image that was accidentally dragged into the graph.
</p>

<p>
Here is the modified function with the added safeguard:
</p>

<pre python>
# Define a robust limit for safety check
SAFEGUARD_COLOR_LIMIT = 200

def sample_and_aggregate_colors(linear_pixels, pixel_stride=16, color_quantization_factor=1000, max_colors=SAFEGUARD_COLOR_LIMIT):
    """
    Sequentially samples pixels, quantizes the linear RGB values, counts frequency,
    and includes a safeguard against analyzing standard, highly-varied images.
    """
    pixel_count = len(linear_pixels)
    if pixel_count == 0:
        return [[0.0, 0.0, 0.0]]

    quantized_colors = []
    
    # 1. Sample Pixels and Quantize
    for i in range(0, pixel_count, pixel_stride):
        r, g, b = linear_pixels[i]
        
        # Quantization: Multiply by a factor, round to integer, convert to a hashable tuple
        q_r = int(round(r * color_quantization_factor))
        q_g = int(round(g * color_quantization_factor))
        q_b = int(round(b * color_quantization_factor))
        
        quantized_colors.append((q_r, q_g, q_b))
        
    # 2. Count Frequencies and Sort
    color_frequencies = Counter(quantized_colors)
    sorted_frequencies = color_frequencies.most_common() 
    
    # 3. De-quantize back to linear float RGB
    dominant_colors_linear = []
    for (q_r, q_g, q_b), count in sorted_frequencies:
        # Divide by the factor to get the float color back
        linear_r = q_r / color_quantization_factor
        linear_g = q_g / color_quantization_factor
        linear_b = q_b / color_quantization_factor
        
        dominant_colors_linear.append([linear_r, linear_g, linear_b])

    # 4. SAFETY CHECK AND LIMITATION:
    # This limits the output to prevent massive JSON files if a user runs the script
    # on a photographic image (which might yield thousands of unique, quantized colors).
    if len(dominant_colors_linear) &gt; max_colors:
        nuke.warning(f"Warning: Image contained {len(dominant_colors_linear)} unique colors. Limiting export to the top {max_colors} colors to prevent excessively large data files.")
        
    return dominant_colors_linear[:max_colors]
</pre>

<div id="Reading Colors to JSON-QUESTION-Benefits of the max_colors=200 Safeguard"><h3 id="Benefits of the max_colors=200 Safeguard" class="header"><a href="#Reading Colors to JSON-QUESTION-Benefits of the max_colors=200 Safeguard">Benefits of the max_colors=200 Safeguard</a></h3></div>

<table>
<tr>
<td>
Benefit
</td>
<td>
Explanation
</td>
</tr>
<tr>
<td>
Error Prevention
</td>
<td>
If a complex, photographic image is used, the brute-force method can still generate thousands of unique, quantized colors, leading to a huge JSON file and a very slow export. Limiting it to 200 prevents this file size bloat.
</td>
</tr>
<tr>
<td>
User Feedback
</td>
<td>
The added nuke.warning provides immediate feedback that the source footage does not conform to the expected "flat color" template.
</td>
</tr>
<tr>
<td>
Guaranteed Structure
</td>
<td>
The downstream function apply_colors_to_selected_read_node is guaranteed to be working with a predictable, finite data structure, enhancing stability.
</td>
</tr>
</table>

<p>
Setting the limit to 200 is a perfect compromise: it's well above your expected 50 colors, but far below the thousands you'd get from a regular production image.
</p>

<div id="Reading Colors to JSON-QUESTION"><h2 id="QUESTION" class="header"><a href="#Reading Colors to JSON-QUESTION">QUESTION</a></h2></div>

<table>
<tr>
<td>
Can you add that updated function into the latest script (so that I don't introduce errors)?
</td>
</tr>
</table>

<p>
Certainly. Here is the complete, updated script.
</p>

<p>
This version replaces the slow K-Means logic with the fast Sampled Quantization method, uses Nuke's Python API (no OpenImageIO dependency), and includes the max_colors=200 safeguard to prevent errors from accidentally analyzing high-detail images.
</p>

<p>
Complete Nuke Script (Sampled Quantization)
</p>

<pre python>
import nuke
import math
import random
import json
import os
import glob
from collections import defaultdict, Counter 

# --- CONFIGURATION CONSTANTS ---
PIXEL_STRIDE = 16          # Samples 1/16th of the pixels for speed
QUANTIZE_FACTOR = 1000     # Groups colors by rounding to 3 decimal places (0.001)
SAFEGUARD_COLOR_LIMIT = 200 # Max number of unique colors to save in the JSON

# --- CORE 1: COLOR SAMPLING &amp; QUANTIZATION (UPDATED) ---

def sample_and_aggregate_colors(linear_pixels, pixel_stride=PIXEL_STRIDE, color_quantization_factor=QUANTIZE_FACTOR, max_colors=SAFEGUARD_COLOR_LIMIT):
    """
    Sequentially samples pixels, quantizes the linear RGB values, counts frequency,
    and includes a safeguard against analyzing standard, highly-varied images.
    """
    pixel_count = len(linear_pixels)
    if pixel_count == 0:
        return [[0.0, 0.0, 0.0]]

    quantized_colors = []
    
    # 1. Sample Pixels and Quantize
    for i in range(0, pixel_count, pixel_stride):
        r, g, b = linear_pixels[i]
        
        # Quantization: Multiply by a factor, round to integer, convert to a hashable tuple
        q_r = int(round(r * color_quantization_factor))
        q_g = int(round(g * color_quantization_factor))
        q_b = int(round(b * color_quantization_factor))
        
        quantized_colors.append((q_r, q_g, q_b))
        
    # 2. Count Frequencies and Sort (Most dominant first)
    color_frequencies = Counter(quantized_colors)
    sorted_frequencies = color_frequencies.most_common() 
    
    # 3. De-quantize back to linear float RGB
    dominant_colors_linear = []
    for (q_r, q_g, q_b), count in sorted_frequencies:
        # Divide by the factor to get the float color back
        linear_r = q_r / color_quantization_factor
        linear_g = q_g / color_quantization_factor
        linear_b = q_b / color_quantization_factor
        
        dominant_colors_linear.append([linear_r, linear_g, linear_b])

    # 4. SAFETY CHECK AND LIMITATION:
    # If the number of unique colors exceeds the safeguard limit, issue a warning 
    # and only return the top 'max_colors'.
    if len(dominant_colors_linear) &gt; max_colors:
        nuke.warning(f"Warning: Image contained {len(dominant_colors_linear)} unique colors. Limiting export to the top {max_colors} colors to prevent excessively large data files.")
        
    # Returns all colors found (sorted by dominance), sliced to the safety limit.
    return dominant_colors_linear[:max_colors]


# --- CORE 2: NUKE API PIXEL EXTRACTION ---

def get_linear_pixels_from_node(read_node, frame):
    """
    Reads ALL linear pixel data directly from a Nuke node for a single frame.
    Uses nuke.sample() which is in the scene's linear working space.
    """
    # Create a temporary, invisible output pipe to sample the pixels
    crop = nuke.nodes.Crop(inputs=[read_node], postage_stamp=False)
    crop['box'].setValue([0, 0, read_node.width(), read_node.height()])
    
    pixels = []
    width = read_node.width()
    height = read_node.height()
    
    try:
        # Request the channel data (R, G, B) as a flat list of floats
        data = crop.sample('red', 0, 0, width, height, frame=frame)
        data += crop.sample('green', 0, 0, width, height, frame=frame)
        data += crop.sample('blue', 0, 0, width, height, frame=frame)
        
        num_pixels = width * height
        
        # Reshape the flat data (R...R, G...G, B...B) into [R, G, B] triplets
        for i in range(num_pixels):
            r = data[i]
            g = data[i + num_pixels]
            b = data[i + 2 * num_pixels]
            pixels.append([r, g, b])
            
    finally:
        # Clean up the temporary node immediately
        nuke.delete(crop)
        
    return pixels

def get_sequence_info(read_node):
    """Gets the file path, directory, and JSON path from a Read node."""
    file_path_full = read_node['file'].value()
    
    if '%' in file_path_full:
        dir_path = os.path.dirname(file_path_full)
        base_name_pattern = os.path.basename(file_path_full)
        json_file_name = base_name_pattern.split('.')[0] + ".json"
        start = int(read_node['first'].value())
        end = int(read_node['last'].value())
        frame_list = range(start, end + 1)
    else:
        dir_path = os.path.dirname(file_path_full)
        base_name = os.path.basename(file_path_full)
        json_file_name = os.path.splitext(base_name)[0] + ".json"
        frame_list = [int(nuke.root()['first_frame'].value())]

    json_path = os.path.join(dir_path, json_file_name)
    return frame_list, dir_path, json_path


# -------------------------------------------------------------
# FUNCTION 1: EXPORT COLORS FROM SELECTED READ NODES
# -------------------------------------------------------------

def export_colors_from_selected_read_nodes():
    """
    Reads selected Read nodes, extracts dominant linear colors using sampling,
    and exports a JSON file.
    """
    selected_nodes = nuke.selectedNodes('Read')
    if not selected_nodes:
        nuke.message("Please select one or more Read nodes.")
        return

    for node in selected_nodes:
        frame_list, dir_path, json_path = get_sequence_info(node)
        all_dominant_colors = {}
        
        if not frame_list:
            nuke.warning(f"Skipping {node.name()}: Invalid frame range.")
            continue

        nuke.message(f"Processing {len(frame_list)} frames for {node.name()} with stride {PIXEL_STRIDE}...")

        original_frame = nuke.frame() 
        
        for frame in frame_list:
            nuke.frame(frame)
            try:
                # 1. Get ALL Linear Pixel Data from Nuke API
                linear_pixels = get_linear_pixels_from_node(node, frame)
                
                # 2. Sample, Quantize, and Aggregate Dominant Colors
                dominant_colors_linear = sample_and_aggregate_colors(linear_pixels)
                
                # 3. Store Results (as linear floats)
                all_dominant_colors[str(frame)] = dominant_colors_linear

            except Exception as e:
                nuke.warning(f"Error processing frame {frame} for {node.name()}: {e}")
                continue
        
        nuke.frame(original_frame)
                
        # 4. Save JSON file to disk
        try:
            with open(json_path, 'w') as f:
                json.dump(all_dominant_colors, f, indent=4)
            nuke.message(f"Successfully exported {len(dominant_colors_linear)} dominant linear colors per frame to:\n{json_path}")
        except Exception as e:
            nuke.critical(f"Failed to save JSON file at {json_path}: {e}")


# -------------------------------------------------------------
# FUNCTION 2: APPLY COLORS TO SELECTED READ NODE
# -------------------------------------------------------------

def convert_linear_to_display_color(linear_rgb_list):
    """Converts a linear RGB color to a display-ready color (for node 'tile_color')."""
    gamma_correction = 1.0 / 2.2
    display_rgb = []
    
    for val in linear_rgb_list:
        clamped_linear_val = max(0.0, val)
        display_val = math.pow(clamped_linear_val, gamma_correction)
        clipped_display_val = min(1.0, display_val)
        display_rgb.append(clipped_display_val)
        
    return display_rgb

def float_to_packed_int(r, g, b, a=1.0):
    """Packs float RGB values (0.0-1.0) into Nuke's single tile_color integer."""
    r_int = int(r * 255)
    g_int = int(g * 255)
    b_int = int(b * 255)
    a_int = int(a * 255)
    return (a_int &lt;&lt; 24) + (b_int &lt;&lt; 16) + (g_int &lt;&lt; 8) + r_int

def set_node_color_from_linear_rgb(node, linear_rgb_color):
    """Sets a Nuke node's tile_color based on a linear RGB list, applying conversion."""
    display_rgb = convert_linear_to_display_color(linear_rgb_color)
    r, g, b = display_rgb[0], display_rgb[1], display_rgb[2]
    packed_color = float_to_packed_int(r, g, b)
    node['tile_color'].setValue(packed_color)

def apply_colors_to_selected_read_node():
    """
    Reads the associated JSON color data and sets the tile_color of the 
    selected Read node based on the most dominant color of the current frame.
    """
    selected_nodes = nuke.selectedNodes('Read')
    if not selected_nodes:
        nuke.message("Please select one or more Read nodes.")
        return

    current_frame = nuke.frame()
    
    for node in selected_nodes:
        frame_list, dir_path, json_path = get_sequence_info(node)
        
        if not os.path.exists(json_path):
            nuke.message(f"Color JSON file not found for {node.name()}:\n{json_path}\n\nRun 'Export Colors' first.")
            continue
            
        try:
            with open(json_path, 'r') as f:
                color_data = json.load(f)
            
            frame_key = str(current_frame) 
            
            # Find the key, handling padded vs unpadded frame numbers
            matching_key = None
            if frame_key in color_data:
                matching_key = frame_key
            else:
                for key in color_data:
                    if key.lstrip('0') == frame_key.lstrip('0'):
                        matching_key = key
                        break
            
            if not matching_key:
                nuke.warning(f"No color data found for frame {current_frame} in {json_path}")
                continue

            # The most dominant color is always the first one in the list [0]
            linear_rgb_color = color_data[matching_key][0]
            
            # Set the node's tile_color with colorspace conversion
            set_node_color_from_linear_rgb(node, linear_rgb_color)
            
        except Exception as e:
            nuke.critical(f"Failed to apply color to {node.name()}: {e}")
            
</pre>

<div id="Further Dev"><h1 id="Further Dev" class="header"><a href="#Further Dev">Further Dev</a></h1></div>

<table>
<tr>
<td>
The input images are probably 16bit, and while the colors are few (with zero noise), I do need the actual values as acurately as possible, so that I may use those values for further analysis (in a later project). There will never be more than 100 colors.
</td>
</tr>
</table>

<div id="Further Dev-Summary"><h3 id="Summary" class="header"><a href="#Further Dev-Summary">Summary</a></h3></div>

<p>
You want to preserve the true discrete colors from 16‚Äëbit images (no aggressive quantizing) while removing floating‚Äëpoint noise and producing a stable, exact representation you can use later. The safest approach is to bucket by 16‚Äëbit integer values (0‚Äì65535), keep counts, and also store a high‚Äëprecision float representative (average of the bucket) so you retain exact linear values for later analysis.
</p>

<p>
Below is a compact, drop‚Äëin consolidation function that:
</p>
<ul>
<li>
converts sampled linear floats to clamped 16‚Äëbit integers (rounding),

<li>
counts occurrences across the sequence using those exact integer keys,

<li>
computes the precise average float color for each 16‚Äëbit bucket (high precision),

<li>
returns a sorted list of color records (most frequent first) limited to a max number.

</ul>
<p>
Use this when you know your source images are effectively 16‚Äëbit (or you want stable integer bucketing). It avoids arbitrary tiny floating variations while preserving per‚Äëbucket float accuracy.
</p>
<pre python>
from collections import Counter, defaultdict
import math

def consolidate_precise_colors(all_dominant_colors,
                               max_colors=100,
                               clamp_min=0.0,
                               clamp_max=1.0):
    """
    Consolidate per-frame dominant colors into precise 16-bit buckets.

    Parameters
      all_dominant_colors : dict -&gt; { "frame": [[r,g,b], ...], ... } (linear floats)
      max_colors          : int  -&gt; maximum distinct colors to return (default 100)
      clamp_min, clamp_max: float -&gt; clamp range for converting to 16-bit (default 0..1)

    Returns (list of dicts, sorted by frequency desc):
      [
        {
          "rgb_16bit": [r16, g16, b16],      # integers 0..65535 (bucket key)
          "count": N,                        # occurrences across frames
          "rgb_linear_avg": [r_f, g_f, b_f]  # high-precision linear float average for that bucket
        },
        ...
      ]
    """
    if not all_dominant_colors:
        return []

    # Count occurrences by 16-bit bucket and accumulate floats for an accurate average
    counts = Counter()
    accum = defaultdict(lambda: [0.0, 0.0, 0.0])

    for frame, color_list in all_dominant_colors.items():
        for col in color_list:
            if not col or len(col) &lt; 3:
                continue
            r_f, g_f, b_f = float(col[0]), float(col[1]), float(col[2])

            # clamp to expected range; if your data can exceed 1.0 and you want to preserve it,
            # choose a different clamp_max or remove clamping
            r_c = max(clamp_min, min(clamp_max, r_f))
            g_c = max(clamp_min, min(clamp_max, g_f))
            b_c = max(clamp_min, min(clamp_max, b_f))

            # convert to exact 16-bit integers (rounding)
            r16 = int(round(r_c * 65535.0))
            g16 = int(round(g_c * 65535.0))
            b16 = int(round(b_c * 65535.0))

            key = (r16, g16, b16)
            counts[key] += 1
            acc = accum[key]
            acc[0] += r_f
            acc[1] += g_f
            acc[2] += b_f

    # Build sorted list by frequency
    items = counts.most_common(max_colors)

    consolidated = []
    for key, cnt in items:
        r16, g16, b16 = key
        summed = accum[key]
        # Compute the average float per bucket using the raw summed floats / count
        avg_r = summed[0] / cnt
        avg_g = summed[1] / cnt
        avg_b = summed[2] / cnt

        consolidated.append({
            "rgb_16bit": [r16, g16, b16],
            "count": int(cnt),
            "rgb_linear_avg": [avg_r, avg_g, avg_b]
        })

    return consolidated
</pre>

<div id="Further Dev-Summary-Usage notes and integration"><h4 id="Usage notes and integration" class="header"><a href="#Further Dev-Summary-Usage notes and integration">Usage notes and integration</a></h4></div>

<ul>
<li>
Call this after you produce all_dominant_colors (the per-frame dict). Example:

<ul>
<li>
consolidated = consolidate_precise_colors(all_dominant_colors, max_colors=100)

<li>
Save consolidated to JSON.

</ul>
<li>
The 16‚Äëbit bucket key is exact and stable (integers 0..65535), so you avoid FP drift when comparing colors between runs or systems.

<li>
<code>rgb_linear_avg</code> stores the true average of the sampled floats that fell into that exact 16‚Äëbit bucket, giving you precise linear values for later analysis.

<li>
If your images contain values outside 0..1 (HDR / out-of-gamut) and you want to preserve them, either:

<ul>
<li>
increase clamp_max (e.g., 4.0) or

<li>
remove clamping and instead map floats to a signed or higher‚Äëbit integer representation. Ask if you need that behavior.

</ul>
<li>
If you also want the raw integer counts per frame or a mapping of which frames contain which color buckets, I can extend the function to return a per-bucket frame list.

<li>
Would you like the function to also:

</ul>
<p>
write both a human‚Äëreadable consolidated JSON and a machine JSON with counts and frame indices, or
produce a small swatch image showing the consolidated colors?
</p>

<table>
<tr>
<td>
Before I ask anything else to improve or extend functionality, can you create the full version of the script (with this lastes function incorporated)? I'm worried I will miss a detail - like imports and function calls.
</td>
</tr>
</table>

<p>
Full script ‚Äî export per-frame dominant colors and a consolidated 16‚Äëbit color summary
</p>

<p>
Save this as e.g. export_cellcolors.py in your ~/.nuke/ and call export_colors_from_selected_read_nodes() from your menu or Script Editor.
</p>

<pre python>
<table>
<tr>
<td>
TESTED - WORKS
</td>
</tr>
</table>
import nuke
import os
import json
import math
from collections import Counter, defaultdict

# --- CONFIGURATION ---
PIXEL_STRIDE = 16          # sample every Nth pixel in X and Y for speed
SAFEGUARD_COLOR_LIMIT = 200
MAX_CONSOLIDATED_COLORS = 100
CLAMP_MIN = 0.0
CLAMP_MAX = 1.0

# --- PIXEL SAMPLING (robust per-pixel sampling compatible with typical Nuke builds) ---

def get_linear_pixels_from_node(read_node, frame, pixel_stride=PIXEL_STRIDE):
    """
    Sample pixels from a Read node at `frame`. Returns list of [r,g,b] floats.
    Uses read_node.sample(channel, x, y, frame=frame) per pixel.
    """
    width = int(read_node.width())
    height = int(read_node.height())

    if width &lt;= 0 or height &lt;= 0:
        raise ValueError(f"Invalid node dimensions: {read_node.name()} {width}x{height}")

    pixels = []
    # iterate with stride (y then x)
    for y in range(0, height, pixel_stride):
        for x in range(0, width, pixel_stride):
            try:
                r = read_node.sample('red', x, y, frame=frame)
                g = read_node.sample('green', x, y, frame=frame)
                b = read_node.sample('blue', x, y, frame=frame)
            except Exception:
                # fallback to short channel names if needed
                try:
                    r = read_node.sample('r', x, y, frame=frame)
                    g = read_node.sample('g', x, y, frame=frame)
                    b = read_node.sample('b', x, y, frame=frame)
                except Exception:
                    # give up for this pixel
                    continue
            pixels.append([float(r), float(g), float(b)])
    return pixels

# --- COLOR SAMPLING &amp; AGGREGATION (per-frame) ---

def sample_and_aggregate_colors(linear_pixels, pixel_stride=1, color_quantization_factor=1000, max_colors=SAFEGUARD_COLOR_LIMIT):
    """
    Sampled pixels (list of [r,g,b]) -&gt; quantize -&gt; count -&gt; return sorted dominant colors (linear floats).
    This function is conservative and deals with possible empty input.
    """
    if not linear_pixels:
        return []

    quantized = []
    for i in range(0, len(linear_pixels), pixel_stride):
        r, g, b = linear_pixels[i]
        q_r = int(round(r * color_quantization_factor))
        q_g = int(round(g * color_quantization_factor))
        q_b = int(round(b * color_quantization_factor))
        quantized.append((q_r, q_g, q_b))

    freqs = Counter(quantized).most_common()
    dominant_linear = []
    for (q_r, q_g, q_b), _count in freqs:
        dominant_linear.append([q_r / float(color_quantization_factor),
                                q_g / float(color_quantization_factor),
                                q_b / float(color_quantization_factor)])
    # safety limit
    if len(dominant_linear) &gt; max_colors:
        nuke.warning(f"Image contained {len(dominant_linear)} unique colors. Limiting to top {max_colors}.")
    return dominant_linear[:max_colors]

# --- CONSOLIDATION USING 16-BIT BUCKETS (precise, stable) ---

def consolidate_precise_colors(all_dominant_colors, max_colors=MAX_CONSOLIDATED_COLORS,
                               clamp_min=CLAMP_MIN, clamp_max=CLAMP_MAX):
    """
    Consolidate per-frame dominant colors into precise 16-bit buckets.
    Returns a list of dicts sorted by frequency (desc):
      {
        "rgb_16bit": [r16,g16,b16],
        "count": N,
        "rgb_linear_avg": [r_f, g_f, b_f]
      }
    """
    if not all_dominant_colors:
        return []

    counts = Counter()
    accum = defaultdict(lambda: [0.0, 0.0, 0.0])

    for frame_key, color_list in all_dominant_colors.items():
        for col in color_list:
            if not col or len(col) &lt; 3:
                continue
            r_f, g_f, b_f = float(col[0]), float(col[1]), float(col[2])
            # clamp to expected range
            r_c = max(clamp_min, min(clamp_max, r_f))
            g_c = max(clamp_min, min(clamp_max, g_f))
            b_c = max(clamp_min, min(clamp_max, b_f))

            r16 = int(round(r_c * 65535.0))
            g16 = int(round(g_c * 65535.0))
            b16 = int(round(b_c * 65535.0))

            key = (r16, g16, b16)
            counts[key] += 1
            acc = accum[key]
            acc[0] += r_f
            acc[1] += g_f
            acc[2] += b_f

    items = counts.most_common(max_colors)
    consolidated = []
    for key, cnt in items:
        summed = accum[key]
        avg_r = summed[0] / cnt
        avg_g = summed[1] / cnt
        avg_b = summed[2] / cnt
        consolidated.append({
            "rgb_16bit": [int(key[0]), int(key[1]), int(key[2])],
            "count": int(cnt),
            "rgb_linear_avg": [avg_r, avg_g, avg_b]
        })
    return consolidated

# --- HELPERS FOR READ NODE PATHS / FRAMES ---

def get_sequence_info(read_node):
    """
    Returns (frame_list, dir_path, json_path).
    If Read has a %04d style filename, returns range from first..last on the Read node.
    Otherwise returns a single-frame list (current script first frame).
    """
    file_path_full = read_node['file'].value()
    dir_path = os.path.dirname(file_path_full)

    if '%' in file_path_full:
        base_name_pattern = os.path.basename(file_path_full)
        json_file_name = os.path.splitext(base_name_pattern.split('%')[0])[0] + ".json"
        start = int(read_node['first'].value())
        end = int(read_node['last'].value())
        frame_list = list(range(start, end + 1))
    else:
        base_name = os.path.basename(file_path_full)
        json_file_name = os.path.splitext(base_name)[0] + ".json"
        frame_list = [int(nuke.root()['first_frame'].value())]

    json_path = os.path.join(dir_path, json_file_name)
    return frame_list, dir_path, json_path

# --- MAIN EXPORT FUNCTION ---

def export_colors_from_selected_read_nodes(pixel_stride=PIXEL_STRIDE, quantize_factor=1000):
    selected_nodes = nuke.selectedNodes('Read')
    if not selected_nodes:
        nuke.message("Please select one or more Read nodes.")
        return

    for node in selected_nodes:
        try:
            frame_list, dir_path, json_path = get_sequence_info(node)
        except Exception as e:
            nuke.critical(f"Failed to get sequence info for {node.name()}: {e}")
            continue

        all_dominant_colors = {}
        if not frame_list:
            nuke.warning(f"Skipping {node.name()}: Invalid frame range.")
            continue

        original_frame = nuke.frame()
        nuke.message(f"Processing {len(frame_list)} frames for {node.name()} (stride {pixel_stride})...")

        for frame in frame_list:
            nuke.frame(frame)
            try:
                linear_pixels = get_linear_pixels_from_node(node, frame, pixel_stride=pixel_stride)
                if not linear_pixels:
                    nuke.warning(f"No pixels sampled for frame {frame} on {node.name()}, skipping.")
                    continue
                dominant_colors_linear = sample_and_aggregate_colors(linear_pixels,
                                                                     pixel_stride=1,
                                                                     color_quantization_factor=quantize_factor)
                all_dominant_colors[str(frame)] = dominant_colors_linear
            except Exception as e:
                nuke.warning(f"Error processing frame {frame} for {node.name()}: {e}")
                continue

        # restore original frame
        nuke.frame(original_frame)

        # write per-frame JSON
        try:
            with open(json_path, 'w') as f:
                json.dump(all_dominant_colors, f, indent=2)
        except Exception as e:
            nuke.critical(f"Failed to save per-frame JSON at {json_path}: {e}")
            continue

        # produce consolidated 16-bit summary and write to separate JSON
        consolidated = consolidate_precise_colors(all_dominant_colors, max_colors=MAX_CONSOLIDATED_COLORS,
                                                  clamp_min=CLAMP_MIN, clamp_max=CLAMP_MAX)
        consolidated_path = os.path.splitext(json_path)[0] + "_consolidated.json"
        try:
            with open(consolidated_path, 'w') as f:
                json.dump(consolidated, f, indent=2)
        except Exception as e:
            nuke.critical(f"Failed to save consolidated JSON at {consolidated_path}: {e}")
            continue

        # success message summarizing results
        num_frames = len(all_dominant_colors)
        num_colors = len(consolidated)
        nuke.message(f"Exported {num_frames} frames (per-frame JSON) to:\n{json_path}\n\n"
                     f"Exported consolidated {num_colors} colors to:\n{consolidated_path}")

# --- Optional helper to apply the first consolidated color to selected node tile_color ---
def convert_linear_to_display_color(linear_rgb_list):
    gamma_correction = 1.0 / 2.2
    display_rgb = []
    for val in linear_rgb_list:
        clamped_linear_val = max(0.0, val)
        display_val = math.pow(clamped_linear_val, gamma_correction)
        clipped_display_val = min(1.0, display_val)
        display_rgb.append(clipped_display_val)
    return display_rgb

def float_to_packed_int(r, g, b, a=1.0):
    r_int = int(round(r * 255))
    g_int = int(round(g * 255))
    b_int = int(round(b * 255))
    a_int = int(round(a * 255))
    return (a_int &lt;&lt; 24) + (b_int &lt;&lt; 16) + (g_int &lt;&lt; 8) + r_int

def apply_consolidated_color_to_selected_node(consolidated, node):
    if not consolidated:
        nuke.message("No consolidated colors available.")
        return
    # use the most frequent color (first)
    rec = consolidated[0]
    linear_avg = rec["rgb_linear_avg"]
    display_rgb = convert_linear_to_display_color(linear_avg)
    packed = float_to_packed_int(display_rgb[0], display_rgb[1], display_rgb[2])
    node['tile_color'].setValue(packed)

# --- Example usage from Script Editor ---
# import export_cellcolors
# export_cellcolors.export_colors_from_selected_read_nodes()
</pre>

<div id="Further Dev-QUESTION"><h2 id="QUESTION" class="header"><a href="#Further Dev-QUESTION">QUESTION</a></h2></div>

<table>
<tr>
<td>
I'm using 'sample' method to query the values of images in Nuke. I'm saving JSON dictionaries of color values. Is there any way to make something as fast as Nuke's CurveTool? My guess is that Python is a bottleneck.
</td>
</tr>
</table>


<p>
You're correct; when querying thousands or millions of pixels across multiple frames, Python is almost certainly the bottleneck compared to Nuke's internal C++ processing, like the CurveTool. The sample() method requires Nuke to translate data from its native C++ memory structure into Python objects, which is an expensive process repeated for every pixel you sample.
</p>

<p>
To achieve speeds closer to the CurveTool, you must leverage Nuke's internal vectorized processing using the nuke.buffers module (AI GARBAGE - DOESN'T EXIST) or OIIO (OpenImageIO), which are specifically designed to minimize the Python bottleneck by processing large blocks of data at once.
</p>

<p>
Here are the two fastest approaches, which avoid the per-pixel Python overhead:
</p>

<div id="Further Dev-QUESTION-Using nuke.buffers.getImage() (The Fastest Nuke-Native Way)"><h4 id="Using nuke.buffers.getImage() (The Fastest Nuke-Native Way)" class="header"><a href="#Further Dev-QUESTION-Using nuke.buffers.getImage() (The Fastest Nuke-Native Way)">Using nuke.buffers.getImage() (The Fastest Nuke-Native Way)</a></h4></div>

<table>
<tr>
<td>
AI Garbage deleted.
</td>
</tr>
</table>

<p>
Methods don't exist.
</p>

<div id="Further Dev-QUESTION-Using Nuke's CurveTool Itself (The &quot;No Code&quot; Solution)"><h3 id="Using Nuke's CurveTool Itself (The &quot;No Code&quot; Solution)" class="header"><a href="#Further Dev-QUESTION-Using Nuke's CurveTool Itself (The &quot;No Code&quot; Solution)">Using Nuke's CurveTool Itself (The "No Code" Solution)</a></h3></div>

<p>
The CurveTool is a powerful and very fast way to generate statistical data (average, min/max, deviation) from an image sequence. Since you're looking for something as fast as it, you can simply use the CurveTool node's output data instead of writing your own sampling script.
</p>

<div id="Further Dev-QUESTION-Using Nuke's CurveTool Itself (The &quot;No Code&quot; Solution)-‚öôÔ∏è Workflow"><h4 id="‚öôÔ∏è Workflow" class="header"><a href="#Further Dev-QUESTION-Using Nuke's CurveTool Itself (The &quot;No Code&quot; Solution)-‚öôÔ∏è Workflow">‚öôÔ∏è Workflow</a></h4></div>
<ul>
<li>
Insert CurveTool: Add a CurveTool node after the image sequence you want to sample.

<li>
Configure: Set the "ROI" (Region of Interest) and select the statistics you want to track (e.g., Average, Max).

<li>
Analyze: Press the "Analyze" button. The CurveTool writes all the requested data to its own knobs (e.g., average.r, average.g).

<li>
Extract Data: Use a simple Python script to read the animated values from these knobs and save them to your JSON file.

</ul>
<div id="Further Dev-QUESTION-Using Nuke's CurveTool Itself (The &quot;No Code&quot; Solution)-Extraction Code (Very Fast)"><h4 id="Extraction Code (Very Fast)" class="header"><a href="#Further Dev-QUESTION-Using Nuke's CurveTool Itself (The &quot;No Code&quot; Solution)-Extraction Code (Very Fast)">Extraction Code (Very Fast)</a></h4></div>

<p>
This approach is fast because the heavy pixel processing is done by the C++ CurveTool, and Python only has to read a few animated knob values per frame.
</p>

<pre python>
<table>
<tr>
<td>
UNTESTED
</td>
</tr>
</table>

import nuke

def extract_curvetool_data(curvetool_node_name, start_frame, end_frame):
    node = nuke.toNode(curvetool_node_name)
    if not node or node.Class() != 'CurveTool':
        print(f"Node {curvetool_node_name} not found or not a CurveTool.")
        return []

    json_data = []
    
    # The statistics knobs are animated, we just read the values
    for frame in range(start_frame, end_frame + 1):
        nuke.frame(frame)
        
        frame_data = {
            "frame": frame,
            "r_avg": node.knob('average.r').value(),
            "g_avg": node.knob('average.g').value(),
            "b_avg": node.knob('average.b').value()
            # Add other stats as needed
        }
        json_data.append(frame_data)
        
    return json_data

# Example Usage:
# all_stats = extract_curvetool_data('CurveTool1', 1, 100)
# # You can now save 'all_stats' to your JSON file
</pre>

<p>
The second method is often the simplest and fastest solution, as it offloads all the heavy computational work to the optimized Nuke/C++ core.
</p>

<div id="Further Dev-QUESTION"><h2 id="QUESTION" class="header"><a href="#Further Dev-QUESTION">QUESTION</a></h2></div>

<table>
<tr>
<td>
I have images with a maximum of 50 identical colors. I want to go through and list these values in a dictionary (perhaps JSON). Can I leverage this technique to gather the set of colors?
</td>
</tr>
</table>

<p>
Yes, you can absolutely leverage the technique of processing pixel data in large blocks to efficiently gather the set of up to 50 unique colors, and it will be significantly faster than using repeated nuke.sample().
</p>

<p>
Efficiently Gathering Unique Colors (The NumPy Method)
This method avoids the Python per-pixel loop entirely, which is essential for speed.
</p>

<div id="Further Dev-QUESTION-üêç Workflow"><h4 id="üêç Workflow" class="header"><a href="#Further Dev-QUESTION-üêç Workflow">üêç Workflow</a></h4></div>


<table>
<tr>
<td>
AI Garbage deleted
</td>
</tr>
</table>


<div id="PyOpenImageIO (PyIOP)"><h1 id="PyOpenImageIO (PyIOP)" class="header"><a href="#PyOpenImageIO (PyIOP)">PyOpenImageIO (PyIOP)</a></h1></div>

<p>
To gather the set of unique colors quickly, you must access the raw image data and process it using a fast, external library like NumPy. We can use the library Nuke relies on for image I/O: PyOpenImageIO (PyIOP).
</p>

<div id="PyOpenImageIO (PyIOP)-üêç Workflow"><h2 id="üêç Workflow" class="header"><a href="#PyOpenImageIO (PyIOP)-üêç Workflow">üêç Workflow</a></h2></div>

<ul>
<li>
<span id="PyOpenImageIO (PyIOP)-üêç Workflow-Force Data into Memory"></span><strong id="Force Data into Memory">Force Data into Memory</strong>: Use a nuke.execute() call on a temporary Write node to force the image data to be rendered into a temporary file on disk.

<li>
<span id="PyOpenImageIO (PyIOP)-üêç Workflow-Read with PyIOP"></span><strong id="Read with PyIOP">Read with PyIOP</strong>: Use OpenImageIO.ImageBuf to read the entire image file into memory.

<li>
<span id="PyOpenImageIO (PyIOP)-üêç Workflow-Convert to NumPy"></span><strong id="Convert to NumPy">Convert to NumPy</strong>: Convert the ImageBuf data into a NumPy array.

<li>
<span id="PyOpenImageIO (PyIOP)-üêç Workflow-Find Unique Colors"></span><strong id="Find Unique Colors">Find Unique Colors</strong>: Use the highly optimized numpy.unique function to find the color set.

</ul>
<div id="PyOpenImageIO (PyIOP)-Corrected Example Code"><h2 id="Corrected Example Code" class="header"><a href="#PyOpenImageIO (PyIOP)-Corrected Example Code">Corrected Example Code</a></h2></div>

<p>
This code requires a temporary file write, but the processing speed is still vastly superior to using per-pixel nuke.sample().
</p>

<pre python>
<table>
<tr>
<td>
FAILED
</td>
</tr>
</table>

import nuke
import OpenImageIO as oiio
import numpy as np
import tempfile
import os

def get_unique_colors_fast(input_node, frame, max_colors=50):
    node = nuke.toNode(input_node)
    if not node:
        return {"error": f"Node '{input_node}' not found."}

    # 1. SET UP AND RENDER TO A TEMPORARY FILE
    # Use a high-precision format (e.g., .exr) for color fidelity
    temp_file = os.path.join(tempfile.gettempdir(), f"nuke_temp_colors_{os.getpid()}.exr")
    
    # Create a temporary Write node
    w = nuke.createNode('Write', inpanel=False)
    w['file'].setValue(temp_file)
    w['file_type'].setValue('exr')
    w.setInput(0, node)

    try:
        # Execute the write to render the image to the temp file
        nuke.execute(w, frame, frame)

        # 2. READ THE DATA WITH OPENIMAGEIO (OIIO)
        image_buf = oiio.ImageBuf(temp_file)
        if not image_buf.initialized:
            return {"error": "Failed to read temporary image file."}
            
        # 3. CONVERT TO NUMPY
        # Read the RGB channel data into a NumPy array
        # This is the fastest way to get pixel data in Nuke
        data = image_buf.get_pixels(oiio.TypeDesc.FLOAT) 
        
        # OIIO returns [Height, Width, Channels]. Reshape to [Pixels, Channels]
        # We only need the first 3 channels (RGB)
        data_2d = data.reshape(-1, data.shape[-1])[:, :3] 
        # Explanation:
            # Collapse the shape of the array with -1 (collapse to just the pixel column
            # then slice with 'all rows' ([:]), the first three entries ([:3]).

        # 4. FIND UNIQUE COLORS
        unique_colors, counts = np.unique(data_2d, axis=0, return_counts=True)
        
        if len(unique_colors) &gt; max_colors:
            return {"warning": f"Image has {len(unique_colors)} unique colors, exceeding limit."}

        # 5. FORMAT OUTPUT
        color_dict = {}
        for i, color_vector in enumerate(unique_colors):
            r, g, b = color_vector.tolist()
            color_key = f"({r:.6f}, {g:.6f}, {b:.6f})"
            color_dict[color_key] = {
                "r": r,
                "g": g,
                "b": b,
                "count": int(counts[i])
            }
        
        return {"unique_colors": color_dict}

    finally:
        # Clean up the temporary Write node and file
        nuke.delete(w)
        if os.path.exists(temp_file):
            os.remove(temp_file)

# Example Usage:
# results = get_unique_colors_fast('Read1', 101)
# print(results)
</pre>

<p>
The code above errorred on the following lines (since this syntax doesn't exist):
</p>
<pre python>
        # 3. CONVERT TO NUMPY
        # Read the RGB channel data into a NumPy array
        # This is the fastest way to get pixel data in Nuke
        data = image_buf.get_pixels(oiio.TypeDesc.FLOAT) 
</pre>

<p>
Another attempt:
</p>
<pre python>
<table>
<tr>
<td>
WORKS!
</td>
</tr>
</table>
import nuke
import OpenImageIO as oiio
import numpy as np
import tempfile
import os

def get_unique_colors_fast(input_node, frame, max_colors=50):
    node = nuke.toNode(input_node)
    if not node:
        return {"error": f"Node '{input_node}' not found."}

    # 1. SET UP AND RENDER TO A TEMPORARY FILE
    # Use a high-precision format (e.g., .exr) for color fidelity
    temp_file = os.path.join(tempfile.gettempdir(), f"nuke_temp_colors_{os.getpid()}.exr")
    
    # Create a temporary Write node
    w = nuke.createNode('Write', inpanel=False)
    w['file'].setValue(temp_file)
    w['file_type'].setValue('exr')
    w.setInput(0, node)

    try:
        # Execute the write to render the image to the temp file
        nuke.execute(w, frame, frame)

        # 2. READ THE DATA WITH OPENIMAGEIO (OIIO)
        image_buf = oiio.ImageBuf(temp_file)
        if not image_buf.initialized:
            return {"error": "Failed to read temporary image file."}
            
        # 3. CONVERT TO NUMPY
        # Read the RGB channel data into a NumPy array
        # Use oiio.FLOAT for single-precision float type
        data = image_buf.get_pixels(oiio.FLOAT) # The corrected line
        
        # OIIO returns [Height, Width, Channels]. Reshape to [Pixels, Channels]
        # We only need the first 3 channels (RGB)
        data_2d = data.reshape(-1, data.shape[-1])[:, :3] 
        # Explanation:
            # Collapse the shape of the array with -1 (collapse to just the pixel column
            # then slice with 'all rows' ([:]), the first three entries ([:3]).

        # 4. FIND UNIQUE COLORS
        # Note: np.unique can be slow for large images and many unique values
        unique_colors, counts = np.unique(data_2d, axis=0, return_counts=True)
        
        if len(unique_colors) &gt; max_colors:
            return {"warning": f"Image has {len(unique_colors)} unique colors, exceeding limit of {max_colors}."}

        # 5. FORMAT OUTPUT
        color_dict = {}
        for i, color_vector in enumerate(unique_colors):
            r, g, b = color_vector.tolist()
            color_key = f"({r:.6f}, {g:.6f}, {b:.6f})"
            color_dict[color_key] = {
                "r": r,
                "g": g,
                "b": b,
                "count": int(counts[i])
            }
        
        return {"unique_colors": color_dict}

    finally:
        # Clean up the temporary Write node and file
        nuke.delete(w)
        if os.path.exists(temp_file):
            os.remove(temp_file)
</pre>

<p>
This script is well-structured and efficient for extracting unique RGB colors from a Nuke node using OpenImageIO and NumPy. Here are a few suggestions and observations to further refine or safeguard the workflow:
</p>

<div id="PyOpenImageIO (PyIOP)-Corrected Example Code-‚úÖ Strengths"><h4 id="‚úÖ Strengths" class="header"><a href="#PyOpenImageIO (PyIOP)-Corrected Example Code-‚úÖ Strengths">‚úÖ Strengths</a></h4></div>

<ul>
<li>
High-precision EXR output ensures accurate color fidelity.

<li>
Efficient use of NumPy for reshaping and deduplicating pixel data.

<li>
Temporary file management is clean and avoids clutter.

<li>
Graceful error handling for missing nodes and unreadable image buffers.

</ul>
<div id="PyOpenImageIO (PyIOP)-Corrected Example Code-üõ† Suggestions for Improvement"><h3 id="üõ† Suggestions for Improvement" class="header"><a href="#PyOpenImageIO (PyIOP)-Corrected Example Code-üõ† Suggestions for Improvement">üõ† Suggestions for Improvement</a></h3></div>

<div id="PyOpenImageIO (PyIOP)-Corrected Example Code-üõ† Suggestions for Improvement-Avoid Overwriting Temp Files"><h4 id="Avoid Overwriting Temp Files" class="header"><a href="#PyOpenImageIO (PyIOP)-Corrected Example Code-üõ† Suggestions for Improvement-Avoid Overwriting Temp Files">Avoid Overwriting Temp Files</a></h4></div>

<p>
If multiple frames or nodes are processed in parallel, the temp file name could clash. Consider adding a random suffix:
</p>

<pre python>
import uuid
temp_file = os.path.join(tempfile.gettempdir(), f"nuke_temp_colors_{uuid.uuid4().hex}.exr")
</pre>

<div id="PyOpenImageIO (PyIOP)-Corrected Example Code-üõ† Suggestions for Improvement-Limit Memory Usage for Large Images"><h4 id="Limit Memory Usage for Large Images" class="header"><a href="#PyOpenImageIO (PyIOP)-Corrected Example Code-üõ† Suggestions for Improvement-Limit Memory Usage for Large Images">Limit Memory Usage for Large Images</a></h4></div>

<p>
For very large images, reading all pixels into memory might be expensive. You could add a resolution check:
</p>

<pre python>
if image_buf.spec().width * image_buf.spec().height &gt; 1e7:  # e.g., 10 million pixels
    return {"error": "Image too large for color analysis."}
</pre>

<div id="PyOpenImageIO (PyIOP)-Corrected Example Code-üõ† Suggestions for Improvement-Optional: Sort Colors by Frequency"><h4 id="Optional: Sort Colors by Frequency" class="header"><a href="#PyOpenImageIO (PyIOP)-Corrected Example Code-üõ† Suggestions for Improvement-Optional: Sort Colors by Frequency">Optional: Sort Colors by Frequency</a></h4></div>

<p>
If you want to prioritize the most common colors:
</p>

<pre python>
sorted_indices = np.argsort(-counts)
unique_colors = unique_colors[sorted_indices]
counts = counts[sorted_indices]
</pre>

<div id="PyOpenImageIO (PyIOP)-Count the Number of Unique Colors"><h2 id="Count the Number of Unique Colors" class="header"><a href="#PyOpenImageIO (PyIOP)-Count the Number of Unique Colors">Count the Number of Unique Colors</a></h2></div>

<p>
This counts the number of unique colors and reports them in the returned string:
</p>
<pre python>
<table>
<tr>
<td>
UNTESTED
</td>
</tr>
</table>
import nuke
import OpenImageIO as oiio
import numpy as np
import tempfile
import os

def get_unique_colors_fast(input_node, frame, max_colors=50):
    node = nuke.toNode(input_node)
    if not node:
        return {"error": f"Node '{input_node}' not found."}

    # 1. SET UP AND RENDER TO A TEMPORARY FILE
    # Use a high-precision format (e.g., .exr) for color fidelity
    # Using oiio.FLOAT, as corrected in the previous turn
    temp_file = os.path.join(tempfile.gettempdir(), f"nuke_temp_colors_{os.getpid()}.exr")
    
    # Create a temporary Write node
    w = nuke.createNode('Write', inpanel=False)
    w['file'].setValue(temp_file)
    w['file_type'].setValue('exr')
    w.setInput(0, node)

    try:
        # Execute the write to render the image to the temp file
        nuke.execute(w, frame, frame)

        # 2. READ THE DATA WITH OPENIMAGEIO (OIIO)
        image_buf = oiio.ImageBuf(temp_file)
        if not image_buf.initialized:
            return {"error": "Failed to read temporary image file."}
            
        # 3. CONVERT TO NUMPY
        # Read the RGB channel data into a NumPy array
        data = image_buf.get_pixels(oiio.FLOAT)
        
        # OIIO returns [Height, Width, Channels]. Reshape to [Pixels, Channels]
        data_2d = data.reshape(-1, data.shape[-1])[:, :3] 

        # 4. FIND UNIQUE COLORS
        unique_colors, counts = np.unique(data_2d, axis=0, return_counts=True)
        
        # Capture the total count here
        total_unique_colors = len(unique_colors) # &lt;--- NEW LINE: Get the count
        
        if total_unique_colors &gt; max_colors:
            return {"warning": f"Image has {total_unique_colors} unique colors, exceeding limit of {max_colors}."}

        # 5. FORMAT OUTPUT
        color_dict = {}
        for i, color_vector in enumerate(unique_colors):
            r, g, b = color_vector.tolist()
            color_key = f"({r:.6f}, {g:.6f}, {b:.6f})"
            color_dict[color_key] = {
                "r": r,
                "g": g,
                "b": b,
                "count": int(counts[i])
            }
        
        # &lt;--- MODIFICATION: Add the total count to the final return dictionary
        return {
            "total_unique_colors": total_unique_colors,
            "unique_colors": color_dict
        }

    finally:
        # Clean up the temporary Write node and file
        nuke.delete(w)
        if os.path.exists(temp_file):
            os.remove(temp_file)
</pre>

<div id="PyOpenImageIO (PyIOP)-QUESTION"><h2 id="QUESTION" class="header"><a href="#PyOpenImageIO (PyIOP)-QUESTION">QUESTION</a></h2></div>

<table>
<tr>
<td>
If this were intended to be run on the Read node's input frame range, how would the code change? Would another calling function be the best way ahead (to call the 'per frame' function, or would just adding a range to the arguments for the existing function suffice? And how would you determine how many frames were being read by the Read node? Using a separate calling function is the best and most idiomatic way to handle a frame range in Nuke. The existing get_unique_colors_fast function is well-designed as a single-frame operation, and adding a loop to a separate function keeps the logic clean and reusable.
</td>
</tr>
</table>

<div id="PyOpenImageIO (PyIOP)-QUESTION-ANSWER"><h3 id="ANSWER" class="header"><a href="#PyOpenImageIO (PyIOP)-QUESTION-ANSWER">ANSWER</a></h3></div>

<p>
Here's how to determine the frame range of the Read node and the code changes required.
</p>

<div id="PyOpenImageIO (PyIOP)-QUESTION-Determining the Read Node's Frame Range"><h3 id="Determining the Read Node's Frame Range" class="header"><a href="#PyOpenImageIO (PyIOP)-QUESTION-Determining the Read Node's Frame Range">Determining the Read Node's Frame Range</a></h3></div>

<p>
You can determine the frame range of a Nuke node (like a Read node) by querying its first and last knobs.
</p>

<pre python>
node = nuke.toNode('Read1') # Example node name

if node and node.Class() == 'Read':
    first_frame = int(node['first'].value())
    last_frame = int(node['last'].value())
    
    # Or, use the global settings if the Read node is set to 'frame range' 'input'
    # nuke.root()['first_frame'].value() 
    # nuke.root()['last_frame'].value()
    
    print(f"Read node frame range: {first_frame} to {last_frame}")
</pre>

<p>
For robust analysis, you typically want the frame range defined by the node itself (the first and last knobs), as this is the data it is capable of providing.
</p>

<div id="PyOpenImageIO (PyIOP)-QUESTION-Implementing the Multi-Frame Caller Function"><h3 id="Implementing the Multi-Frame Caller Function" class="header"><a href="#PyOpenImageIO (PyIOP)-QUESTION-Implementing the Multi-Frame Caller Function">Implementing the Multi-Frame Caller Function</a></h3></div>

<p>
We'll create a new function, analyze_range, that loops through the desired frame range and calls your existing get_unique_colors_fast for each frame, aggregating the results.
</p>

<div id="PyOpenImageIO (PyIOP)-QUESTION-Implementing the Multi-Frame Caller Function-Modified Code Structure"><h4 id="Modified Code Structure" class="header"><a href="#PyOpenImageIO (PyIOP)-QUESTION-Implementing the Multi-Frame Caller Function-Modified Code Structure">Modified Code Structure</a></h4></div>

<p>
Here is the complete, recommended structure:
</p>

<pre python>
<table>
<tr>
<td>
WORKS!
</td>
</tr>
</table>
import nuke
import OpenImageIO as oiio
import numpy as np
import tempfile
import os
import uuid # Using uuid for better robustness as discussed

# ----------------------------------------------------------------------
# A. SINGLE-FRAME PROCESSING FUNCTION (Updated for robustness)
# ----------------------------------------------------------------------
def get_unique_colors_fast(input_node, frame, max_colors=50):
    node = nuke.toNode(input_node)
    if not node:
        # Note: If called from outside Nuke's GUI, nuke.toNode may not work as expected.
        return {"error": f"Node '{input_node}' not found."}

    # 1. SET UP AND RENDER TO A TEMPORARY FILE
    # Use uuid.uuid4() for unique filenames per call (robustness)
    temp_file = os.path.join(tempfile.gettempdir(), f"nuke_temp_colors_{uuid.uuid4().hex}.exr")
    
    w = nuke.createNode('Write', inpanel=False)
    w['file'].setValue(temp_file)
    w['file_type'].setValue('exr')
    w.setInput(0, node)

    try:
        # Execute the write to render the image to the temp file
        # The frame argument tells Nuke which frame to process
        nuke.execute(w, frame, frame)

        # 2. READ THE DATA AND PROCESS (Steps 2-5 remain the same)
        image_buf = oiio.ImageBuf(temp_file)
        if not image_buf.initialized:
            return {"error": "Failed to read temporary image file."}
            
        data = image_buf.get_pixels(oiio.FLOAT)
        data_2d = data.reshape(-1, data.shape[-1])[:, :3] 

        unique_colors, counts = np.unique(data_2d, axis=0, return_counts=True)
        total_unique_colors = len(unique_colors)
        
        if total_unique_colors &gt; max_colors:
             # Returning a warning instead of full processing to save memory/time
             return {"warning": f"Image has {total_unique_colors} unique colors, exceeding limit of {max_colors}."}

        color_dict = {}
        for i, color_vector in enumerate(unique_colors):
            r, g, b = color_vector.tolist()
            color_key = f"({r:.6f}, {g:.6f}, {b:.6f})"
            color_dict[color_key] = {
                "r": r, "g": g, "b": b, "count": int(counts[i])
            }
        
        return {
            "total_unique_colors": total_unique_colors,
            "unique_colors": color_dict
        }

    finally:
        # Clean up the temporary Write node and file
        nuke.delete(w)
        if os.path.exists(temp_file):
            os.remove(temp_file)


# ----------------------------------------------------------------------
# B. MULTI-FRAME CALLER FUNCTION (New Function)
# ----------------------------------------------------------------------
def analyze_range(input_node_name, max_colors=50):
    """
    Analyzes the unique colors for every frame in the input node's
    defined frame range.
    """
    node = nuke.toNode(input_node_name)
    if not node:
        return {"error": f"Node '{input_node_name}' not found."}
    
    # 1. DETERMINE FRAME RANGE
    # Get the frame range from the node's knobs (e.g., Read node 'first' and 'last')
    try:
        first_frame = int(node['first'].value())
        last_frame = int(node['last'].value())
    except NameError:
        return {"error": f"Node '{input_node_name}' does not have 'first'/'last' frame knobs."}

    print(f"Starting analysis for {input_node_name} from frame {first_frame} to {last_frame}...")
    
    range_results = {}
    
    # 2. LOOP THROUGH FRAMES
    for frame in range(first_frame, last_frame + 1):
        print(f"Processing frame: {frame}...")
        
        # Call the single-frame function
        result = get_unique_colors_fast(input_node_name, frame, max_colors)
        
        # Store the result keyed by the frame number
        range_results[frame] = result

    print("Analysis complete.")
    return {"full_analysis": range_results}


# Example Usage for a Read node named 'Read1':
# results_over_range = analyze_range('Read1')
# print(results_over_range)
</pre>

<div id="PyOpenImageIO (PyIOP)-QUESTION-Implementing the Multi-Frame Caller Function-Why a Separate Function is Best"><h4 id="Why a Separate Function is Best" class="header"><a href="#PyOpenImageIO (PyIOP)-QUESTION-Implementing the Multi-Frame Caller Function-Why a Separate Function is Best">Why a Separate Function is Best</a></h4></div>

<ul>
<li>
<span id="PyOpenImageIO (PyIOP)-QUESTION-Implementing the Multi-Frame Caller Function-Why a Separate Function is Best-Separation of Concerns"></span><strong id="Separation of Concerns">Separation of Concerns</strong>: get_unique_colors_fast is now responsible only for processing a single frame, making it easier to test and debug. analyze_range is responsible only for managing the sequence (looping and aggregation).

<li>
<span id="PyOpenImageIO (PyIOP)-QUESTION-Implementing the Multi-Frame Caller Function-Why a Separate Function is Best-Flexibility"></span><strong id="Flexibility">Flexibility</strong>: The get_unique_colors_fast function can still be called directly for single-frame testing or custom needs.

<li>
<span id="PyOpenImageIO (PyIOP)-QUESTION-Implementing the Multi-Frame Caller Function-Why a Separate Function is Best-Clarity"></span><strong id="Clarity">Clarity</strong>: The caller function clearly defines the control flow (looping over the range), making the script's intent immediately obvious.

</ul>
<div id="PyOpenImageIO (PyIOP)-Add Support for Alpha or Other Channels"><h2 id="Add Support for Alpha or Other Channels" class="header"><a href="#PyOpenImageIO (PyIOP)-Add Support for Alpha or Other Channels">Add Support for Alpha or Other Channels</a></h2></div>

<p>
If needed, you can extend the slicing logic to include alpha or other channels:
</p>

<pre python>
data_2d = data.reshape(-1, data.shape[-1])[:, :4]  # RGBA
</pre>

<div id="PyOpenImageIO (PyIOP)-Add Support for Alpha or Other Channels-üß™ Optional Enhancements"><h4 id="üß™ Optional Enhancements" class="header"><a href="#PyOpenImageIO (PyIOP)-Add Support for Alpha or Other Channels-üß™ Optional Enhancements">üß™ Optional Enhancements</a></h4></div>

<ul>
<li>
Add a verbose flag to return additional metadata (image size, bit depth, etc.).

<li>
Expose file_type as a parameter if you want flexibility beyond EXR.

<li>
Support batch processing of multiple frames or nodes.

</ul>
<div id="Understanding data.shape"><h1 id="Understanding data.shape" class="header"><a href="#Understanding data.shape">Understanding data.shape</a></h1></div>

<p>
Every NumPy array has a .shape attribute, which is a tuple describing its dimensions.
</p>

<p>
For an image read by OIIO, you typically get:
</p>

<pre>
data.shape = (Height, Width, Channels)
</pre>

<div id="Understanding data.shape-Example:"><h4 id="Example:" class="header"><a href="#Understanding data.shape-Example:">Example:</a></h4></div>

<ul>
<li>
A 720p RGB image ‚Üí (720, 1280, 3)

<li>
A 4K RGBA image ‚Üí (2160, 3840, 4)

</ul>
<div id="Understanding data.shape-What data.shape[-1] means"><h3 id="What data.shape[-1] means" class="header"><a href="#Understanding data.shape-What data.shape[-1] means">What data.shape[-1] means</a></h3></div>

<ul>
<li>
shape[-1] means ‚Äúthe last element of the shape tuple.‚Äù

</ul>
<p>
In this case, that‚Äôs the number of channels (e.g., 3 for RGB, 4 for RGBA).
</p>

<p>
So if:
</p>

<pre python>
data.shape == (720, 1280, 3)
</pre>
<p>
then:
</p>
<pre python>
data.shape[-1] == 3
</pre>

<div id="Understanding data.shape-What data.shape[-1] means-Why use it in reshape(-1, data.shape[-1])"><h4 id="Why use it in reshape(-1, data.shape[-1])" class="header"><a href="#Understanding data.shape-What data.shape[-1] means-Why use it in reshape(-1, data.shape[-1])">Why use it in reshape(-1, data.shape[-1])</a></h4></div>

<p>
The reshape call is:
</p>

<pre python>
data.reshape(-1, data.shape[-1])
</pre>
<ul>
<li>
The -1 tells NumPy: ‚Äúfigure out this dimension automatically by flattening everything else.‚Äù

<li>
<code>data.shape[-1]</code> fixes the second dimension to be the number of channels.

</ul>
<p>
So the result is a 2D array:
</p>

<pre>
(Pixels, Channels)
</pre>

<div id="Understanding data.shape-What data.shape[-1] means-Example:"><h4 id="Example:" class="header"><a href="#Understanding data.shape-What data.shape[-1] means-Example:">Example:</a></h4></div>

<ul>
<li>
Original shape: (720, 1280, 3)

<li>
After reshape: (921600, 3) ‚Üí 921,600 pixels, each with 3 values (R, G, B).

</ul>
<p>
‚úÖ In plain English: data.shape[-1] is just a neat way of saying ‚Äúhowever many channels this image has, keep that as the second dimension.‚Äù
</p>

<div id="Understanding data.shape-Illustration with An Example Array"><h3 id="Illustration with An Example Array" class="header"><a href="#Understanding data.shape-Illustration with An Example Array">Illustration with An Example Array</a></h3></div>

<table>
<tr>
<td>
Can you give an example of the array so I can see it working step by step on a real array?
</td>
</tr>
</table>

<p>
Here is a step-by-step example using a hypothetical 2x2, 4-channel (RGBA) image.
</p>

<div id="Understanding data.shape-Illustration with An Example Array-Step 1: Initial ImageBuf data"><h4 id="Step 1: Initial ImageBuf data" class="header"><a href="#Understanding data.shape-Illustration with An Example Array-Step 1: Initial ImageBuf data">Step 1: Initial ImageBuf data</a></h4></div>

<p>
Assume a 2x2 pixel image with 4 channels (RGBA) is loaded into the ImageBuf. The pixel values are stored in a multi-dimensional array. 
</p>

<div id="Understanding data.shape-Illustration with An Example Array-Initial Data Array (Conceptual):"><h4 id="Initial Data Array (Conceptual):" class="header"><a href="#Understanding data.shape-Illustration with An Example Array-Initial Data Array (Conceptual):">Initial Data Array (Conceptual):</a></h4></div>

<p>
This is a 3D array representing a 2x2 image, with each pixel having 4 color values. 
</p>
<pre>
[
  [ [0.1, 0.2, 0.3, 1.0], [0.4, 0.5, 0.6, 1.0] ],  // Row 1
  [ [0.7, 0.8, 0.9, 1.0], [1.0, 1.0, 1.0, 1.0] ]   // Row 2
]
</pre>

<ul>
<li>
Dimensions: (2, 2, 4)

<li>
Data Type: The original data type could be different, but it's not important until the next step.

</ul>
<div id="Understanding data.shape-Illustration with An Example Array-Step 2: image_buf.get_pixels(oiio.FLOAT)"><h4 id="Step 2: image_buf.get_pixels(oiio.FLOAT)" class="header"><a href="#Understanding data.shape-Illustration with An Example Array-Step 2: image_buf.get_pixels(oiio.FLOAT)">Step 2: image_buf.get_pixels(oiio.FLOAT)</a></h4></div>

<p>
The get_pixels() call converts the data into a NumPy array with the float data type. 
</p>

<p>
Intermediate Array (data):
</p>

<p>
This is the NumPy array that get_pixels() returns.
</p>

<pre>
[
  [ [0.1, 0.2, 0.3, 1.0], [0.4, 0.5, 0.6, 1.0] ],
  [ [0.7, 0.8, 0.9, 1.0], [1.0, 1.0, 1.0, 1.0] ]
]
</pre>
<ul>
<li>
Dimensions: (2, 2, 4)

<li>
Data Type: numpy.float32 (assuming a standard float conversion). 

</ul>
<div id="Understanding data.shape-Illustration with An Example Array-Step 3: data.reshape(-1, data.shape[-1])"><h4 id="Step 3: data.reshape(-1, data.shape[-1])" class="header"><a href="#Understanding data.shape-Illustration with An Example Array-Step 3: data.reshape(-1, data.shape[-1])">Step 3: data.reshape(-1, data.shape[-1])</a></h4></div>

<p>
The reshape() operation flattens the first two dimensions (width and height) into a single dimension. 
</p>

<p>
Intermediate Array (data_2d before slicing):
</p>

<p>
This creates a new array where all pixels are in a single list, but their channel data is still grouped. 
</p>
<pre>
[[0.1, 0.2, 0.3, 1.0],   // Pixel 1, Channel 1-4
 [0.4, 0.5, 0.6, 1.0],   // Pixel 2, Channel 1-4
 [0.7, 0.8, 0.9, 1.0],   // Pixel 3, Channel 1-4
 [1.0, 1.0, 1.0, 1.0]]   // Pixel 4, Channel 1-4
</pre>
<ul>
<li>
Dimensions: (4, 4)

<li>
Data Type: numpy.float32 

</ul>
<div id="Understanding data.shape-Illustration with An Example Array-Step 4: [:, :3]"><h4 id="Step 4: [:, :3]" class="header"><a href="#Understanding data.shape-Illustration with An Example Array-Step 4: [:, :3]">Step 4: [:, :3]</a></h4></div>

<p>
The slicing operation [:, :3] selects all rows (:) and only the first three columns (:3), effectively dropping the fourth channel (alpha). 
</p>

<p>
Final Array (data_2d):
</p>

<p>
This is the final array, with all pixels in a single list, and only the first three channels (RGB).
</p>
<pre>
[[0.1, 0.2, 0.3],   // Pixel 1, Channels 1-3
 [0.4, 0.5, 0.6],   // Pixel 2, Channels 1-3
 [0.7, 0.8, 0.9],   // Pixel 3, Channels 1-3
 [1.0, 1.0, 1.0]]   // Pixel 4, Channels 1-3
</pre>

<ul>
<li>
Dimensions: (4, 3)

<li>
Data Type: numpy.float32

</ul>
<div id="Understanding data.shape-QUESTION"><h2 id="QUESTION" class="header"><a href="#Understanding data.shape-QUESTION">QUESTION</a></h2></div>

<table>
<tr>
<td>
My mind's eye focuses briefly on the concept, but it's hard to retain.
</td>
</tr>
</table>

<p>
Perfect ‚Äî let‚Äôs make it concrete with a tiny toy example you can actually visualize. We‚Äôll use a 2√ó2 RGB image so the numbers stay small.
</p>

<div id="Understanding data.shape-QUESTION-Step 1. Imagine the raw image data"><h4 id="Step 1. Imagine the raw image data" class="header"><a href="#Understanding data.shape-QUESTION-Step 1. Imagine the raw image data">Step 1. Imagine the raw image data</a></h4></div>
<p>
Suppose we have a 2√ó2 image with 3 channels (RGB). OIIO gives us:
</p>

<pre python>
import numpy as np

# Fake 2x2 RGB image
data = np.array([
    [[1.0, 0.0, 0.0], [0.0, 1.0, 0.0]],   # Row 0: Red, Green
    [[0.0, 0.0, 1.0], [1.0, 1.0, 1.0]]    # Row 1: Blue, White
])

print("Original shape:", data.shape)
print(data)
</pre>
<p>
Output:
</p>

<pre python>
Original shape: (2, 2, 3)

[
  [[1. 0. 0.]   [0. 1. 0.]]
  [[0. 0. 1.]   [1. 1. 1.]]
]
Shape (2, 2, 3) = Height 2, Width 2, Channels 3
</pre>

<p>
It‚Äôs literally a grid: 2 rows √ó 2 columns, each pixel has 3 values (R, G, B).
</p>

<div id="Understanding data.shape-QUESTION-Step 2. Reshape to a flat list of pixels"><h4 id="Step 2. Reshape to a flat list of pixels" class="header"><a href="#Understanding data.shape-QUESTION-Step 2. Reshape to a flat list of pixels">Step 2. Reshape to a flat list of pixels</a></h4></div>

<p>
Now apply the reshape:
</p>

<pre python>
data_2d = data.reshape(-1, data.shape[-1])
print("Reshaped shape:", data_2d.shape)
print(data_2d)
</pre>
<p>
Output:
</p>

<pre python>
Reshaped shape: (4, 3)

[
  [1. 0. 0.]   # Pixel 0: Red
  [0. 1. 0.]   # Pixel 1: Green
  [0. 0. 1.]   # Pixel 2: Blue
  [1. 1. 1.]   # Pixel 3: White
]
Shape (4, 3) = 4 pixels, each with 3 channels.
</pre>

<p>
The 2D grid has been flattened into a simple list of pixels.
</p>

<div id="Understanding data.shape-QUESTION-Step 3. Why data.shape[-1]"><h4 id="Step 3. Why data.shape[-1]" class="header"><a href="#Understanding data.shape-QUESTION-Step 3. Why data.shape[-1]">Step 3. Why data.shape[-1]</a></h4></div>

<ul>
<li>
<code>data.shape[-1] = 3</code> (the number of channels).

</ul>
<p>
So reshape(-1, data.shape[-1]) means: ‚Üí ‚ÄúFlatten everything else into one dimension, but keep the last dimension (channels) intact.‚Äù
</p>

<div id="Understanding data.shape-QUESTION-‚ú® So in your head:"><h4 id="‚ú® So in your head:" class="header"><a href="#Understanding data.shape-QUESTION-‚ú® So in your head:">‚ú® So in your head:</a></h4></div>

<ul>
<li>
Before reshape: a cube-like structure [rows][cols][channels].

<li>
After reshape: a flat spreadsheet [pixels][channels].

</ul>
<p>
Let‚Äôs extend the toy example to RGBA so you can see exactly what happens when there‚Äôs an alpha channel, and why the code trims it back to RGB with [:, :3].
</p>

<div id="Understanding data.shape-QUESTION-Step 1. A 2√ó2 RGBA image"><h4 id="Step 1. A 2√ó2 RGBA image" class="header"><a href="#Understanding data.shape-QUESTION-Step 1. A 2√ó2 RGBA image">Step 1. A 2√ó2 RGBA image</a></h4></div>

<p>
We‚Äôll make a tiny 2√ó2 image with 4 channels (R, G, B, A):
</p>

<pre python>
import numpy as np

# Fake 2x2 RGBA image
data = np.array([
    [[1.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.5]],   # Row 0: Red (opaque), Green (50% alpha)
    [[0.0, 0.0, 1.0, 1.0], [1.0, 1.0, 1.0, 0.0]]    # Row 1: Blue (opaque), White (transparent)
])

print("Original shape:", data.shape)
print(data)
</pre>
<p>
Output:
</p>

<pre python>
Original shape: (2, 2, 4)

[
  [[1. 0. 0. 1.]   [0. 1. 0. 0.5]]
  [[0. 0. 1. 1.]   [1. 1. 1. 0. ]]
]
Shape (2, 2, 4) = 2 rows √ó 2 cols √ó 4 channels (RGBA).
</pre>

<div id="Understanding data.shape-QUESTION-Step 2. Reshape to a flat list of pixels"><h4 id="Step 2. Reshape to a flat list of pixels" class="header"><a href="#Understanding data.shape-QUESTION-Step 2. Reshape to a flat list of pixels">Step 2. Reshape to a flat list of pixels</a></h4></div>

<pre python>
data_2d = data.reshape(-1, data.shape[-1])
print("Reshaped shape:", data_2d.shape)
print(data_2d)
</pre>
<p>
Output:
</p>

<pre>
Reshaped shape: (4, 4)

[
  [1. 0. 0. 1. ]   # Pixel 0: Red, alpha=1
  [0. 1. 0. 0.5]   # Pixel 1: Green, alpha=0.5
  [0. 0. 1. 1. ]   # Pixel 2: Blue, alpha=1
  [1. 1. 1. 0. ]   # Pixel 3: White, alpha=0
]
</pre>

<p>
Now it‚Äôs (4, 4) ‚Üí 4 pixels, each with 4 values (R, G, B, A).
</p>

<div id="Understanding data.shape-QUESTION-Step 3. Slice off the alpha"><h4 id="Step 3. Slice off the alpha" class="header"><a href="#Understanding data.shape-QUESTION-Step 3. Slice off the alpha">Step 3. Slice off the alpha</a></h4></div>
<p>
Your code does:
</p>

<pre python>
rgb_only = data_2d[:, :3]
print("RGB only:\n", rgb_only)
</pre>
<p>
Output:
</p>

<pre python>
[
  [1. 0. 0.]   # Red
  [0. 1. 0.]   # Green
  [0. 0. 1.]   # Blue
  [1. 1. 1.]   # White
]
Shape (4, 3) ‚Üí 4 pixels, each with just RGB.
</pre>

<p>
The alpha channel is dropped, which is usually what you want if you‚Äôre only analyzing color values.
</p>

<div id="Understanding data.shape-QUESTION-‚ú® So the flow is:"><h4 id="‚ú® So the flow is:" class="header"><a href="#Understanding data.shape-QUESTION-‚ú® So the flow is:">‚ú® So the flow is:</a></h4></div>

<ul>
<li>
Original: (H, W, 4) ‚Üí grid with RGBA.

<li>
Reshaped: (Pixels, 4) ‚Üí flat list of pixels with RGBA.

<li>
Sliced: (Pixels, 3) ‚Üí flat list of pixels with RGB only.

</ul>
<div id="Understanding data.shape-QUESTION-One more Walk-Through of data.reshape(-1, data.shape[-1])"><h3 id="One more Walk-Through of data.reshape(-1, data.shape[-1])" class="header"><a href="#Understanding data.shape-QUESTION-One more Walk-Through of data.reshape(-1, data.shape[-1])">One more Walk-Through of data.reshape(-1, data.shape[-1])</a></h3></div>

<table>
<tr>
<td>
I still can't the process of: data.reshape(-1, data.shape[-1]) What do the arguments mean and what do they actually do to the (2, 2, 4) array?
</td>
</tr>
</table>

<p>
Based on your 2x2x4 example array, let's break down exactly what happens with data.reshape(-1, data.shape[-1]).
</p>

<div id="Understanding data.shape-QUESTION-The data array"><h3 id="The data array" class="header"><a href="#Understanding data.shape-QUESTION-The data array">The data array</a></h3></div>

<p>
The starting point is your 3D NumPy array, representing the 2x2 image with 4 channels (RGBA).
</p>

<p>
Array shape: (2, 2, 4)
</p>

<p>
Array content:
</p>
<pre python>
[[[0.1, 0.2, 0.3, 1.0], [0.4, 0.5, 0.6, 1.0]],
 [[0.7, 0.8, 0.9, 1.0], [1.0, 1.0, 1.0, 1.0]]]
</pre>

<div id="Understanding data.shape-QUESTION-Breakdown of data.reshape(-1, data.shape[-1])"><h3 id="Breakdown of data.reshape(-1, data.shape[-1])" class="header"><a href="#Understanding data.shape-QUESTION-Breakdown of data.reshape(-1, data.shape[-1])">Breakdown of data.reshape(-1, data.shape[-1])</a></h3></div>

<p>
The reshape() function takes a tuple of integers as arguments to define the new shape. 
</p>
<pre>
data.shape[-1]
</pre>

<ul>
<li>
<span id="Understanding data.shape-QUESTION-Breakdown of data.reshape(-1, data.shape[-1])-What it does"></span><strong id="What it does">What it does</strong>: This expression gets the size of the last dimension of the data array. In Python, a negative index like -1 accesses the last element of a sequence.

<li>
<span id="Understanding data.shape-QUESTION-Breakdown of data.reshape(-1, data.shape[-1])-Calculation"></span><strong id="Calculation">Calculation</strong>: For your (2, 2, 4) array, data.shape[-1] evaluates to 4. This means the new array will have 4 columns. 

</ul>
<div id="Understanding data.shape-QUESTION--1"><h3 id="-1" class="header"><a href="#Understanding data.shape-QUESTION--1">-1</a></h3></div>

<ul>
<li>
<span id="Understanding data.shape-QUESTION--1-What it does"></span><strong id="What it does">What it does</strong>: The -1 argument is a special instruction to NumPy. It tells NumPy to "automatically infer" the size of this dimension based on the array's total number of elements and the other dimensions you have specified.

<li>
<span id="Understanding data.shape-QUESTION--1-Calculation"></span><strong id="Calculation">Calculation</strong>:

<ul>
<li>
Total elements: The original array has 2 * 2 * 4 = 16 elements.

<li>
Known dimension: The second dimension is fixed at 4.

<li>
Inferred dimension: To maintain the same number of total elements (16), NumPy calculates the first dimension as 16 / 4 = 4. 

</ul>
</ul>
<div id="Understanding data.shape-QUESTION-The result of the reshape() call"><h3 id="The result of the reshape() call" class="header"><a href="#Understanding data.shape-QUESTION-The result of the reshape() call">The result of the reshape() call</a></h3></div>

<p>
Putting it all together, data.reshape(-1, data.shape[-1]) is equivalent to data.reshape(4, 4). 
</p>

<p>
Resulting Array:
</p>

<p>
NumPy takes the elements from the original array in a flattened, row-by-row (C-style) order and arranges them into the new (4, 4) shape. 
</p>
<pre python>
[[0.1, 0.2, 0.3, 1.0],   # First pixel
 [0.4, 0.5, 0.6, 1.0],   # Second pixel
 [0.7, 0.8, 0.9, 1.0],   # Third pixel
 [1.0, 1.0, 1.0, 1.0]]   # Fourth pixel
</pre>

<div id="Understanding data.shape-QUESTION-Purpose of this transformation"><h3 id="Purpose of this transformation" class="header"><a href="#Understanding data.shape-QUESTION-Purpose of this transformation">Purpose of this transformation</a></h3></div>

<p>
The purpose of this operation is to convert the image data from a spatial representation (width x height) into a list of individual pixels. Each row of the new 2D array represents a single pixel, and each column represents a color channel. This is a common and necessary step for many data analysis and machine learning tasks that operate on a per-pixel basis rather than a per-image basis.
</p>

<div id="Understanding data.shape-np.unique"><h2 id="np.unique" class="header"><a href="#Understanding data.shape-np.unique">np.unique</a></h2></div>

<p>
Let‚Äôs see how np.unique works on that reshaped pixel list. We‚Äôll use the little 2√ó2 RGBA toy image from before, flatten it, slice to RGB, and then run np.unique.
</p>

<div id="Understanding data.shape-np.unique-Step 1. Our reshaped RGB pixel list"><h4 id="Step 1. Our reshaped RGB pixel list" class="header"><a href="#Understanding data.shape-np.unique-Step 1. Our reshaped RGB pixel list">Step 1. Our reshaped RGB pixel list</a></h4></div>
<p>
From the RGBA example, after reshaping and slicing to RGB we had:
</p>

<pre python>
import numpy as np

rgb_only = np.array([
    [1.0, 0.0, 0.0],   # Red
    [0.0, 1.0, 0.0],   # Green
    [0.0, 0.0, 1.0],   # Blue
    [1.0, 1.0, 1.0]    # White
])
</pre>

<div id="Understanding data.shape-np.unique-Step 2. Apply np.unique"><h4 id="Step 2. Apply np.unique" class="header"><a href="#Understanding data.shape-np.unique-Step 2. Apply np.unique">Step 2. Apply np.unique</a></h4></div>

<pre python>
unique_colors, counts = np.unique(rgb_only, axis=0, return_counts=True)

print("Unique colors:\n", unique_colors)
print("Counts:\n", counts)
</pre>

<p>
Output:
</p>

<pre python>
Unique colors:
[[0. 0. 1.]
 [0. 1. 0.]
 [1. 0. 0.]
 [1. 1. 1.]]

Counts:
[1 1 1 1]
</pre>

<ul>
<li>
unique_colors is a 2D array of distinct RGB triplets.

<li>
counts tells you how many times each color appeared in the image.

</ul>
<div id="Understanding data.shape-np.unique-Step 3. Try with duplicates"><h4 id="Step 3. Try with duplicates" class="header"><a href="#Understanding data.shape-np.unique-Step 3. Try with duplicates">Step 3. Try with duplicates</a></h4></div>

<p>
Let‚Äôs add a duplicate pixel to see the counting in action:
</p>

<pre python>
rgb_with_dupes = np.array([
    [1.0, 0.0, 0.0],   # Red
    [0.0, 1.0, 0.0],   # Green
    [1.0, 0.0, 0.0],   # Red again
    [1.0, 1.0, 1.0]    # White
])

unique_colors, counts = np.unique(rgb_with_dupes, axis=0, return_counts=True)

print("Unique colors:\n", unique_colors)
print("Counts:\n", counts)
</pre>
<p>
Output:
</p>

<pre python>
Unique colors:
[[0. 1. 0.]
 [1. 0. 0.]
 [1. 1. 1.]]

Counts:
[1 2 1]
</pre>

<ul>
<li>
Green appears once.

<li>
Red appears twice.

<li>
White appears once.

</ul>
<div id="Understanding data.shape-np.unique-‚ú® That‚Äôs exactly what your function is doing under the hood:"><h4 id="‚ú® That‚Äôs exactly what your function is doing under the hood:" class="header"><a href="#Understanding data.shape-np.unique-‚ú® That‚Äôs exactly what your function is doing under the hood:">‚ú® That‚Äôs exactly what your function is doing under the hood:</a></h4></div>

<ul>
<li>
Flatten the image into a list of pixels.

<li>
Use np.unique(..., axis=0, return_counts=True) to find distinct colors and how often they occur.

<li>
Package that into a dictionary for easy use.

</ul>
<div id="Understanding data.shape-Sorting by Frequency"><h2 id="Sorting by Frequency" class="header"><a href="#Understanding data.shape-Sorting by Frequency">Sorting by Frequency</a></h2></div>

<p>
Sorting by frequency is a really handy trick when you want to know the most common colors first (like finding a palette). Let‚Äôs build on the duplicate example we just did.
</p>

<div id="Understanding data.shape-Sorting by Frequency-Step 1. Our pixel list with duplicates"><h4 id="Step 1. Our pixel list with duplicates" class="header"><a href="#Understanding data.shape-Sorting by Frequency-Step 1. Our pixel list with duplicates">Step 1. Our pixel list with duplicates</a></h4></div>

<pre python>
import numpy as np

rgb_with_dupes = np.array([
    [1.0, 0.0, 0.0],   # Red
    [0.0, 1.0, 0.0],   # Green
    [1.0, 0.0, 0.0],   # Red again
    [1.0, 1.0, 1.0]    # White
])
</pre>

<div id="Understanding data.shape-Sorting by Frequency-Step 2. Find unique colors and counts"><h4 id="Step 2. Find unique colors and counts" class="header"><a href="#Understanding data.shape-Sorting by Frequency-Step 2. Find unique colors and counts">Step 2. Find unique colors and counts</a></h4></div>

<pre python>
unique_colors, counts = np.unique(rgb_with_dupes, axis=0, return_counts=True)

print("Unique colors:\n", unique_colors)
print("Counts:\n", counts)
</pre>
<p>
Output:
</p>

<pre python>
Unique colors:
[[0. 1. 0.]
 [1. 0. 0.]
 [1. 1. 1.]]
Counts:
[1 2 1]
</pre>
<p>
So:
</p>
<ul>
<li>
Green ‚Üí 1

<li>
Red ‚Üí 2

<li>
White ‚Üí 1

</ul>
<div id="Understanding data.shape-Sorting by Frequency-Step 3. Sort by frequency"><h4 id="Step 3. Sort by frequency" class="header"><a href="#Understanding data.shape-Sorting by Frequency-Step 3. Sort by frequency">Step 3. Sort by frequency</a></h4></div>

<p>
We want the most frequent colors first. That‚Äôs where np.argsort comes in:
</p>

<pre python>
# Sort indices by counts (descending)
sorted_indices = np.argsort(-counts)

# Reorder both arrays
sorted_colors = unique_colors[sorted_indices]
sorted_counts = counts[sorted_indices]

print("Sorted colors:\n", sorted_colors)
print("Sorted counts:\n", sorted_counts)
</pre>

<p>
Output:
</p>

<pre python>
Sorted colors:
[[1. 0. 0.]
 [0. 1. 0.]
 [1. 1. 1.]]
Sorted counts:
[2 1 1]
</pre>

<p>
Now the results are ordered by frequency:
</p>
<ul>
<li>
Red (2 times)

<li>
Green (1 time)

<li>
White (1 time)

</ul>
<div id="Understanding data.shape-Sorting by Frequency-Step 4. Wrap into a dictionary (like your function)"><h4 id="Step 4. Wrap into a dictionary (like your function)" class="header"><a href="#Understanding data.shape-Sorting by Frequency-Step 4. Wrap into a dictionary (like your function)">Step 4. Wrap into a dictionary (like your function)</a></h4></div>

<pre python>
color_dict = {}
for i, color in enumerate(sorted_colors):
    r, g, b = color.tolist()
    color_key = f"({r:.2f}, {g:.2f}, {b:.2f})"
    color_dict[color_key] = {
        "r": r,
        "g": g,
        "b": b,
        "count": int(sorted_counts[i])
    }

print(color_dict)
</pre>
<p>
Output:
</p>

<pre python>
{
  '(1.00, 0.00, 0.00)': {'r': 1.0, 'g': 0.0, 'b': 0.0, 'count': 2},
  '(0.00, 1.00, 0.00)': {'r': 0.0, 'g': 1.0, 'b': 0.0, 'count': 1},
  '(1.00, 1.00, 1.00)': {'r': 1.0, 'g': 1.0, 'b': 1.0, 'count': 1}
}
</pre>

<div id="Understanding data.shape-Sorting by Frequency-‚ú® That‚Äôs the whole pipeline:"><h4 id="‚ú® That‚Äôs the whole pipeline:" class="header"><a href="#Understanding data.shape-Sorting by Frequency-‚ú® That‚Äôs the whole pipeline:">‚ú® That‚Äôs the whole pipeline:</a></h4></div>

<ul>
<li>
Flatten pixels ‚Üí

<li>
np.unique(..., return_counts=True) ‚Üí

<li>
Sort by counts ‚Üí

<li>
Package into a dictionary.

</ul>
<div id="Understanding data.shape-Why a Temp File"><h2 id="Why a Temp File" class="header"><a href="#Understanding data.shape-Why a Temp File">Why a Temp File</a></h2></div>

<div id="Understanding data.shape-QUESTION"><h2 id="QUESTION" class="header"><a href="#Understanding data.shape-QUESTION">QUESTION</a></h2></div>

<table>
<tr>
<td>
In my original script, why do we need to create temporary files? Can't we read this data into memory and skip the file creation?
</td>
</tr>
</table>

<p>
That‚Äôs a sharp observation ‚Äî the temporary file step is really just a workaround for how Nuke‚Äôs API exposes pixel data.
</p>

<div id="Understanding data.shape-QUESTION-Why the temp file is there"><h3 id="Why the temp file is there" class="header"><a href="#Understanding data.shape-QUESTION-Why the temp file is there">Why the temp file is there</a></h3></div>

<ul>
<li>
In Nuke, most nodes don‚Äôt give you direct access to their pixel buffers in Python.

<li>
The usual way to ‚Äúsample‚Äù pixels is through nuke.sample() or nuke.PixelAnalyzer, but those are slow if you want the entire image.

</ul>
<p>
To get the full frame into a NumPy array, the common trick is:
</p>
<ul>
<li>
Render the node into a high‚Äëprecision file (EXR).

<li>
Re‚Äëread that file with a library like OpenImageIO, which does expose the pixels as a NumPy array.

</ul>
<p>
So the temp file is basically a bridge between Nuke‚Äôs internal image engine and Python‚Äôs numerical ecosystem.
</p>

<div id="Understanding data.shape-QUESTION-Could we skip the file?"><h3 id="Could we skip the file?" class="header"><a href="#Understanding data.shape-QUESTION-Could we skip the file?">Could we skip the file?</a></h3></div>

<p>
Yes, but with caveats:
</p>
<ul>
<li>
Using Nuke‚Äôs toImage() (Nuke 13+ with Python 3)

<ul>
<li>
Some nodes (like nuke.Node.toImage()) can return a nuke.Image object directly in memory.

<li>
You can then call .getPixel() or .channels() on it.

<li>
But this is not as fast as OIIO for large frames, and it‚Äôs not as NumPy‚Äëfriendly.

</ul>
<li>
Using BlinkScript or custom plugins

<ul>
<li>
You could write a BlinkScript node that dumps pixel data into an array.

<li>
This avoids disk I/O but requires more setup.

</ul>
<li>
Using the Viewer Process / Scanline API

<ul>
<li>
Nuke‚Äôs C++ API can access pixels directly, but from Python it‚Äôs limited.

<li>
That‚Äôs why most Python‚Äëonly workflows fall back to the temp file trick.

</ul>
</ul>
<div id="Understanding data.shape-QUESTION-Could we skip the file?-Trade‚Äëoffs"><h4 id="Trade‚Äëoffs" class="header"><a href="#Understanding data.shape-QUESTION-Could we skip the file?-Trade‚Äëoffs">Trade‚Äëoffs</a></h4></div>

<ul>
<li>
Temp file approach ‚úÖ Simple, works everywhere, leverages OIIO‚Äôs speed. ‚ùå Disk I/O overhead, not ideal for huge sequences.

<li>
In‚Äëmemory approach ‚úÖ No disk writes, potentially faster for small images. ‚ùå More complex, less documented, sometimes slower for big frames.

</ul>
<div id="Understanding data.shape-QUESTION-Could we skip the file?-‚ú® In practice:"><h4 id="‚ú® In practice:" class="header"><a href="#Understanding data.shape-QUESTION-Could we skip the file?-‚ú® In practice:">‚ú® In practice:</a></h4></div>

<p>
If you‚Äôre analyzing just a few frames, the temp file method is fine and very reliable. If you need to process hundreds of frames per second, then exploring in‚Äëmemory APIs (like toImage()) or a custom BlinkScript exporter would be worth it.
</p>

<div id="Understanding data.shape-Batch Processing Frames"><h2 id="Batch Processing Frames" class="header"><a href="#Understanding data.shape-Batch Processing Frames">Batch Processing Frames</a></h2></div>

<p>
Batch processing is a natural next step if you want to analyze multiple frames or multiple nodes instead of just one. Right now your function is designed for a single node and frame. Let‚Äôs expand the idea.
</p>

<div id="Understanding data.shape-Batch Processing Frames-üîÑ What Batch Processing Could Mean"><h3 id="üîÑ What Batch Processing Could Mean" class="header"><a href="#Understanding data.shape-Batch Processing Frames-üîÑ What Batch Processing Could Mean">üîÑ What Batch Processing Could Mean</a></h3></div>

<ul>
<li>
Multiple Frames from the Same Node

<ul>
<li>
Example: analyze every 10th frame of a shot to see how colors evolve over time.

<li>
Input: ('Read1', frames=[100, 110, 120, 130]).

</ul>
<li>
Multiple Nodes at Once

<ul>
<li>
Example: run the same analysis on several Read nodes in a script.

<li>
Input: (['Read1', 'Read2', 'Read3'], frame=101).

</ul>
<li>
Full Sequences

<ul>
<li>
Example: analyze all frames in a range and build a color histogram across the entire shot.

<li>
Input: ('Read1', frames=range(100, 200)).

</ul>
</ul>
<div id="Understanding data.shape-Batch Processing Frames-üõ† How to Implement It"><h3 id="üõ† How to Implement It" class="header"><a href="#Understanding data.shape-Batch Processing Frames-üõ† How to Implement It">üõ† How to Implement It</a></h3></div>

<p>
You could wrap your existing function in a batch driver:
</p>

<pre python>
def batch_unique_colors(nodes, frames, max_colors=50):
    if isinstance(nodes, str):
        nodes = [nodes]
    if isinstance(frames, int):
        frames = [frames]

    results = {}
    for node in nodes:
        results[node] = {}
        for f in frames:
            results[node][f] = get_unique_colors_fast(node, f, max_colors=max_colors)
    return results
</pre>

<div id="Understanding data.shape-Batch Processing Frames-üõ† How to Implement It-üìä Example Usage"><h4 id="üìä Example Usage" class="header"><a href="#Understanding data.shape-Batch Processing Frames-üõ† How to Implement It-üìä Example Usage">üìä Example Usage</a></h4></div>

<pre python>
# Analyze multiple frames from one node
results = batch_unique_colors('Read1', frames=[100, 105, 110])

# Analyze multiple nodes at one frame
results = batch_unique_colors(['Read1', 'Read2'], frames=101)

# Analyze a whole range
results = batch_unique_colors('Read1', frames=range(100, 110))
</pre>
<p>
The returned structure would look like:
</p>

<pre python>
{
  'Read1': {
    100: {...},
    105: {...},
    110: {...}
  },
  'Read2': {
    101: {...}
  }
}
</pre>

<div id="Understanding data.shape-Batch Processing Frames-‚ö° Enhancements You Could Add"><h3 id="‚ö° Enhancements You Could Add" class="header"><a href="#Understanding data.shape-Batch Processing Frames-‚ö° Enhancements You Could Add">‚ö° Enhancements You Could Add</a></h3></div>

<ul>
<li>
Progress reporting: print or log which frame/node is being processed.

<li>
Parallelization: use Python‚Äôs concurrent.futures to process frames in parallel (careful with Nuke‚Äôs thread safety).

<li>
Aggregation: instead of returning per‚Äëframe results, you could merge counts across frames to get a ‚Äúglobal palette.‚Äù

<li>
Top‚ÄëN filtering: store only the most frequent colors per frame to keep results manageable.

</ul>
<p>
‚ú® In short: batch processing turns your single‚Äëframe color analysis into a timeline‚Äë or sequence‚Äëlevel tool, letting you track palettes across shots or compare multiple sources.
</p>

<p>
Would you like me to sketch out how to aggregate across frames (e.g. ‚Äúgive me the top 10 colors across the whole sequence‚Äù instead of per‚Äëframe breakdowns)?
</p>


<div id="Understanding data.shape-QUESTION"><h2 id="QUESTION" class="header"><a href="#Understanding data.shape-QUESTION">QUESTION</a></h2></div>

<table>
<tr>
<td>
What I want is a list off all colors present from across all frames. My image sequences will never have more than 50 colors.
</td>
</tr>
</table>

<p>
Perfect ‚Äî since you know your sequences will never exceed ~50 unique colors, you can simplify things a lot. Instead of analyzing each frame separately and then merging, you can accumulate unique colors across all frames into one set/dictionary. That way you end up with a single palette for the whole sequence.
</p>

<div id="Understanding data.shape-QUESTION-üõ† How to Do It"><h4 id="üõ† How to Do It" class="header"><a href="#Understanding data.shape-QUESTION-üõ† How to Do It">üõ† How to Do It</a></h4></div>

<p>
Here‚Äôs a sketch of how you could adapt your function:
</p>

<pre python>
def get_all_unique_colors(input_node, frames, max_colors=50):
    node = nuke.toNode(input_node)
    if not node:
        return {"error": f"Node '{input_node}' not found."}

    all_colors = []
    all_counts = []

    for f in frames:
        result = get_unique_colors_fast(input_node, f, max_colors=max_colors)
        if "unique_colors" not in result:
            return result  # propagate error/warning

        # Collect colors + counts
        for color_key, info in result["unique_colors"].items():
            all_colors.append((info["r"], info["g"], info["b"]))
            all_counts.append(info["count"])

    # Convert to NumPy for deduplication
    all_colors = np.array(all_colors, dtype=np.float32)
    all_counts = np.array(all_counts, dtype=np.int32)

    # Deduplicate across frames
    unique_colors, inverse_indices = np.unique(all_colors, axis=0, return_inverse=True)

    # Sum counts for duplicates
    summed_counts = np.zeros(len(unique_colors), dtype=np.int32)
    for idx, count in zip(inverse_indices, all_counts):
        summed_counts[idx] += count

    if len(unique_colors) &gt; max_colors:
        return {"warning": f"Sequence has {len(unique_colors)} unique colors, exceeding limit."}

    # Format output
    color_dict = {}
    for i, color_vector in enumerate(unique_colors):
        r, g, b = color_vector.tolist()
        color_key = f"({r:.6f}, {g:.6f}, {b:.6f})"
        color_dict[color_key] = {
            "r": r,
            "g": g,
            "b": b,
            "count": int(summed_counts[i])
        }

    return {"unique_colors": color_dict}
</pre>

<div id="Understanding data.shape-QUESTION-üìä Example Usage"><h4 id="üìä Example Usage" class="header"><a href="#Understanding data.shape-QUESTION-üìä Example Usage">üìä Example Usage</a></h4></div>

<pre python>
# Analyze frames 100‚Äì110
results = get_all_unique_colors('Read1', frames=range(100, 111))
print(results)
</pre>

<p>
This will give you one dictionary of all unique colors across the entire frame range, with counts aggregated.
</p>

<div id="Understanding data.shape-QUESTION-‚ú® Why This Works Well for You"><h4 id="‚ú® Why This Works Well for You" class="header"><a href="#Understanding data.shape-QUESTION-‚ú® Why This Works Well for You">‚ú® Why This Works Well for You</a></h4></div>

<ul>
<li>
Since you know there are never more than ~50 colors, memory and performance are trivial.

<li>
You avoid having to merge results manually.

<li>
You get a global palette for the sequence, with counts showing how often each color appeared across all frames.

</ul>
<div id="Understanding data.shape-Sort by Frequency"><h2 id="Sort by Frequency" class="header"><a href="#Understanding data.shape-Sort by Frequency">Sort by Frequency</a></h2></div>

<p>
How to sort this final palette by frequency (so the most dominant colors across the whole sequence appear first)?
</p>

<p>
Perfect ‚Äî let‚Äôs add the sorting by frequency step to the ‚Äúall‚Äëframes‚Äù version so your final palette is ordered from most common to least common. That way you can immediately see which colors dominate across the whole sequence.
</p>

<div id="Understanding data.shape-Sort by Frequency-üîÑ Updated Function with Sorting"><h4 id="üîÑ Updated Function with Sorting" class="header"><a href="#Understanding data.shape-Sort by Frequency-üîÑ Updated Function with Sorting">üîÑ Updated Function with Sorting</a></h4></div>

<p>
Here‚Äôs how you could extend the get_all_unique_colors function:
</p>

<pre python>
def get_all_unique_colors(input_node, frames, max_colors=50):
    node = nuke.toNode(input_node)
    if not node:
        return {"error": f"Node '{input_node}' not found."}

    all_colors = []
    all_counts = []

    for f in frames:
        result = get_unique_colors_fast(input_node, f, max_colors=max_colors)
        if "unique_colors" not in result:
            return result  # propagate error/warning

        for color_key, info in result["unique_colors"].items():
            all_colors.append((info["r"], info["g"], info["b"]))
            all_counts.append(info["count"])

    # Deduplicate across frames
    all_colors = np.array(all_colors, dtype=np.float32)
    all_counts = np.array(all_counts, dtype=np.int32)

    unique_colors, inverse_indices = np.unique(all_colors, axis=0, return_inverse=True)

    summed_counts = np.zeros(len(unique_colors), dtype=np.int32)
    for idx, count in zip(inverse_indices, all_counts):
        summed_counts[idx] += count

    if len(unique_colors) &gt; max_colors:
        return {"warning": f"Sequence has {len(unique_colors)} unique colors, exceeding limit."}

    # ‚úÖ Sort by frequency (descending)
    sorted_indices = np.argsort(-summed_counts)
    unique_colors = unique_colors[sorted_indices]
    summed_counts = summed_counts[sorted_indices]

    # Format output
    color_dict = {}
    for i, color_vector in enumerate(unique_colors):
        r, g, b = color_vector.tolist()
        color_key = f"({r:.6f}, {g:.6f}, {b:.6f})"
        color_dict[color_key] = {
            "r": r,
            "g": g,
            "b": b,
            "count": int(summed_counts[i])
        }

    return {"unique_colors": color_dict}
</pre>

<div id="Understanding data.shape-Sort by Frequency-üìä Example Usage"><h4 id="üìä Example Usage" class="header"><a href="#Understanding data.shape-Sort by Frequency-üìä Example Usage">üìä Example Usage</a></h4></div>

<pre python>
results = get_all_unique_colors('Read1', frames=range(100, 111))
for color, info in results["unique_colors"].items():
    print(color, "‚Üí", info["count"])
</pre>

<p>
Output (example):
</p>

<pre python>
(1.000000, 0.000000, 0.000000) ‚Üí 1200   # Red
(0.000000, 1.000000, 0.000000) ‚Üí 800    # Green
(0.000000, 0.000000, 1.000000) ‚Üí 600    # Blue
...
</pre>

<p>
Now your palette is globally aggregated and sorted by frequency, so the most dominant colors across the entire sequence appear first.
</p>

<p>
‚ú® This gives you exactly what you described:
</p>
<ul>
<li>
One list of all colors across all frames.

<li>
Counts aggregated.

<li>
Sorted by dominance.

</ul>
<div id="Using Tile instead of Whole Image"><h1 id="Using Tile instead of Whole Image" class="header"><a href="#Using Tile instead of Whole Image">Using Tile instead of Whole Image</a></h1></div>
<table>
<tr>
<td>
With the tile options, and also the reshape method (both of which can reduce memory usage), can you suggest and update the code?
</td>
</tr>
</table>

<p>
Here is an updated version of your code that uses OpenImageIO's tile-based processing. This approach is more memory-efficient for large images, as it avoids loading the entire pixel data into memory at once. It reads the image in smaller chunks (tiles), processes each tile, and aggregates the results. 
</p>

<p>
Updated get_unique_colors_fast function
</p>

<pre python>
<table>
<tr>
<td>
UNTESTED
</td>
</tr>
</table>
import nuke
import OpenImageIO as oiio
import numpy as np
import tempfile
import os
import uuid

# ----------------------------------------------------------------------
# A. SINGLE-FRAME PROCESSING FUNCTION (Optimized for Large Images)
# ----------------------------------------------------------------------
def get_unique_colors_fast_tiled(input_node, frame, max_colors=50):
    node = nuke.toNode(input_node)
    if not node:
        return {"error": f"Node '{input_node}' not found."}

    # 1. SET UP AND RENDER TO A TEMPORARY FILE
    temp_file = os.path.join(tempfile.gettempdir(), f"nuke_temp_colors_{uuid.uuid4().hex}.exr")
    
    w = nuke.createNode('Write', inpanel=False)
    w['file'].setValue(temp_file)
    w['file_type'].setValue('exr')
    w.setInput(0, node)

    try:
        nuke.execute(w, frame, frame)

        # 2. READ THE DATA IN TILES AND PROCESS
        input_file = oiio.ImageInput.open(temp_file)
        if not input_file:
            return {"error": "Failed to read temporary image file with OpenImageIO."}
        
        spec = input_file.spec()
        width, height, channels = spec.width, spec.height, spec.nchannels
        
        # Determine tile size, using OIIO's native if available, otherwise a sensible default
        tile_width = spec.tile_width if spec.tile_width else 64
        tile_height = spec.tile_height if spec.tile_height else 64
        
        all_pixels = []
        
        # Loop over tiles and process each one
        for y in range(0, height, tile_height):
            for x in range(0, width, tile_width):
                # Clamp coordinates to ensure we don't read past the image boundaries
                x_end = min(x + tile_width, width)
                y_end = min(y + tile_height, height)
                
                # Read a single tile as a float NumPy array
                tile_data = input_file.read_tile(x, y, 0, oiio.FLOAT)
                
                if tile_data is not None:
                    # Reshape and slice the tile data
                    data_2d = tile_data.reshape(-1, tile_data.shape[-1])[:, :3]
                    all_pixels.append(data_2d)

        input_file.close()

        # Combine all tile pixel data into a single array
        if not all_pixels:
            return {"error": "No pixel data could be read from the image."}
            
        full_image_data = np.concatenate(all_pixels, axis=0)

        # 3. ANALYZE UNIQUE COLORS
        unique_colors, counts = np.unique(full_image_data, axis=0, return_counts=True)
        total_unique_colors = len(unique_colors)
        
        if total_unique_colors &gt; max_colors:
             return {"warning": f"Image has {total_unique_colors} unique colors, exceeding limit of {max_colors}."}

        color_dict = {}
        for i, color_vector in enumerate(unique_colors):
            r, g, b = color_vector.tolist()
            color_key = f"({r:.6f}, {g:.6f}, {b:.6f})"
            color_dict[color_key] = {
                "r": r, "g": g, "b": b, "count": int(counts[i])
            }
        
        return {
            "total_unique_colors": total_unique_colors,
            "unique_colors": color_dict
        }

    finally:
        nuke.delete(w)
        if os.path.exists(temp_file):
            os.remove(temp_file)

# ----------------------------------------------------------------------
# B. MULTI-FRAME CALLER FUNCTION
# ----------------------------------------------------------------------
# The caller function does not need to be changed. It simply calls the new
# tile-based processing function instead of the old one.
def analyze_range(input_node_name, max_colors=50):
    node = nuke.toNode(input_node_name)
    if not node:
        return {"error": f"Node '{input_node_name}' not found."}
    
    try:
        first_frame = int(node['first'].value())
        last_frame = int(node['last'].value())
    except NameError:
        return {"error": f"Node '{input_node_name}' does not have 'first'/'last' frame knobs."}

    print(f"Starting analysis for {input_node_name} from frame {first_frame} to {last_frame}...")
    
    range_results = {}
    
    for frame in range(first_frame, last_frame + 1):
        print(f"Processing frame: {frame}...")
        result = get_unique_colors_fast_tiled(input_node_name, frame, max_colors) # Call the new function
        range_results[frame] = result

    print("Analysis complete.")
    return {"full_analysis": range_results}


# Example Usage (assuming a Read node named 'Read1' exists)
# results_over_range = analyze_range('Read1')
# print(results_over_range)
</pre>
<p>
Use code with caution.
</p>

<div id="Using Tile instead of Whole Image-Summary of updates"><h3 id="Summary of updates" class="header"><a href="#Using Tile instead of Whole Image-Summary of updates">Summary of updates</a></h3></div>

<p>
The main changes are in the get_unique_colors_fast_tiled function, replacing the direct get_pixels() call with a tiled approach.
</p>
<ul>
<li>
<span id="Using Tile instead of Whole Image-Summary of updates-Read ImageInput"></span><strong id="Read ImageInput">Read ImageInput</strong>: Instead of using oiio.ImageBuf, the code opens a oiio.ImageInput. This object provides lower-level access, which is necessary for reading tiles.

<li>
<span id="Using Tile instead of Whole Image-Summary of updates-Get image spec"></span><strong id="Get image spec">Get image spec</strong>: It retrieves the image's specifications (spec = input_file.spec()) to get details like image and tile dimensions.

<li>
<span id="Using Tile instead of Whole Image-Summary of updates-Determine tile size"></span><strong id="Determine tile size">Determine tile size</strong>: The code checks if the image format is natively tiled (spec.tile_width). If not, it defaults to a reasonable tile size (e.g., 64x64).

<li>
<span id="Using Tile instead of Whole Image-Summary of updates-Loop over tiles"></span><strong id="Loop over tiles">Loop over tiles</strong>: A nested loop iterates through the image, reading one tile at a time. The input_file.read_tile(x, y, 0, oiio.FLOAT) call gets a chunk of the image as a NumPy array.

<li>
<span id="Using Tile instead of Whole Image-Summary of updates-Process tiles individually"></span><strong id="Process tiles individually">Process tiles individually</strong>: The same reshape and slicing logic is applied to each smaller tile array.

<li>
<span id="Using Tile instead of Whole Image-Summary of updates-Aggregate results"></span><strong id="Aggregate results">Aggregate results</strong>: The processed tiles are appended to a list (all_pixels). After the loop, they are combined using np.concatenate into a single array for the final np.unique analysis.

<li>
<span id="Using Tile instead of Whole Image-Summary of updates-Close the file"></span><strong id="Close the file">Close the file</strong>: The input_file is explicitly closed after reading is complete. 

</ul>
<p>
This approach offers a significant memory improvement for high-resolution images by never holding all pixel data in memory at once.
</p>

<p>
<a href="pmount_index.html">PMount</a>
</p>

<p>
<a href="index.html">Index</a>
</p>

</body>
</html>
